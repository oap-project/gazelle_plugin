{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gazelle Plugin A Native Engine for Spark SQL with vectorized SIMD optimizations Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL. Key Features Apache Arrow formatted intermediate data among Spark operator With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format. Apache Arrow based Native Readers for Parquet and other formats A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Apache Arrow Compute/Gandiva based operators We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch. Native Columnar Shuffle Operator with efficient compression support We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here How to use OAP: Gazelle Plugin There are three ways to use OAP: Gazelle Plugin, 1. Use precompiled jars 2. Building by Conda Environment 3. Building by Yourself Use precompiled jars Please go to OAP's Maven Central Repository to find Gazelle Plugin jars. For usage, you will require below two jar files: 1. spark-arrow-datasource-standard- -jar-with-dependencies.jar is located in com/intel/oap/spark-arrow-datasource-standard/ / 2. spark-columnar-core- -jar-with-dependencies.jar is located in com/intel/oap/spark-columnar-core/ / Please notice the files are fat jars shipped with our custom Arrow library and pre-compiled from our server(using GCC 9.3.0 and LLVM 7.0.1), which means you will require to pre-install GCC 9.3.0 and LLVM 7.0.1 in your system for normal usage. Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started . Building by yourself If you prefer to build from the source code on your hand, please follow below steps to set up your environment. Prerequisite There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node. Installation Please check the document Installation Guide Get started To enable Gazelle Plugin, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. More Configuration, please check the document Configuration Guide Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if Gazelle Plugin works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should showup on Spark console and you can check the DAG diagram with some Columnar Processing stage. Gazelle Plugin still lacks some features, please check out the limitations . Could/K8s Integration Google Cloud Dataproc Gazelle Plugin now supports to run on Dataproc 2.0, we provide a guide to help quickly install Gazelle Plugin and run TPC-DS with notebooks or scripts. Please refer to Gazelle_on_Dataproc to find details about: Create a cluster on Dataproc 2.0 with initialization actions. Gazelle Plugin jars compiled with -Pdataproc-2.0 parameter will installed by Conda in all cluster nodes. Config for enabling Gazelle Plugin. Run TPC-DS with notebooks or scripts. Performance data For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin. Memory allocation The memory usage in Gazelle Plugin is high. The allocations goes to two parts: 1) Java based allocation which is widely used in Arrow Java API. 2) Native side memory allocation used in each native kernel. We investigated the memory allocation behavior and made more turnings here , the memroy footprint is stable during a TPC-DS power run. Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact chendi.xue@intel.com binwei.yang@intel.com","title":"Gazelle Plugin"},{"location":"#gazelle-plugin","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations","title":"Gazelle Plugin"},{"location":"#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL.","title":"Introduction"},{"location":"#key-features","text":"","title":"Key Features"},{"location":"#apache-arrow-formatted-intermediate-data-among-spark-operator","text":"With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format.","title":"Apache Arrow formatted intermediate data among Spark operator"},{"location":"#apache-arrow-based-native-readers-for-parquet-and-other-formats","text":"A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source","title":"Apache Arrow based Native Readers for Parquet and other formats"},{"location":"#apache-arrow-computegandiva-based-operators","text":"We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch.","title":"Apache Arrow Compute/Gandiva based operators"},{"location":"#native-columnar-shuffle-operator-with-efficient-compression-support","text":"We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here","title":"Native Columnar Shuffle Operator with efficient compression support"},{"location":"#how-to-use-oap-gazelle-plugin","text":"There are three ways to use OAP: Gazelle Plugin, 1. Use precompiled jars 2. Building by Conda Environment 3. Building by Yourself","title":"How to use OAP: Gazelle Plugin"},{"location":"#use-precompiled-jars","text":"Please go to OAP's Maven Central Repository to find Gazelle Plugin jars. For usage, you will require below two jar files: 1. spark-arrow-datasource-standard- -jar-with-dependencies.jar is located in com/intel/oap/spark-arrow-datasource-standard/ / 2. spark-columnar-core- -jar-with-dependencies.jar is located in com/intel/oap/spark-columnar-core/ / Please notice the files are fat jars shipped with our custom Arrow library and pre-compiled from our server(using GCC 9.3.0 and LLVM 7.0.1), which means you will require to pre-install GCC 9.3.0 and LLVM 7.0.1 in your system for normal usage.","title":"Use precompiled jars"},{"location":"#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started .","title":"Building by Conda"},{"location":"#building-by-yourself","text":"If you prefer to build from the source code on your hand, please follow below steps to set up your environment.","title":"Building by yourself"},{"location":"#prerequisite","text":"There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node.","title":"Prerequisite"},{"location":"#installation","text":"Please check the document Installation Guide","title":"Installation"},{"location":"#get-started","text":"To enable Gazelle Plugin, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. More Configuration, please check the document Configuration Guide Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if Gazelle Plugin works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should showup on Spark console and you can check the DAG diagram with some Columnar Processing stage. Gazelle Plugin still lacks some features, please check out the limitations .","title":"Get started"},{"location":"#couldk8s-integration","text":"","title":"Could/K8s Integration"},{"location":"#google-cloud-dataproc","text":"Gazelle Plugin now supports to run on Dataproc 2.0, we provide a guide to help quickly install Gazelle Plugin and run TPC-DS with notebooks or scripts. Please refer to Gazelle_on_Dataproc to find details about: Create a cluster on Dataproc 2.0 with initialization actions. Gazelle Plugin jars compiled with -Pdataproc-2.0 parameter will installed by Conda in all cluster nodes. Config for enabling Gazelle Plugin. Run TPC-DS with notebooks or scripts.","title":"Google Cloud Dataproc"},{"location":"#performance-data","text":"For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin.","title":"Performance data"},{"location":"#memory-allocation","text":"The memory usage in Gazelle Plugin is high. The allocations goes to two parts: 1) Java based allocation which is widely used in Arrow Java API. 2) Native side memory allocation used in each native kernel. We investigated the memory allocation behavior and made more turnings here , the memroy footprint is stable during a TPC-DS power run.","title":"Memory allocation"},{"location":"#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"#contact","text":"chendi.xue@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"ApacheArrowInstallation/","text":"llvm-7.0: Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install cmake: Please make sure your cmake version is qualified based on the prerequisite. Arrow git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-4.0.0-oap mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_CSV=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni After arrow installed in the specific directory, please make sure to set up -Dbuild_arrow=OFF -Darrow_root=/path/to/arrow when building Gazelle Plugin.","title":"ApacheArrowInstallation"},{"location":"ApacheArrowInstallation/#llvm-70","text":"Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install","title":"llvm-7.0:"},{"location":"ApacheArrowInstallation/#cmake","text":"Please make sure your cmake version is qualified based on the prerequisite.","title":"cmake:"},{"location":"ApacheArrowInstallation/#arrow","text":"git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-4.0.0-oap mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_CSV=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni After arrow installed in the specific directory, please make sure to set up -Dbuild_arrow=OFF -Darrow_root=/path/to/arrow when building Gazelle Plugin.","title":"Arrow"},{"location":"Configuration/","text":"Spark Configurations for Gazelle Plugin There are many configuration could impact the Gazelle Plugin performance and can be fine tune in Spark. You can add these configuration into spark-defaults.conf to enable or disable the setting. Parameters Description Recommend Setting spark.driver.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Driver /path/to/jar_file1:/path/to/jar_file2 spark.executor.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Executor /path/to/jar_file1:/path/to/jar_file2 spark.executorEnv.LIBARROW_DIR To set up the location of Arrow library, by default it will search the loation of jar to be uncompressed /path/to/arrow_library/ spark.executorEnv.CC To set up the location of gcc /path/to/gcc/ spark.executor.memory To set up how much memory to be used for Spark Executor. spark.memory.offHeap.size To set up how much memory to be used for Java OffHeap. Please notice Gazelle Plugin will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Gazelle Plugin 30G spark.sql.sources.useV1SourceList Choose to use V1 source avro spark.sql.join.preferSortMergeJoin To turn off preferSortMergeJoin in Spark false spark.sql.extensions To turn on Gazelle Plugin com.intel.oap.ColumnarPlugin spark.shuffle.manager To turn on Gazelle Columnar Shuffle Plugin org.apache.spark.shuffle.sort.ColumnarShuffleManager spark.oap.sql.columnar.batchscan Enable or Disable Columnar Batchscan, default is true true spark.oap.sql.columnar.hashagg Enable or Disable Columnar Hash Aggregate, default is true true spark.oap.sql.columnar.projfilter Enable or Disable Columnar Project and Filter, default is true true spark.oap.sql.columnar.codegen.sort Enable or Disable Columnar Sort, default is true true spark.oap.sql.columnar.window Enable or Disable Columnar Window, default is true true spark.oap.sql.columnar.shuffledhashjoin Enable or Disable ShffuledHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin Enable or Disable Columnar Sort Merge Join, default is true true spark.oap.sql.columnar.union Enable or Disable Columnar Union, default is true true spark.oap.sql.columnar.expand Enable or Disable Columnar Expand, default is true true spark.oap.sql.columnar.broadcastexchange Enable or Disable Columnar Broadcast Exchange, default is true true spark.oap.sql.columnar.nanCheck Enable or Disable Nan Check, default is true true spark.oap.sql.columnar.hashCompare Enable or Disable Hash Compare in HashJoins or HashAgg, default is true true spark.oap.sql.columnar.broadcastJoin Enable or Disable Columnar BradcastHashJoin, default is true true spark.oap.sql.columnar.wholestagecodegen Enable or Disable Columnar WholeStageCodeGen, default is true true spark.oap.sql.columnar.preferColumnar Enable or Disable Columnar Operators, default is false. This parameter could impact the performance in different case. In some cases, to set false can get some performance boost. false spark.oap.sql.columnar.joinOptimizationLevel Fallback to row operators if there are several continous joins 6 spark.sql.execution.arrow.maxRecordsPerBatch Set up the Max Records per Batch 10000 spark.oap.sql.columnar.wholestagecodegen.breakdownTime Enable or Disable metrics in Columnar WholeStageCodeGen false spark.oap.sql.columnar.tmp_dir Set up a folder to store the codegen files /tmp spark.oap.sql.columnar.shuffle.customizedCompression.codec Set up the codec to be used for Columnar Shuffle, default is lz4 lz4 spark.oap.sql.columnar.numaBinding Set up NUMABinding, default is false true spark.oap.sql.columnar.coreRange Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system. Use 72 cores as an example. 0-17,36-53 |18-35,54-71 Below is an example for spark-default.conf, if you are using conda to install OAP project. ##### Columnar Process Configuration spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.sql.extensions com.intel.oap.ColumnarPlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note Gazelle Plugin depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/","title":"Configuration"},{"location":"Configuration/#spark-configurations-for-gazelle-plugin","text":"There are many configuration could impact the Gazelle Plugin performance and can be fine tune in Spark. You can add these configuration into spark-defaults.conf to enable or disable the setting. Parameters Description Recommend Setting spark.driver.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Driver /path/to/jar_file1:/path/to/jar_file2 spark.executor.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Executor /path/to/jar_file1:/path/to/jar_file2 spark.executorEnv.LIBARROW_DIR To set up the location of Arrow library, by default it will search the loation of jar to be uncompressed /path/to/arrow_library/ spark.executorEnv.CC To set up the location of gcc /path/to/gcc/ spark.executor.memory To set up how much memory to be used for Spark Executor. spark.memory.offHeap.size To set up how much memory to be used for Java OffHeap. Please notice Gazelle Plugin will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Gazelle Plugin 30G spark.sql.sources.useV1SourceList Choose to use V1 source avro spark.sql.join.preferSortMergeJoin To turn off preferSortMergeJoin in Spark false spark.sql.extensions To turn on Gazelle Plugin com.intel.oap.ColumnarPlugin spark.shuffle.manager To turn on Gazelle Columnar Shuffle Plugin org.apache.spark.shuffle.sort.ColumnarShuffleManager spark.oap.sql.columnar.batchscan Enable or Disable Columnar Batchscan, default is true true spark.oap.sql.columnar.hashagg Enable or Disable Columnar Hash Aggregate, default is true true spark.oap.sql.columnar.projfilter Enable or Disable Columnar Project and Filter, default is true true spark.oap.sql.columnar.codegen.sort Enable or Disable Columnar Sort, default is true true spark.oap.sql.columnar.window Enable or Disable Columnar Window, default is true true spark.oap.sql.columnar.shuffledhashjoin Enable or Disable ShffuledHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin Enable or Disable Columnar Sort Merge Join, default is true true spark.oap.sql.columnar.union Enable or Disable Columnar Union, default is true true spark.oap.sql.columnar.expand Enable or Disable Columnar Expand, default is true true spark.oap.sql.columnar.broadcastexchange Enable or Disable Columnar Broadcast Exchange, default is true true spark.oap.sql.columnar.nanCheck Enable or Disable Nan Check, default is true true spark.oap.sql.columnar.hashCompare Enable or Disable Hash Compare in HashJoins or HashAgg, default is true true spark.oap.sql.columnar.broadcastJoin Enable or Disable Columnar BradcastHashJoin, default is true true spark.oap.sql.columnar.wholestagecodegen Enable or Disable Columnar WholeStageCodeGen, default is true true spark.oap.sql.columnar.preferColumnar Enable or Disable Columnar Operators, default is false. This parameter could impact the performance in different case. In some cases, to set false can get some performance boost. false spark.oap.sql.columnar.joinOptimizationLevel Fallback to row operators if there are several continous joins 6 spark.sql.execution.arrow.maxRecordsPerBatch Set up the Max Records per Batch 10000 spark.oap.sql.columnar.wholestagecodegen.breakdownTime Enable or Disable metrics in Columnar WholeStageCodeGen false spark.oap.sql.columnar.tmp_dir Set up a folder to store the codegen files /tmp spark.oap.sql.columnar.shuffle.customizedCompression.codec Set up the codec to be used for Columnar Shuffle, default is lz4 lz4 spark.oap.sql.columnar.numaBinding Set up NUMABinding, default is false true spark.oap.sql.columnar.coreRange Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system. Use 72 cores as an example. 0-17,36-53 |18-35,54-71 Below is an example for spark-default.conf, if you are using conda to install OAP project. ##### Columnar Process Configuration spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.sql.extensions com.intel.oap.ColumnarPlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note Gazelle Plugin depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/","title":"Spark Configurations for Gazelle Plugin"},{"location":"Installation/","text":"Gazelle Plugin Installation For detailed testing scripts, please refer to solution guide Install Googletest and Googlemock yum install gtest-devel yum install gmock Build Gazelle Plugin git clone -b ${version} https://github.com/oap-project/native-sql-engine.git cd oap-native-sql mvn clean package -DskipTests -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Based on the different environment, there are some parameters can be set via -D with mvn. Parameters Description Default Value cpp_tests Enable or Disable CPP Tests False build_arrow Build Arrow from Source True arrow_root When build_arrow set to False, arrow_root will be enabled to find the location of your existing arrow library. /usr/local build_protobuf Build Protobuf from Source. If set to False, default library path will be used to find protobuf library. True When build_arrow set to True, the build_arrow.sh will be launched and compile a custom arrow library from OAP Arrow If you wish to change any parameters from Arrow, you can change it from the build_arrow.sh script under native-sql-engine/arrow-data-source/script/ . Additional Notes Notes for Installation Issues","title":"Installation"},{"location":"Installation/#gazelle-plugin-installation","text":"For detailed testing scripts, please refer to solution guide","title":"Gazelle Plugin Installation"},{"location":"Installation/#install-googletest-and-googlemock","text":"yum install gtest-devel yum install gmock","title":"Install Googletest and Googlemock"},{"location":"Installation/#build-gazelle-plugin","text":"git clone -b ${version} https://github.com/oap-project/native-sql-engine.git cd oap-native-sql mvn clean package -DskipTests -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Based on the different environment, there are some parameters can be set via -D with mvn. Parameters Description Default Value cpp_tests Enable or Disable CPP Tests False build_arrow Build Arrow from Source True arrow_root When build_arrow set to False, arrow_root will be enabled to find the location of your existing arrow library. /usr/local build_protobuf Build Protobuf from Source. If set to False, default library path will be used to find protobuf library. True When build_arrow set to True, the build_arrow.sh will be launched and compile a custom arrow library from OAP Arrow If you wish to change any parameters from Arrow, you can change it from the build_arrow.sh script under native-sql-engine/arrow-data-source/script/ .","title":"Build Gazelle Plugin"},{"location":"Installation/#additional-notes","text":"Notes for Installation Issues","title":"Additional Notes"},{"location":"InstallationNotes/","text":"Notes for Installation Issues Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"InstallationNotes"},{"location":"InstallationNotes/#notes-for-installation-issues","text":"Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"Notes for Installation Issues"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Gazelle Plugin Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.2.0 corresponds to all OAP modules' tag version v1.2.0 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. Building Building OAP OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-*.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh Building OAP specific module If you just want to build a specific OAP Module, such as sql-ds-cache , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --component=sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Gazelle Plugin","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.2.0 corresponds to all OAP modules' tag version v1.2.0 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance.","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"","title":"Building"},{"location":"OAP-Developer-Guide/#building-oap_1","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-*.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh","title":"Building OAP"},{"location":"OAP-Developer-Guide/#building-oap-specific-module","text":"If you just want to build a specific OAP Module, such as sql-ds-cache , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --component=sql-ds-cache","title":"Building OAP specific module"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -c conda-forge -c intel -y oap=1.2.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -c conda-forge -c intel -y oap=1.2.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"Prerequisite/","text":"Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. GCC 7.0 or higher version LLVM 7.0 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.16 or higher version Maven 3.6.3 or higher version Hadoop 2.7.5 or higher version Spark 3.1.1 or higher version Intel Optimized Arrow 4.0.0 gcc installation // installing GCC 7.0 or higher version Please notes for better performance support, GCC 7.0 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download gcc. C++ library may ask a certain version, if you are using GCC 7.0 the version would be libstdc++.so.6.0.28. You may have to launch ./contrib/download_prerequisites command to install all the prerequisites for gcc. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. //Follow the steps to configure gcc https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure //Follow the steps to build gc https://gcc.gnu.org/install/build.html //Follow the steps to install gcc https://gcc.gnu.org/install/finalinstall.html //Set up Environment for new gcc export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc //Verify if gcc has been installation Use gcc -v command to verify if your gcc version is correct.(Must larger than 7.0) LLVM 7.0 installation Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing Cmake 3.16.1 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn HADOOP/SPARK Installation If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Intel Optimized Apache Arrow Installation During the mvn compile command, it will launch a script(build_arrow.sh) to help install and compile a Intel custom Arrow library. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Prerequisite"},{"location":"Prerequisite/#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. GCC 7.0 or higher version LLVM 7.0 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.16 or higher version Maven 3.6.3 or higher version Hadoop 2.7.5 or higher version Spark 3.1.1 or higher version Intel Optimized Arrow 4.0.0","title":"Prerequisite"},{"location":"Prerequisite/#gcc-installation","text":"// installing GCC 7.0 or higher version Please notes for better performance support, GCC 7.0 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download gcc. C++ library may ask a certain version, if you are using GCC 7.0 the version would be libstdc++.so.6.0.28. You may have to launch ./contrib/download_prerequisites command to install all the prerequisites for gcc. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. //Follow the steps to configure gcc https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure //Follow the steps to build gc https://gcc.gnu.org/install/build.html //Follow the steps to install gcc https://gcc.gnu.org/install/finalinstall.html //Set up Environment for new gcc export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc //Verify if gcc has been installation Use gcc -v command to verify if your gcc version is correct.(Must larger than 7.0)","title":"gcc installation"},{"location":"Prerequisite/#llvm-70-installation","text":"Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install","title":"LLVM 7.0 installation"},{"location":"Prerequisite/#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing Cmake 3.16.1 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"Prerequisite/#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"Prerequisite/#hadoopspark-installation","text":"If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation","title":"HADOOP/SPARK Installation"},{"location":"Prerequisite/#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"Prerequisite/#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"Prerequisite/#intel-optimized-apache-arrow-installation","text":"During the mvn compile command, it will launch a script(build_arrow.sh) to help install and compile a Intel custom Arrow library. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Intel Optimized Apache Arrow Installation"},{"location":"SparkInstallation/","text":"Download Spark 3.1.1 Currently Gazelle Plugin works on the Spark 3.1.1 version. wget http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.1.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.1.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.1.1-bin-hadoop3.2/ Or building Spark from source git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH} Hadoop building from source git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"SparkInstallation"},{"location":"SparkInstallation/#download-spark-311","text":"Currently Gazelle Plugin works on the Spark 3.1.1 version. wget http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.1.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.1.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.1.1-bin-hadoop3.2/","title":"Download Spark 3.1.1"},{"location":"SparkInstallation/#or-building-spark-from-source","text":"git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH}","title":"Or building Spark from source"},{"location":"SparkInstallation/#hadoop-building-from-source","text":"git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"Hadoop building from source"},{"location":"User-Guide/","text":"Gazelle Plugin A Native Engine for Spark SQL with vectorized SIMD optimizations Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL. Key Features Apache Arrow formatted intermediate data among Spark operator With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format. Apache Arrow based Native Readers for Parquet and other formats A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Apache Arrow Compute/Gandiva based operators We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch. Native Columnar Shuffle Operator with efficient compression support We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here How to use OAP: Gazelle Plugin There are three ways to use OAP: Gazelle Plugin, 1. Use precompiled jars 2. Building by Conda Environment 3. Building by Yourself Use precompiled jars Please go to OAP's Maven Central Repository to find Gazelle Plugin jars. For usage, you will require below two jar files: 1. spark-arrow-datasource-standard- -jar-with-dependencies.jar is located in com/intel/oap/spark-arrow-datasource-standard/ / 2. spark-columnar-core- -jar-with-dependencies.jar is located in com/intel/oap/spark-columnar-core/ / Please notice the files are fat jars shipped with our custom Arrow library and pre-compiled from our server(using GCC 9.3.0 and LLVM 7.0.1), which means you will require to pre-install GCC 9.3.0 and LLVM 7.0.1 in your system for normal usage. Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started . Building by yourself If you prefer to build from the source code on your hand, please follow below steps to set up your environment. Prerequisite There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node. Installation Please check the document Installation Guide Get started To enable Gazelle Plugin, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. More Configuration, please check the document Configuration Guide Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if Gazelle Plugin works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should showup on Spark console and you can check the DAG diagram with some Columnar Processing stage. Gazelle Plugin still lacks some features, please check out the limitations . Could/K8s Integration Google Cloud Dataproc Gazelle Plugin now supports to run on Dataproc 2.0, we provide a guide to help quickly install Gazelle Plugin and run TPC-DS with notebooks or scripts. Please refer to Gazelle_on_Dataproc to find details about: Create a cluster on Dataproc 2.0 with initialization actions. Gazelle Plugin jars compiled with -Pdataproc-2.0 parameter will installed by Conda in all cluster nodes. Config for enabling Gazelle Plugin. Run TPC-DS with notebooks or scripts. Performance data For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin. Memory allocation The memory usage in Gazelle Plugin is high. The allocations goes to two parts: 1) Java based allocation which is widely used in Arrow Java API. 2) Native side memory allocation used in each native kernel. We investigated the memory allocation behavior and made more turnings here , the memroy footprint is stable during a TPC-DS power run. Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact chendi.xue@intel.com binwei.yang@intel.com","title":"User Guide"},{"location":"User-Guide/#gazelle-plugin","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations","title":"Gazelle Plugin"},{"location":"User-Guide/#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL.","title":"Introduction"},{"location":"User-Guide/#key-features","text":"","title":"Key Features"},{"location":"User-Guide/#apache-arrow-formatted-intermediate-data-among-spark-operator","text":"With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format.","title":"Apache Arrow formatted intermediate data among Spark operator"},{"location":"User-Guide/#apache-arrow-based-native-readers-for-parquet-and-other-formats","text":"A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source","title":"Apache Arrow based Native Readers for Parquet and other formats"},{"location":"User-Guide/#apache-arrow-computegandiva-based-operators","text":"We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch.","title":"Apache Arrow Compute/Gandiva based operators"},{"location":"User-Guide/#native-columnar-shuffle-operator-with-efficient-compression-support","text":"We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here","title":"Native Columnar Shuffle Operator with efficient compression support"},{"location":"User-Guide/#how-to-use-oap-gazelle-plugin","text":"There are three ways to use OAP: Gazelle Plugin, 1. Use precompiled jars 2. Building by Conda Environment 3. Building by Yourself","title":"How to use OAP: Gazelle Plugin"},{"location":"User-Guide/#use-precompiled-jars","text":"Please go to OAP's Maven Central Repository to find Gazelle Plugin jars. For usage, you will require below two jar files: 1. spark-arrow-datasource-standard- -jar-with-dependencies.jar is located in com/intel/oap/spark-arrow-datasource-standard/ / 2. spark-columnar-core- -jar-with-dependencies.jar is located in com/intel/oap/spark-columnar-core/ / Please notice the files are fat jars shipped with our custom Arrow library and pre-compiled from our server(using GCC 9.3.0 and LLVM 7.0.1), which means you will require to pre-install GCC 9.3.0 and LLVM 7.0.1 in your system for normal usage.","title":"Use precompiled jars"},{"location":"User-Guide/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started .","title":"Building by Conda"},{"location":"User-Guide/#building-by-yourself","text":"If you prefer to build from the source code on your hand, please follow below steps to set up your environment.","title":"Building by yourself"},{"location":"User-Guide/#prerequisite","text":"There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node.","title":"Prerequisite"},{"location":"User-Guide/#installation","text":"Please check the document Installation Guide","title":"Installation"},{"location":"User-Guide/#get-started","text":"To enable Gazelle Plugin, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. More Configuration, please check the document Configuration Guide Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if Gazelle Plugin works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should showup on Spark console and you can check the DAG diagram with some Columnar Processing stage. Gazelle Plugin still lacks some features, please check out the limitations .","title":"Get started"},{"location":"User-Guide/#couldk8s-integration","text":"","title":"Could/K8s Integration"},{"location":"User-Guide/#google-cloud-dataproc","text":"Gazelle Plugin now supports to run on Dataproc 2.0, we provide a guide to help quickly install Gazelle Plugin and run TPC-DS with notebooks or scripts. Please refer to Gazelle_on_Dataproc to find details about: Create a cluster on Dataproc 2.0 with initialization actions. Gazelle Plugin jars compiled with -Pdataproc-2.0 parameter will installed by Conda in all cluster nodes. Config for enabling Gazelle Plugin. Run TPC-DS with notebooks or scripts.","title":"Google Cloud Dataproc"},{"location":"User-Guide/#performance-data","text":"For advanced performance testing, below charts show the results by using two benchmarks: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. All the testing environment for Decision Support Benchmark1&2 are using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin.","title":"Performance data"},{"location":"User-Guide/#memory-allocation","text":"The memory usage in Gazelle Plugin is high. The allocations goes to two parts: 1) Java based allocation which is widely used in Arrow Java API. 2) Native side memory allocation used in each native kernel. We investigated the memory allocation behavior and made more turnings here , the memroy footprint is stable during a TPC-DS power run.","title":"Memory allocation"},{"location":"User-Guide/#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"User-Guide/#contact","text":"chendi.xue@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"limitations/","text":"Limitations for Gazelle Plugin Spark compability Gazelle Plugin currenlty works with Spark 3.0.0 only. There are still some trouble with latest Shuffle/AQE API from Spark 3.0.1, 3.0.2 or 3.1.x. Operator limitations All performance critical operators in TPC-H/TPC-DS should be supported. For those unsupported operators, Gazelle Plugin will automatically fallback to row operators in vanilla Spark. Columnar Projection with Filter We used 16 bit selection vector for filter so the max batch size need to be < 65536 Columnar Sort Columnar Sort does not support spill to disk yet. To reduce the peak memory usage, we used smaller data structure(uin16_t), so this limits - the max batch size to be < 65536 - the number of batches in one partiton to be < 65536","title":"Limitations for Gazelle Plugin"},{"location":"limitations/#limitations-for-gazelle-plugin","text":"","title":"Limitations for Gazelle Plugin"},{"location":"limitations/#spark-compability","text":"Gazelle Plugin currenlty works with Spark 3.0.0 only. There are still some trouble with latest Shuffle/AQE API from Spark 3.0.1, 3.0.2 or 3.1.x.","title":"Spark compability"},{"location":"limitations/#operator-limitations","text":"All performance critical operators in TPC-H/TPC-DS should be supported. For those unsupported operators, Gazelle Plugin will automatically fallback to row operators in vanilla Spark.","title":"Operator limitations"},{"location":"limitations/#columnar-projection-with-filter","text":"We used 16 bit selection vector for filter so the max batch size need to be < 65536","title":"Columnar Projection with Filter"},{"location":"limitations/#columnar-sort","text":"Columnar Sort does not support spill to disk yet. To reduce the peak memory usage, we used smaller data structure(uin16_t), so this limits - the max batch size to be < 65536 - the number of batches in one partiton to be < 65536","title":"Columnar Sort"},{"location":"memory/","text":"Memory allocation in Gazelle Plugin Java memory allocation By default, Arrow columnar vector Java API is using netty pooledbytebuffer allocator , which will try to hold on the \"free memory\" by not returning back to System immediately for better performance. This will result big memory footprint on operators relying on this API, e.g., CoalesceBatches . We changed to use unsafe API since 1.2 release, which means the freed memory will be returned to system directly. Performance tests showed the performance of this change is negatable. Native memory allocation Modern memory allocators like jemalloc will not return the just freed memory to system to achieve better performance, also a set of memory allocation pools will be used to reduce the lock contention. Recent version of glibc also used similar design - by default it has 8 * cores pools, each with 64MB size. This will introduce a big memory footprint. Jemalloc's pool is smaller relatively - it has 4 * cores pools, each with 2MB size. Both glibc/jemalloc provides turning knobs to control these behaviors. Using lower number of pools can reduce the memory footprint, but also may impact the performance. It's a tradeoff on memory vs. performance. Turnings to reduce memory footprint Gazelle Plugin 1.2+ Using jemalloc in Arrow build -DARROW_USEJEMALLOC=True Build Gazelle Plugin with arrow-unsafe profile mvn clean pacakge -DskipTests -Parrow-unsafe LD_PRELOAD jemalloc in each Spark executor --conf spark.executorEnv.LD_PRELOAD=/path/to/libjemalloc.so Reduce number memory pools in jemalloc --conf spark.executorEnv.MALLOC_CONF=narenas:2 Reduce number memory pools in glibc --conf spark.executorEnv.MALLOC_ARENA_MAX=2","title":"Memory allocation in Gazelle Plugin"},{"location":"memory/#memory-allocation-in-gazelle-plugin","text":"","title":"Memory allocation in Gazelle Plugin"},{"location":"memory/#java-memory-allocation","text":"By default, Arrow columnar vector Java API is using netty pooledbytebuffer allocator , which will try to hold on the \"free memory\" by not returning back to System immediately for better performance. This will result big memory footprint on operators relying on this API, e.g., CoalesceBatches . We changed to use unsafe API since 1.2 release, which means the freed memory will be returned to system directly. Performance tests showed the performance of this change is negatable.","title":"Java memory allocation"},{"location":"memory/#native-memory-allocation","text":"Modern memory allocators like jemalloc will not return the just freed memory to system to achieve better performance, also a set of memory allocation pools will be used to reduce the lock contention. Recent version of glibc also used similar design - by default it has 8 * cores pools, each with 64MB size. This will introduce a big memory footprint. Jemalloc's pool is smaller relatively - it has 4 * cores pools, each with 2MB size. Both glibc/jemalloc provides turning knobs to control these behaviors. Using lower number of pools can reduce the memory footprint, but also may impact the performance. It's a tradeoff on memory vs. performance.","title":"Native memory allocation"},{"location":"memory/#turnings-to-reduce-memory-footprint","text":"Gazelle Plugin 1.2+ Using jemalloc in Arrow build -DARROW_USEJEMALLOC=True Build Gazelle Plugin with arrow-unsafe profile mvn clean pacakge -DskipTests -Parrow-unsafe LD_PRELOAD jemalloc in each Spark executor --conf spark.executorEnv.LD_PRELOAD=/path/to/libjemalloc.so Reduce number memory pools in jemalloc --conf spark.executorEnv.MALLOC_CONF=narenas:2 Reduce number memory pools in glibc --conf spark.executorEnv.MALLOC_ARENA_MAX=2","title":"Turnings to reduce memory footprint"},{"location":"operators/","text":"No. Executor Description Notes BOOLEAN BYTE SHORT INT LONG FLOAT DOUBLE STRING DECIMAL DATE TIMESTAMP NULL BINARY CALENDAR ARRAY MAP STRUCT UDT 1 CoalesceExec y y y y y y y y y y 2 CollectLimitExec using row version 3 ExpandExec y y y y y y y y y y 4 FileSourceScanExec y y y y y y y y y y 5 FilterExec y y y y y y y y y y 6 GenerateExec using row version 7 GlobalLimitExec using row version 8 LocalLimitExec 9 ProjectExec y y y y y y y y y y 10 RangeExec using row version 11 SortExec y y y y y y y y y y 12 TakeOrderedAndPorjectExec using row version 13 UnionExec y y y y y y y y y y 14 CustomShuffleReaderExec y y y y y y y y y y 15 HashAggregateExec y y y y y y y y y y 16 SortAggregateExec y y y y y y y y y y 17 DataWritingCommandExec using row version 18 BatchScanExec y y y y y y y y y y 19 BroadcastExchangeExec y y y y y y y y y y 20 ShuffleExchangeExec y y y y y y y y y y 21 BroadcastHashJoinExec y y y y y y y y y y 22 BroadcastNestedLoopJoinExec using row version 23 CartesianProductExec using row version 24 ShuffledHashJoinExec y y y y y y y y y y 25 SortMergeJoinExec y y y y y y y y y y 26 ArrowEvalPythonExec using row version 27 WindowINPandasExec using row version 28 WindowExec y y y y y y y y y y","title":"Operators"}]}