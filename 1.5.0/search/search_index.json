{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gazelle Plugin A Native Engine for Spark SQL with vectorized SIMD optimizations. Please refer to user guide for details on how to enable Gazelle. Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL. Performance data For advanced performance testing, below charts show the results by using two benchmarks with Gazelle v1.1: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. The testing environment for Decision Support Benchmark1 is using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset and parquet format. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. The testing environment for Decision Support Benchmark2 is using 1 master + 3 workers and Intel(r) Xeon(r) Platinum 8360Y CPU|1440GB memory|NVMe SSD x4 per single node with 3TB dataset and parquet format. * Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please note that the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin. Please check the detailed page on performance tuning for TPC-H and TPC-DS workloads. Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact weiting.chen@intel.com binwei.yang@intel.com","title":"Gazelle Plugin"},{"location":"#gazelle-plugin","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations. Please refer to user guide for details on how to enable Gazelle.","title":"Gazelle Plugin"},{"location":"#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL.","title":"Introduction"},{"location":"#performance-data","text":"For advanced performance testing, below charts show the results by using two benchmarks with Gazelle v1.1: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. The testing environment for Decision Support Benchmark1 is using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset and parquet format. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. The testing environment for Decision Support Benchmark2 is using 1 master + 3 workers and Intel(r) Xeon(r) Platinum 8360Y CPU|1440GB memory|NVMe SSD x4 per single node with 3TB dataset and parquet format. * Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please note that the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin. Please check the detailed page on performance tuning for TPC-H and TPC-DS workloads.","title":"Performance data"},{"location":"#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"#contact","text":"weiting.chen@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"ApacheArrowInstallation/","text":"llvm-7.0: Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install cmake: Please make sure your cmake version is qualified based on the prerequisite. Arrow git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-4.0.0-oap mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_ORC=ON -DARROW_CSV=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni After arrow installed in the specific directory, please make sure to set up -Dbuild_arrow=OFF -Darrow_root=/path/to/arrow when building Gazelle Plugin.","title":"ApacheArrowInstallation"},{"location":"ApacheArrowInstallation/#llvm-70","text":"Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install","title":"llvm-7.0:"},{"location":"ApacheArrowInstallation/#cmake","text":"Please make sure your cmake version is qualified based on the prerequisite.","title":"cmake:"},{"location":"ApacheArrowInstallation/#arrow","text":"git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-4.0.0-oap mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_ORC=ON -DARROW_CSV=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni After arrow installed in the specific directory, please make sure to set up -Dbuild_arrow=OFF -Darrow_root=/path/to/arrow when building Gazelle Plugin.","title":"Arrow"},{"location":"Columnar-Expression-Developer-Guide/","text":"Columnar Expression Developer Guide This is a short guide on adding new columnar expressions in Gazelle. Currently, the columnar expressions in Gazelle are implemented based on Arrow/gandiva. Developer needs to implement a columnar expression class in Gazelle scala code and also add some logic to replace Spark expression with the implemented columnar expression. And the native code is implemented in Arrow/gandiva for expression's core functionality. Before native code development, we should check whether the desired function is already implemented in gandiva. If so, we can directly use it or just make a few code changes to meet the compatibility with Spark. Take regexp_extract as example. Arrow/gandiva Native Code functions need to use external C++ libs See arrow/pull/97 . Since C++ lib google/RE2 is leveraged, we implemented the core function in a gandiva function holder. In extract_holder.h , we need declare the below functions other than some other necessary functions. static Status Make(const FunctionNode &node, std::shared_ptr<ExtractHolder> *holder); This function is used to construct ExtractHolder and check the legality of input if needed. It will be called by function_holder_registry.h to register the function holder. In gdv_function_stubs.cc \uff0cwe need to implement a function called gdv_fn_regexp_extract_utf8_utf8_int32 , which will invoke overloaded operator() in ExtractHolder . The operator() is the core function to do the extract work. We need also register regexp_extract in function_registry_string.cc (for functions handling strings). This exposed function name will be used to create function tree in Gazelle scala code. In this case, function holder is required, so we should specify NativeFunction::kNeedsFunctionHolder in the registry. functions does not need to use external C++ libs see arrow/pull/103 The pmod function uses standard C libs only so it can be precompiled in Gandiva. The idea is similar with adding functions using external libs: - adding the function implemenation in precompile/xxx.c - register the function pointer in the function registry Here's also one detailed guide from Dremio For unit test, please refer to extract_holder_test.cc . Here is the compile steps. cd arrow/cpp/release-build (create by yourself if not exists) cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_CSV=ON -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_WITH_SNAPPY=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON -DARROW_WITH_PROTOBUF=ON -DARROW_DATASET=ON -DARROW_IPC=ON -DARROW_WITH_LZ4=ON -DARROW_JEMALLOC=OFF -DARROW_BUILD_TESTS=ON .. make -j ./release/gandiva-internals-test Gazelle Scala Code See gazelle_plugin/pull/847 . In scala code, ColumnarRegExpExtract is created to replace Spark's RegExpExtract . We can add buildCheck to check the input types. For legal types currently not supported in this implementation, we can throw an UnsupportedOperationException to let the expression fallback. In doColumnarCodeGen , arrow function node is constructed with gandiva function name regexp_extract specified. The supportColumnarCodegen function is used to check if columnar wholestage codegen support is added, please set to false if it's not implemented. At last, in replaceWithColumnarExpression of ColumnarExpressionConverter.scala \uff0cwe need to replace Spark's expression to the implemented columnar expression.","title":"Columnar Expression Developer Guide"},{"location":"Columnar-Expression-Developer-Guide/#columnar-expression-developer-guide","text":"This is a short guide on adding new columnar expressions in Gazelle. Currently, the columnar expressions in Gazelle are implemented based on Arrow/gandiva. Developer needs to implement a columnar expression class in Gazelle scala code and also add some logic to replace Spark expression with the implemented columnar expression. And the native code is implemented in Arrow/gandiva for expression's core functionality. Before native code development, we should check whether the desired function is already implemented in gandiva. If so, we can directly use it or just make a few code changes to meet the compatibility with Spark. Take regexp_extract as example.","title":"Columnar Expression Developer Guide"},{"location":"Columnar-Expression-Developer-Guide/#arrowgandiva-native-code","text":"","title":"Arrow/gandiva Native Code"},{"location":"Columnar-Expression-Developer-Guide/#functions-need-to-use-external-c-libs","text":"See arrow/pull/97 . Since C++ lib google/RE2 is leveraged, we implemented the core function in a gandiva function holder. In extract_holder.h , we need declare the below functions other than some other necessary functions. static Status Make(const FunctionNode &node, std::shared_ptr<ExtractHolder> *holder); This function is used to construct ExtractHolder and check the legality of input if needed. It will be called by function_holder_registry.h to register the function holder. In gdv_function_stubs.cc \uff0cwe need to implement a function called gdv_fn_regexp_extract_utf8_utf8_int32 , which will invoke overloaded operator() in ExtractHolder . The operator() is the core function to do the extract work. We need also register regexp_extract in function_registry_string.cc (for functions handling strings). This exposed function name will be used to create function tree in Gazelle scala code. In this case, function holder is required, so we should specify NativeFunction::kNeedsFunctionHolder in the registry.","title":"functions need to use external C++ libs"},{"location":"Columnar-Expression-Developer-Guide/#functions-does-not-need-to-use-external-c-libs","text":"see arrow/pull/103 The pmod function uses standard C libs only so it can be precompiled in Gandiva. The idea is similar with adding functions using external libs: - adding the function implemenation in precompile/xxx.c - register the function pointer in the function registry Here's also one detailed guide from Dremio For unit test, please refer to extract_holder_test.cc . Here is the compile steps. cd arrow/cpp/release-build (create by yourself if not exists) cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_CSV=ON -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_WITH_SNAPPY=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON -DARROW_WITH_PROTOBUF=ON -DARROW_DATASET=ON -DARROW_IPC=ON -DARROW_WITH_LZ4=ON -DARROW_JEMALLOC=OFF -DARROW_BUILD_TESTS=ON .. make -j ./release/gandiva-internals-test","title":"functions does not need to use external C++ libs"},{"location":"Columnar-Expression-Developer-Guide/#gazelle-scala-code","text":"See gazelle_plugin/pull/847 . In scala code, ColumnarRegExpExtract is created to replace Spark's RegExpExtract . We can add buildCheck to check the input types. For legal types currently not supported in this implementation, we can throw an UnsupportedOperationException to let the expression fallback. In doColumnarCodeGen , arrow function node is constructed with gandiva function name regexp_extract specified. The supportColumnarCodegen function is used to check if columnar wholestage codegen support is added, please set to false if it's not implemented. At last, in replaceWithColumnarExpression of ColumnarExpressionConverter.scala \uff0cwe need to replace Spark's expression to the implemented columnar expression.","title":"Gazelle Scala Code"},{"location":"Columnar-UDF-Development-Guide/","text":"Columnar UDF Development Guide Developer can implement columnar Hive UDF or scala UDF in Gazelle for performance benefits. The original UDF still needs to be registered to Spark for two reasons: 1) Spark will create a kind of expression for the registered UDF, which makes expression replacement possible. 2) expression fallback still need work well for cases that the expression tree has some expression currently unsupported by Gazelle. Hive UDF Suppose there is a UDF for handling string type input. package com.intel.test; import org.apache.hadoop.hive.ql.exec.UDF; public class MyHiveUDF extends UDF { public String evaluate(String b) { ... } } The jar contains this class should be put into Spark class path. Then, the UDF can be registered to Spark at runtime as the below shows. spark.sql(\"CREATE TEMPORARY FUNCTION MyHiveUDFName AS 'com.intel.test.MyHiveUDF';\") MyHiveUDFName is the unique name for UDF and it's case insensitive to Spark & Gazelle. We need to implement a native version of evaluate function in arrow/gandiva . And in Gazelle ColumnarExpressionConverter.scala , we need to add some logic to find the expression whose pretty name is MyHiveUDFName , and replace it with the implemented columnar expression. The columnar expression will call gandiva function for evaluating given input. Scala UDF In Gazelle, the implementation for scala UDF is similar to Hive UDF. There is only few difference in finding the matched expression for replacing it. See code details in ColumnarExpressionConverter.scala . We still need to register the original scala UDF to Spark, e.g., spark.udf.register(\"MyScalaUDFName\", (s : String) => s) It is also required to implement a columnar expression and a gandiva function, similar to Hive UDF. Reference to Developer Support a UDF: URLDecoder","title":"Columnar UDF Development"},{"location":"Columnar-UDF-Development-Guide/#columnar-udf-development-guide","text":"Developer can implement columnar Hive UDF or scala UDF in Gazelle for performance benefits. The original UDF still needs to be registered to Spark for two reasons: 1) Spark will create a kind of expression for the registered UDF, which makes expression replacement possible. 2) expression fallback still need work well for cases that the expression tree has some expression currently unsupported by Gazelle.","title":"Columnar UDF Development Guide"},{"location":"Columnar-UDF-Development-Guide/#hive-udf","text":"Suppose there is a UDF for handling string type input. package com.intel.test; import org.apache.hadoop.hive.ql.exec.UDF; public class MyHiveUDF extends UDF { public String evaluate(String b) { ... } } The jar contains this class should be put into Spark class path. Then, the UDF can be registered to Spark at runtime as the below shows. spark.sql(\"CREATE TEMPORARY FUNCTION MyHiveUDFName AS 'com.intel.test.MyHiveUDF';\") MyHiveUDFName is the unique name for UDF and it's case insensitive to Spark & Gazelle. We need to implement a native version of evaluate function in arrow/gandiva . And in Gazelle ColumnarExpressionConverter.scala , we need to add some logic to find the expression whose pretty name is MyHiveUDFName , and replace it with the implemented columnar expression. The columnar expression will call gandiva function for evaluating given input.","title":"Hive UDF"},{"location":"Columnar-UDF-Development-Guide/#scala-udf","text":"In Gazelle, the implementation for scala UDF is similar to Hive UDF. There is only few difference in finding the matched expression for replacing it. See code details in ColumnarExpressionConverter.scala . We still need to register the original scala UDF to Spark, e.g., spark.udf.register(\"MyScalaUDFName\", (s : String) => s) It is also required to implement a columnar expression and a gandiva function, similar to Hive UDF.","title":"Scala UDF"},{"location":"Columnar-UDF-Development-Guide/#reference-to-developer","text":"Support a UDF: URLDecoder","title":"Reference to Developer"},{"location":"Configuration/","text":"Spark Configurations for Gazelle Plugin There are many configuration could impact the Gazelle Plugin performance and can be fine tune in Spark. You can add these configuration into spark-defaults.conf to enable or disable the setting. Parameters Description Recommend Setting spark.driver.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Driver /path/to/jar_file1:/path/to/jar_file2 spark.executor.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Executor /path/to/jar_file1:/path/to/jar_file2 spark.executorEnv.LIBARROW_DIR To set up the location of Arrow library, by default it will search the loation of jar to be uncompressed /path/to/arrow_library/ spark.executorEnv.CC To set up the location of gcc /path/to/gcc/ spark.executor.memory To set up how much memory to be used for Spark Executor. spark.memory.offHeap.size To set up how much memory to be used for Java OffHeap. Please notice Gazelle Plugin will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Gazelle Plugin 30G spark.sql.sources.useV1SourceList Choose to use V1 source avro spark.sql.join.preferSortMergeJoin To turn on/off preferSortMergeJoin in Spark. In gazelle we recomend to turn off this to get better performance true spark.plugins To turn on Gazelle Plugin com.intel.oap.GazellePlugin spark.shuffle.manager To turn on Gazelle Columnar Shuffle Plugin org.apache.spark.shuffle.sort.ColumnarShuffleManager spark.sql.shuffle.partitions shuffle partition size, it's recomended to use the same number of your total cores 200 spark.oap.sql.columnar.batchscan Enable or Disable Columnar Batchscan, default is true true spark.oap.sql.columnar.hashagg Enable or Disable Columnar Hash Aggregate, default is true true spark.oap.sql.columnar.projfilter Enable or Disable Columnar Project and Filter, default is true true spark.oap.sql.columnar.codegen.sort Enable or Disable Columnar Sort, default is true true spark.oap.sql.columnar.window Enable or Disable Columnar Window, default is true true spark.oap.sql.columnar.shuffledhashjoin Enable or Disable ShffuledHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin Enable or Disable Columnar Sort Merge Join, default is true true spark.oap.sql.columnar.union Enable or Disable Columnar Union, default is true true spark.oap.sql.columnar.expand Enable or Disable Columnar Expand, default is true true spark.oap.sql.columnar.broadcastexchange Enable or Disable Columnar Broadcast Exchange, default is true true spark.oap.sql.columnar.nanCheck Enable or Disable Nan Check, default is true true spark.oap.sql.columnar.hashCompare Enable or Disable Hash Compare in HashJoins or HashAgg, default is true true spark.oap.sql.columnar.broadcastJoin Enable or Disable Columnar BradcastHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin.lazyread Enable or Disable lazy reading on Sort result. On disable, whole partition will be cached before doing SortMergeJoin false spark.oap.sql.columnar.wholestagecodegen Enable or Disable Columnar WholeStageCodeGen, default is true true spark.oap.sql.columnar.preferColumnar Enable or Disable Columnar Operators, default is false. This parameter could impact the performance in different case. In some cases, to set false can get some performance boost. false spark.oap.sql.columnar.joinOptimizationLevel Fallback to row operators if there are several continous joins 18 spark.sql.execution.arrow.maxRecordsPerBatch Set up the Max Records per Batch 10000 spark.oap.sql.execution.sort.spillThreshold Set up the Max sort in memory threshold in bytes, default is disabled -1 spark.oap.sql.columnar.wholestagecodegen.breakdownTime Enable or Disable metrics in Columnar WholeStageCodeGen false spark.oap.sql.columnar.tmp_dir Set up a folder to store the codegen files, default is disabled \"\" spark.oap.sql.columnar.shuffle.customizedCompression.codec Set up the codec to be used for Columnar Shuffle, default is lz4. The other option is fastpfor which can bring better perf on compressing fixed-size based contents like int lz4 spark.oap.sql.columnar.numaBinding Set up NUMABinding, default is false true spark.oap.sql.columnar.coreRange Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system(lscpu grep node[0-4]). Use 72 cores as an example. Example thrift-server configuration Here's one example of the thrift-server configuration THRIFTSERVER_CONFIG=\"--name ${runname} --num-executors 72 --driver-memory 20g --executor-memory 6g --executor-cores 6 --master yarn --deploy-mode client --conf spark.executor.memoryOverhead=384 --conf spark.executorEnv.CC=/home/sparkuser/miniconda3/envs/arrow-new/bin/gcc --conf spark.plugins=com.intel.oap.GazellePlugin --conf spark.executorEnv.LD_LIBRARY_PATH=/home/sparkuser/miniconda3/envs/arrow-new/lib/:/home/sparkuser/miniconda3/envs/arrow-new/lib64/ --conf spark.executorEnv.LIBARROW_DIR=/home/sparkuser/miniconda3/envs/arrow-new --conf spark.driver.extraClassPath=${nativesql_jars} --conf spark.executor.extraClassPath=${nativesql_jars} --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager --conf spark.sql.join.preferSortMergeJoin=false --conf spark.sql.inMemoryColumnarStorage.batchSize=${batchsize} --conf spark.sql.execution.arrow.maxRecordsPerBatch=${batchsize} --conf spark.sql.parquet.columnarReaderBatchSize=${batchsize} --conf spark.sql.autoBroadcastJoinThreshold=10M --conf spark.sql.broadcastTimeout=600 --conf spark.sql.crossJoin.enabled=true --conf spark.driver.maxResultSize=20g --hiveconf hive.server2.thrift.port=10001 --hiveconf hive.server2.thrift.bind.host=sr270 --conf spark.sql.codegen.wholeStage=true --conf spark.sql.shuffle.partitions=432 --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=15g --conf spark.kryoserializer.buffer.max=128m --conf spark.kryoserializer.buffer=32m --conf spark.oap.sql.columnar.preferColumnar=false --conf spark.oap.sql.columnar.sortmergejoin.lazyread=true --conf spark.sql.execution.sort.spillThreshold=2147483648 --conf spark.executorEnv.LD_PRELOAD=/home/sparkuser/miniconda3/envs/arrow-new/lib/libjemalloc.so --conf spark.executorEnv.MALLOC_CONF=background_thread:true,dirty_decay_ms:0,muzzy_decay_ms:0,narenas:2 --conf spark.executorEnv.MALLOC_ARENA_MAX=2 --conf spark.oap.sql.columnar.numaBinding=true --conf spark.oap.sql.columnar.coreRange=0-35,72-107|36-71,108-143 --conf spark.oap.sql.columnar.joinOptimizationLevel=18 --conf spark.oap.sql.columnar.shuffle.customizedCompression.codec=lz4 --conf spark.yarn.appMasterEnv.LD_PRELOAD=/home/sparkuser/miniconda3/envs/arrow-new/lib/libjemalloc.so\" Below is an example for spark-default.conf, if you are using conda to install OAP project. Example spark-defaults.conf spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.plugins com.intel.oap.GazellePlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note Gazelle Plugin depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/ Notes on driver In gazelle spark driver is used to C++ code generation for different operators. This means driver takes more tasks than vanilla Spark, so it's better to consider allocate more resource to driver. By default, driver will compile C++ codes with best optimizations targeting for local CPU architecture: -O3 -march=native This could be override by a local environment variable before starting driver: export CODEGEN_OPTION=\" -O1 -mavx2 -fno-semantic-interposition \"","title":"Configuration"},{"location":"Configuration/#spark-configurations-for-gazelle-plugin","text":"There are many configuration could impact the Gazelle Plugin performance and can be fine tune in Spark. You can add these configuration into spark-defaults.conf to enable or disable the setting. Parameters Description Recommend Setting spark.driver.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Driver /path/to/jar_file1:/path/to/jar_file2 spark.executor.extraClassPath To add Arrow Data Source and Gazelle Plugin jar file in Spark Executor /path/to/jar_file1:/path/to/jar_file2 spark.executorEnv.LIBARROW_DIR To set up the location of Arrow library, by default it will search the loation of jar to be uncompressed /path/to/arrow_library/ spark.executorEnv.CC To set up the location of gcc /path/to/gcc/ spark.executor.memory To set up how much memory to be used for Spark Executor. spark.memory.offHeap.size To set up how much memory to be used for Java OffHeap. Please notice Gazelle Plugin will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Gazelle Plugin 30G spark.sql.sources.useV1SourceList Choose to use V1 source avro spark.sql.join.preferSortMergeJoin To turn on/off preferSortMergeJoin in Spark. In gazelle we recomend to turn off this to get better performance true spark.plugins To turn on Gazelle Plugin com.intel.oap.GazellePlugin spark.shuffle.manager To turn on Gazelle Columnar Shuffle Plugin org.apache.spark.shuffle.sort.ColumnarShuffleManager spark.sql.shuffle.partitions shuffle partition size, it's recomended to use the same number of your total cores 200 spark.oap.sql.columnar.batchscan Enable or Disable Columnar Batchscan, default is true true spark.oap.sql.columnar.hashagg Enable or Disable Columnar Hash Aggregate, default is true true spark.oap.sql.columnar.projfilter Enable or Disable Columnar Project and Filter, default is true true spark.oap.sql.columnar.codegen.sort Enable or Disable Columnar Sort, default is true true spark.oap.sql.columnar.window Enable or Disable Columnar Window, default is true true spark.oap.sql.columnar.shuffledhashjoin Enable or Disable ShffuledHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin Enable or Disable Columnar Sort Merge Join, default is true true spark.oap.sql.columnar.union Enable or Disable Columnar Union, default is true true spark.oap.sql.columnar.expand Enable or Disable Columnar Expand, default is true true spark.oap.sql.columnar.broadcastexchange Enable or Disable Columnar Broadcast Exchange, default is true true spark.oap.sql.columnar.nanCheck Enable or Disable Nan Check, default is true true spark.oap.sql.columnar.hashCompare Enable or Disable Hash Compare in HashJoins or HashAgg, default is true true spark.oap.sql.columnar.broadcastJoin Enable or Disable Columnar BradcastHashJoin, default is true true spark.oap.sql.columnar.sortmergejoin.lazyread Enable or Disable lazy reading on Sort result. On disable, whole partition will be cached before doing SortMergeJoin false spark.oap.sql.columnar.wholestagecodegen Enable or Disable Columnar WholeStageCodeGen, default is true true spark.oap.sql.columnar.preferColumnar Enable or Disable Columnar Operators, default is false. This parameter could impact the performance in different case. In some cases, to set false can get some performance boost. false spark.oap.sql.columnar.joinOptimizationLevel Fallback to row operators if there are several continous joins 18 spark.sql.execution.arrow.maxRecordsPerBatch Set up the Max Records per Batch 10000 spark.oap.sql.execution.sort.spillThreshold Set up the Max sort in memory threshold in bytes, default is disabled -1 spark.oap.sql.columnar.wholestagecodegen.breakdownTime Enable or Disable metrics in Columnar WholeStageCodeGen false spark.oap.sql.columnar.tmp_dir Set up a folder to store the codegen files, default is disabled \"\" spark.oap.sql.columnar.shuffle.customizedCompression.codec Set up the codec to be used for Columnar Shuffle, default is lz4. The other option is fastpfor which can bring better perf on compressing fixed-size based contents like int lz4 spark.oap.sql.columnar.numaBinding Set up NUMABinding, default is false true spark.oap.sql.columnar.coreRange Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system(lscpu grep node[0-4]). Use 72 cores as an example.","title":"Spark Configurations for Gazelle Plugin"},{"location":"Configuration/#example-thrift-server-configuration","text":"Here's one example of the thrift-server configuration THRIFTSERVER_CONFIG=\"--name ${runname} --num-executors 72 --driver-memory 20g --executor-memory 6g --executor-cores 6 --master yarn --deploy-mode client --conf spark.executor.memoryOverhead=384 --conf spark.executorEnv.CC=/home/sparkuser/miniconda3/envs/arrow-new/bin/gcc --conf spark.plugins=com.intel.oap.GazellePlugin --conf spark.executorEnv.LD_LIBRARY_PATH=/home/sparkuser/miniconda3/envs/arrow-new/lib/:/home/sparkuser/miniconda3/envs/arrow-new/lib64/ --conf spark.executorEnv.LIBARROW_DIR=/home/sparkuser/miniconda3/envs/arrow-new --conf spark.driver.extraClassPath=${nativesql_jars} --conf spark.executor.extraClassPath=${nativesql_jars} --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager --conf spark.sql.join.preferSortMergeJoin=false --conf spark.sql.inMemoryColumnarStorage.batchSize=${batchsize} --conf spark.sql.execution.arrow.maxRecordsPerBatch=${batchsize} --conf spark.sql.parquet.columnarReaderBatchSize=${batchsize} --conf spark.sql.autoBroadcastJoinThreshold=10M --conf spark.sql.broadcastTimeout=600 --conf spark.sql.crossJoin.enabled=true --conf spark.driver.maxResultSize=20g --hiveconf hive.server2.thrift.port=10001 --hiveconf hive.server2.thrift.bind.host=sr270 --conf spark.sql.codegen.wholeStage=true --conf spark.sql.shuffle.partitions=432 --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=15g --conf spark.kryoserializer.buffer.max=128m --conf spark.kryoserializer.buffer=32m --conf spark.oap.sql.columnar.preferColumnar=false --conf spark.oap.sql.columnar.sortmergejoin.lazyread=true --conf spark.sql.execution.sort.spillThreshold=2147483648 --conf spark.executorEnv.LD_PRELOAD=/home/sparkuser/miniconda3/envs/arrow-new/lib/libjemalloc.so --conf spark.executorEnv.MALLOC_CONF=background_thread:true,dirty_decay_ms:0,muzzy_decay_ms:0,narenas:2 --conf spark.executorEnv.MALLOC_ARENA_MAX=2 --conf spark.oap.sql.columnar.numaBinding=true --conf spark.oap.sql.columnar.coreRange=0-35,72-107|36-71,108-143 --conf spark.oap.sql.columnar.joinOptimizationLevel=18 --conf spark.oap.sql.columnar.shuffle.customizedCompression.codec=lz4 --conf spark.yarn.appMasterEnv.LD_PRELOAD=/home/sparkuser/miniconda3/envs/arrow-new/lib/libjemalloc.so\" Below is an example for spark-default.conf, if you are using conda to install OAP project.","title":"Example thrift-server configuration"},{"location":"Configuration/#example-spark-defaultsconf","text":"spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.plugins com.intel.oap.GazellePlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note Gazelle Plugin depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/","title":"Example spark-defaults.conf"},{"location":"Configuration/#notes-on-driver","text":"In gazelle spark driver is used to C++ code generation for different operators. This means driver takes more tasks than vanilla Spark, so it's better to consider allocate more resource to driver. By default, driver will compile C++ codes with best optimizations targeting for local CPU architecture: -O3 -march=native This could be override by a local environment variable before starting driver: export CODEGEN_OPTION=\" -O1 -mavx2 -fno-semantic-interposition \"","title":"Notes on driver"},{"location":"Expressions-Compatibility/","text":"Gazelle Expression Compatibility with Apache Spark There are some cases that Gazelle behaves differently from Apache Spark. Here, we list the compatibility issues we have not addressed so far. No. Expression Incompatibility 1 all expressions Incompatibility issue when ANSI is on (throw exceptions at runtime instead of return null). 2 get_json_object Single quote mark is not supported, but only support regular double quote mark. If multiple same keys are contained, null will be returned. But vanilla spark returns the value for the firstly emerged key. 3 from_unixtime Specifying timezone is not supported. By default, return date for UTC, not for local timezone like vanilla spark. 4 date/time related expressions Incompatible behaviors for different LEGACY_TIME_PARSER_POLICY (corrected, exception, legacy). 5 expressions with date format provided. Parsing user-specified date format is not well supported. 6 castINT/castBIGINT/castFLOAT4/castFLOAT8 Return digital part leading in strings like \"123abc\" in WSCG, but vanilla spark return null.","title":"Expressions Compatibility"},{"location":"Expressions-Compatibility/#gazelle-expression-compatibility-with-apache-spark","text":"There are some cases that Gazelle behaves differently from Apache Spark. Here, we list the compatibility issues we have not addressed so far. No. Expression Incompatibility 1 all expressions Incompatibility issue when ANSI is on (throw exceptions at runtime instead of return null). 2 get_json_object Single quote mark is not supported, but only support regular double quote mark. If multiple same keys are contained, null will be returned. But vanilla spark returns the value for the firstly emerged key. 3 from_unixtime Specifying timezone is not supported. By default, return date for UTC, not for local timezone like vanilla spark. 4 date/time related expressions Incompatible behaviors for different LEGACY_TIME_PARSER_POLICY (corrected, exception, legacy). 5 expressions with date format provided. Parsing user-specified date format is not well supported. 6 castINT/castBIGINT/castFLOAT4/castFLOAT8 Return digital part leading in strings like \"123abc\" in WSCG, but vanilla spark return null.","title":"Gazelle Expression Compatibility with Apache Spark"},{"location":"Installation/","text":"Gazelle Plugin Installation For detailed testing scripts, please refer to solution guide Install Googletest and Googlemock yum install gtest-devel yum install gmock Build Gazelle Plugin git clone -b ${version} https://github.com/oap-project/gazelle_plugin.git cd gazelle_plugin mvn clean package -Pspark-3.1 -DskipTests -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Please note two Spark profiles ( spark-3.1 , spark-3.2 ) are provided to build packages with different versions of Spark dependencies. Currently, a few unit tests are not compatible with spark 3.2. So if profile spark-3.2 is used, -Dmaven.test.skip should be added to skip compiling unit tests. mvn clean package -Pspark-3.2 -Dmaven.test.skip -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Based on the different environment, there are some parameters can be set via -D with mvn. Parameters Description Default Value cpp_tests Enable or Disable CPP Tests False build_arrow Build Arrow from Source True arrow_root When build_arrow set to False, arrow_root will be enabled to find the location of your existing arrow library. /usr/local build_protobuf Build Protobuf from Source. If set to False, default library path will be used to find protobuf library. True When build_arrow set to True, the build_arrow.sh will be launched and compile a custom arrow library from OAP Arrow If you wish to change any parameters from Arrow, you can change it from the build_arrow.sh script under gazelle_plugin/arrow-data-source/script/ . Additional Notes Notes for Installation Issues","title":"Installation"},{"location":"Installation/#gazelle-plugin-installation","text":"For detailed testing scripts, please refer to solution guide","title":"Gazelle Plugin Installation"},{"location":"Installation/#install-googletest-and-googlemock","text":"yum install gtest-devel yum install gmock","title":"Install Googletest and Googlemock"},{"location":"Installation/#build-gazelle-plugin","text":"git clone -b ${version} https://github.com/oap-project/gazelle_plugin.git cd gazelle_plugin mvn clean package -Pspark-3.1 -DskipTests -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Please note two Spark profiles ( spark-3.1 , spark-3.2 ) are provided to build packages with different versions of Spark dependencies. Currently, a few unit tests are not compatible with spark 3.2. So if profile spark-3.2 is used, -Dmaven.test.skip should be added to skip compiling unit tests. mvn clean package -Pspark-3.2 -Dmaven.test.skip -Dcpp_tests=OFF -Dbuild_arrow=ON -Dcheckstyle.skip Based on the different environment, there are some parameters can be set via -D with mvn. Parameters Description Default Value cpp_tests Enable or Disable CPP Tests False build_arrow Build Arrow from Source True arrow_root When build_arrow set to False, arrow_root will be enabled to find the location of your existing arrow library. /usr/local build_protobuf Build Protobuf from Source. If set to False, default library path will be used to find protobuf library. True When build_arrow set to True, the build_arrow.sh will be launched and compile a custom arrow library from OAP Arrow If you wish to change any parameters from Arrow, you can change it from the build_arrow.sh script under gazelle_plugin/arrow-data-source/script/ .","title":"Build Gazelle Plugin"},{"location":"Installation/#additional-notes","text":"Notes for Installation Issues","title":"Additional Notes"},{"location":"InstallationNotes/","text":"Notes for Installation Issues Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"InstallationNotes"},{"location":"InstallationNotes/#notes-for-installation-issues","text":"Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"Notes for Installation Issues"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. OAP MLlib Gazelle Plugin Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.5.0 corresponds to all OAP modules' tag version v1.5.0 . Then the dependencies below will be installed: Cmake GCC > 9 OneAPI Arrow LLVM Building Building OAP OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-*.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh Building OAP specific module If you just want to build a specific OAP Module, such as gazelle_plugin , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --component=gazelle_plugin","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. OAP MLlib Gazelle Plugin","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.5.0 corresponds to all OAP modules' tag version v1.5.0 . Then the dependencies below will be installed: Cmake GCC > 9 OneAPI Arrow LLVM","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"","title":"Building"},{"location":"OAP-Developer-Guide/#building-oap_1","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-*.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh","title":"Building OAP"},{"location":"OAP-Developer-Guide/#building-oap-specific-module","text":"If you just want to build a specific OAP Module, such as gazelle_plugin , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --component=gazelle_plugin","title":"Building OAP specific module"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29, CentOS 7.6 (kernel 4.18.16) and Ubuntu 20.04 (kernel 5.4.0-65-generic). Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -c conda-forge -c intel -y oap=1.5.0.spark32 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29, CentOS 7.6 (kernel 4.18.16) and Ubuntu 20.04 (kernel 5.4.0-65-generic). Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -c conda-forge -c intel -y oap=1.5.0.spark32 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"Prerequisite/","text":"Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. GCC 9.0 or higher version LLVM 7.0 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.16 or higher version Maven 3.6.3 or higher version Hadoop 2.7.5 or higher version Spark 3.1.x or Spark 3.2.x Intel Optimized Arrow 4.0.0 GCC installation Please install GCC 9.0 or higher version. Please note for better performance support, GCC 9.0 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download GCC. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. Follow the steps to configure GCC https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure Follow the steps to build GCC https://gcc.gnu.org/install/build.html Follow the steps to install GCC https://gcc.gnu.org/install/finalinstall.html Set up environment for new GCC export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc Verify if GCC has been installed. Use gcc -v command to verify if your gcc version is correct.(Must larger than 9) LLVM 7.0 installation Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing Cmake 3.16.1 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn HADOOP/SPARK Installation If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Intel Optimized Apache Arrow Installation During the mvn compile command, it will launch a script(build_arrow.sh) to help install and compile a Intel custom Arrow library. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Prerequisite"},{"location":"Prerequisite/#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. GCC 9.0 or higher version LLVM 7.0 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.16 or higher version Maven 3.6.3 or higher version Hadoop 2.7.5 or higher version Spark 3.1.x or Spark 3.2.x Intel Optimized Arrow 4.0.0","title":"Prerequisite"},{"location":"Prerequisite/#gcc-installation","text":"Please install GCC 9.0 or higher version. Please note for better performance support, GCC 9.0 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download GCC. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. Follow the steps to configure GCC https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure Follow the steps to build GCC https://gcc.gnu.org/install/build.html Follow the steps to install GCC https://gcc.gnu.org/install/finalinstall.html Set up environment for new GCC export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc Verify if GCC has been installed. Use gcc -v command to verify if your gcc version is correct.(Must larger than 9)","title":"GCC installation"},{"location":"Prerequisite/#llvm-70-installation","text":"Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install","title":"LLVM 7.0 installation"},{"location":"Prerequisite/#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing Cmake 3.16.1 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"Prerequisite/#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"Prerequisite/#hadoopspark-installation","text":"If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation","title":"HADOOP/SPARK Installation"},{"location":"Prerequisite/#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"Prerequisite/#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"Prerequisite/#intel-optimized-apache-arrow-installation","text":"During the mvn compile command, it will launch a script(build_arrow.sh) to help install and compile a Intel custom Arrow library. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Intel Optimized Apache Arrow Installation"},{"location":"SparkInstallation/","text":"Download Spark binary Currently Gazelle Plugin can work on Spark 3.1.x & 3.2.x. Take Spark 3.1.1 as example. wget http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.1.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.1.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.1.1-bin-hadoop3.2/ Build Spark from source Ref. link . git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH} Build Hadoop from source git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"SparkInstallation"},{"location":"SparkInstallation/#download-spark-binary","text":"Currently Gazelle Plugin can work on Spark 3.1.x & 3.2.x. Take Spark 3.1.1 as example. wget http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.1.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.1.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.1.1-bin-hadoop3.2/","title":"Download Spark binary"},{"location":"SparkInstallation/#build-spark-from-source","text":"Ref. link . git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH}","title":"Build Spark from source"},{"location":"SparkInstallation/#build-hadoop-from-source","text":"git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"Build Hadoop from source"},{"location":"User-Guide/","text":"Gazelle Plugin A Native Engine for Spark SQL with vectorized SIMD optimizations Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL. Key Features Apache Arrow formatted intermediate data among Spark operator With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format. Apache Arrow based Native Readers for Parquet and other formats A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Note both data source V1 and V2 are supported. Please check the example section for arrow data source Apache Arrow Compute/Gandiva based operators We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch. Native Columnar Shuffle Operator with efficient compression support We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here How to use OAP: Gazelle Plugin There are three ways to use OAP: Gazelle Plugin, 1. Use precompiled jars 2. Building by Conda Environment 3. Building by Yourself Use precompiled jars Before 1.4.0 release Please go to OAP's Maven Central Repository to find Gazelle Plugin jars. For usage, you will require below two jar files: 1. spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar is located in com/intel/oap/spark-arrow-datasource-standard/ / 2. spark-columnar-core-<version>-jar-with-dependencies.jar is located in com/intel/oap/spark-columnar-core/ / Since 1.3.1 release, there are two extra jars to work with different Spark minor releases. For spark 3.1.x, the jar whose <spark-version> is spark311 should be used. And for spark 3.2.x, the jar whose <spark-version> is spark321 should be used. spark-sql-columnar-shims-common-<version>-SNAPSHOT.jar spark-sql-columnar-shims-<spark-version>-<version>-SNAPSHOT.jar Start from 1.4.0 release Since 1.4.0 release, we consolidate 4 jars into one single jar. And the supported spark version is contained in the jar name. User can pick one jar according to your spark version. gazelle-plugin-<version>-spark-3.1.1.jar gazelle-plugin-<version>-spark-3.2.1.jar Please note the files are fat jars shipped with our custom Arrow library and pre-compiled from our server(using GCC 9.3.0 and LLVM 7.0.1), which means you will require to pre-install GCC 9.3.0 and LLVM 7.0.1 in your system for normal usage. Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built gazelle-plugin-<version>-<spark-version>.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started . Building by yourself If you prefer to build from the source code on your hand, please follow below steps to set up your environment. Prerequisite There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. A conda env was also provided to help to setup the building env quickly. If you are running a SPARK Cluster, please make sure all the software are installed in every single node. Installation Please check the document Installation Guide Get started To enable Gazelle Plugin, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar and spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. As of 1.3.1 release, a new shim layer was introduced to work with Spark minor releases. The shim layer also have two jars spark-sql-columnar-shims-common-<version>-SNAPSHOT.jar and spark-sql-columnar-shims-<spark-version>-<version>-SNAPSHOT.jar And after 1.4.0 release, only one single jar is required: gazelle-plugin-<version>-spark-3.1.1.jar or gazelle-plugin-<version>-spark-3.2.1.jar . We will demonstrate how to deploy Gazelle (since 1.4.0 release) on Spark 3.2.1. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. More Configuration, please check the document Configuration Guide Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.plugins=com.intel.oap.GazellePlugin \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/gazelle-plugin-<version>-spark-3.2.1.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/gazelle-plugin-<version>-spark-3.2.1.jar \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if Gazelle Plugin works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/data_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should showup on Spark console and you can check the DAG diagram with some Columnar Processing stage. Gazelle Plugin still lacks some features, please check out the limitations . Cloud/K8s Integration Amazon EMR Please refer to Gazelle on EMR to find details about how to use OAP Gazelle on Amazon EMR Cloud. Google Cloud Dataproc Gazelle Plugin now supports to run on Dataproc 2.0, we provide a guide to help quickly install Gazelle Plugin and run TPC-DS with notebooks or scripts. Please refer to Gazelle on Dataproc to find details about: Create a cluster on Dataproc 2.0 with initialization actions. Gazelle Plugin jars compiled with -Pdataproc-2.0 parameter will installed by Conda in all cluster nodes. Config for enabling Gazelle Plugin. Run TPC-DS with notebooks or scripts. You can also find more information about how to use OAP on Google Dataproc . Docker Image and Kubernetes Support For Kubernetes Support, please refer to OAP on Kubernetes For how to build the Docker Image, please refer to OAP Dockerfiles For using an existing Docker Image, please check the repository OAP Docker Image and Gazelle Docker Image Performance data For advanced performance testing, below charts show the results by using two benchmarks with Gazelle v1.1: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. The testing environment for Decision Support Benchmark1 is using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset and parquet format. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. The testing environment for Decision Support Benchmark2 is using 1 master + 3 workers and Intel(r) Xeon(r) Platinum 8360Y CPU|1440GB memory|NVMe SSD x4 per single node with 3TB dataset and parquet format. * Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin. Please check the detailed page on performance tuning for TPC-H and TPC-DS workloads. Memory allocation The memory usage in Gazelle Plugin is high. The allocations goes to two parts: 1) Java based allocation which is widely used in Arrow Java API. 2) Native side memory allocation used in each native kernel. We investigated the memory allocation behavior and made more turnings here , the memroy footprint is stable during a TPC-DS power run. Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact weiting.chen@intel.com binwei.yang@intel.com","title":"User Guide"},{"location":"User-Guide/#gazelle-plugin","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations","title":"Gazelle Plugin"},{"location":"User-Guide/#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient. Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow, and leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL.","title":"Introduction"},{"location":"User-Guide/#key-features","text":"","title":"Key Features"},{"location":"User-Guide/#apache-arrow-formatted-intermediate-data-among-spark-operator","text":"With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format.","title":"Apache Arrow formatted intermediate data among Spark operator"},{"location":"User-Guide/#apache-arrow-based-native-readers-for-parquet-and-other-formats","text":"A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Note both data source V1 and V2 are supported. Please check the example section for arrow data source","title":"Apache Arrow based Native Readers for Parquet and other formats"},{"location":"User-Guide/#apache-arrow-computegandiva-based-operators","text":"We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch.","title":"Apache Arrow Compute/Gandiva based operators"},{"location":"User-Guide/#native-columnar-shuffle-operator-with-efficient-compression-support","text":"We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Please check the operator supporting details here","title":"Native Columnar Shuffle Operator with efficient compression support"},{"location":"User-Guide/#how-to-use-oap-gazelle-plugin","text":"There are three ways to use OAP: Gazelle Plugin, 1. Use precompiled jars 2. Building by Conda Environment 3. Building by Yourself","title":"How to use OAP: Gazelle Plugin"},{"location":"User-Guide/#use-precompiled-jars","text":"","title":"Use precompiled jars"},{"location":"User-Guide/#before-140-release","text":"Please go to OAP's Maven Central Repository to find Gazelle Plugin jars. For usage, you will require below two jar files: 1. spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar is located in com/intel/oap/spark-arrow-datasource-standard/ / 2. spark-columnar-core-<version>-jar-with-dependencies.jar is located in com/intel/oap/spark-columnar-core/ / Since 1.3.1 release, there are two extra jars to work with different Spark minor releases. For spark 3.1.x, the jar whose <spark-version> is spark311 should be used. And for spark 3.2.x, the jar whose <spark-version> is spark321 should be used. spark-sql-columnar-shims-common-<version>-SNAPSHOT.jar spark-sql-columnar-shims-<spark-version>-<version>-SNAPSHOT.jar","title":"Before 1.4.0 release"},{"location":"User-Guide/#start-from-140-release","text":"Since 1.4.0 release, we consolidate 4 jars into one single jar. And the supported spark version is contained in the jar name. User can pick one jar according to your spark version. gazelle-plugin-<version>-spark-3.1.1.jar gazelle-plugin-<version>-spark-3.2.1.jar Please note the files are fat jars shipped with our custom Arrow library and pre-compiled from our server(using GCC 9.3.0 and LLVM 7.0.1), which means you will require to pre-install GCC 9.3.0 and LLVM 7.0.1 in your system for normal usage.","title":"Start from 1.4.0 release"},{"location":"User-Guide/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built gazelle-plugin-<version>-<spark-version>.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Get Started .","title":"Building by Conda"},{"location":"User-Guide/#building-by-yourself","text":"If you prefer to build from the source code on your hand, please follow below steps to set up your environment.","title":"Building by yourself"},{"location":"User-Guide/#prerequisite","text":"There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. A conda env was also provided to help to setup the building env quickly. If you are running a SPARK Cluster, please make sure all the software are installed in every single node.","title":"Prerequisite"},{"location":"User-Guide/#installation","text":"Please check the document Installation Guide","title":"Installation"},{"location":"User-Guide/#get-started","text":"To enable Gazelle Plugin, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar and spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. As of 1.3.1 release, a new shim layer was introduced to work with Spark minor releases. The shim layer also have two jars spark-sql-columnar-shims-common-<version>-SNAPSHOT.jar and spark-sql-columnar-shims-<spark-version>-<version>-SNAPSHOT.jar And after 1.4.0 release, only one single jar is required: gazelle-plugin-<version>-spark-3.1.1.jar or gazelle-plugin-<version>-spark-3.2.1.jar . We will demonstrate how to deploy Gazelle (since 1.4.0 release) on Spark 3.2.1. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. More Configuration, please check the document Configuration Guide Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.plugins=com.intel.oap.GazellePlugin \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/gazelle-plugin-<version>-spark-3.2.1.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/gazelle-plugin-<version>-spark-3.2.1.jar \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if Gazelle Plugin works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/data_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should showup on Spark console and you can check the DAG diagram with some Columnar Processing stage. Gazelle Plugin still lacks some features, please check out the limitations .","title":"Get started"},{"location":"User-Guide/#cloudk8s-integration","text":"","title":"Cloud/K8s Integration"},{"location":"User-Guide/#amazon-emr","text":"Please refer to Gazelle on EMR to find details about how to use OAP Gazelle on Amazon EMR Cloud.","title":"Amazon EMR"},{"location":"User-Guide/#google-cloud-dataproc","text":"Gazelle Plugin now supports to run on Dataproc 2.0, we provide a guide to help quickly install Gazelle Plugin and run TPC-DS with notebooks or scripts. Please refer to Gazelle on Dataproc to find details about: Create a cluster on Dataproc 2.0 with initialization actions. Gazelle Plugin jars compiled with -Pdataproc-2.0 parameter will installed by Conda in all cluster nodes. Config for enabling Gazelle Plugin. Run TPC-DS with notebooks or scripts. You can also find more information about how to use OAP on Google Dataproc .","title":"Google Cloud Dataproc"},{"location":"User-Guide/#docker-image-and-kubernetes-support","text":"For Kubernetes Support, please refer to OAP on Kubernetes For how to build the Docker Image, please refer to OAP Dockerfiles For using an existing Docker Image, please check the repository OAP Docker Image and Gazelle Docker Image","title":"Docker Image and Kubernetes Support"},{"location":"User-Guide/#performance-data","text":"For advanced performance testing, below charts show the results by using two benchmarks with Gazelle v1.1: 1. Decision Support Benchmark1 and 2. Decision Support Benchmark2. The testing environment for Decision Support Benchmark1 is using 1 master + 3 workers and Intel(r) Xeon(r) Gold 6252 CPU|384GB memory|NVMe SSD x3 per single node with 1.5TB dataset and parquet format. * Decision Support Benchmark1 is a query set modified from TPC-H benchmark . We change Decimal to Double since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. Overall, the result shows a 1.49X performance speed up from OAP v1.0-Gazelle Plugin comparing to Vanilla SPARK 3.0.0. We also put the detail result by queries, most of queries in Decision Support Benchmark1 can take the advantages from Gazelle Plugin. The performance boost ratio may depend on the individual query. The testing environment for Decision Support Benchmark2 is using 1 master + 3 workers and Intel(r) Xeon(r) Platinum 8360Y CPU|1440GB memory|NVMe SSD x4 per single node with 3TB dataset and parquet format. * Decision Support Benchmark2 is a query set modified from TPC-DS benchmark . We change Decimal to Doubel since Decimal hasn't been supported in OAP v1.0-Gazelle Plugin. We pick up 10 queries which can be fully supported in OAP v1.0-Gazelle Plugin and the result shows a 1.26X performance speed up comparing to Vanilla SPARK 3.0.0. Please notes the performance data is not an official from TPC-H and TPC-DS. The actual performance result may vary by individual workloads. Please try your workloads with Gazelle Plugin first and check the DAG or log file to see if all the operators can be supported in OAP-Gazelle Plugin. Please check the detailed page on performance tuning for TPC-H and TPC-DS workloads.","title":"Performance data"},{"location":"User-Guide/#memory-allocation","text":"The memory usage in Gazelle Plugin is high. The allocations goes to two parts: 1) Java based allocation which is widely used in Arrow Java API. 2) Native side memory allocation used in each native kernel. We investigated the memory allocation behavior and made more turnings here , the memroy footprint is stable during a TPC-DS power run.","title":"Memory allocation"},{"location":"User-Guide/#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"User-Guide/#contact","text":"weiting.chen@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"limitations/","text":"Limitations for Gazelle Plugin Spark compability Currently, Gazelle Plugin is workable with Spark 3.1.x & 3.2.x. Operator limitations All performance critical operators in TPC-H/TPC-DS should be supported. For those unsupported operators, Gazelle Plugin will automatically fallback to row operators in vanilla Spark. Columnar Projection with Filter We used 16 bit selection vector for filter so the max batch size need to be < 65536 Columnar Sort Columnar Sort does not support spill to disk yet. To reduce the peak memory usage, we used smaller data structure(uin16_t), so this limits - the max batch size to be < 65536 - the number of batches in one partiton to be < 65536","title":"Limitations for Gazelle Plugin"},{"location":"limitations/#limitations-for-gazelle-plugin","text":"","title":"Limitations for Gazelle Plugin"},{"location":"limitations/#spark-compability","text":"Currently, Gazelle Plugin is workable with Spark 3.1.x & 3.2.x.","title":"Spark compability"},{"location":"limitations/#operator-limitations","text":"All performance critical operators in TPC-H/TPC-DS should be supported. For those unsupported operators, Gazelle Plugin will automatically fallback to row operators in vanilla Spark.","title":"Operator limitations"},{"location":"limitations/#columnar-projection-with-filter","text":"We used 16 bit selection vector for filter so the max batch size need to be < 65536","title":"Columnar Projection with Filter"},{"location":"limitations/#columnar-sort","text":"Columnar Sort does not support spill to disk yet. To reduce the peak memory usage, we used smaller data structure(uin16_t), so this limits - the max batch size to be < 65536 - the number of batches in one partiton to be < 65536","title":"Columnar Sort"},{"location":"memory/","text":"Memory Allocation in Gazelle Plugin Java memory allocation By default, Arrow columnar vector Java API is using netty pooledbytebuffer allocator , which will try to hold on the \"free memory\" by not returning back to System immediately for better performance. This will result big memory footprint on operators relying on this API, e.g., CoalesceBatches . We changed to use unsafe API since 1.2 release, which means the freed memory will be returned to system directly. Performance tests showed the performance of this change is negatable. Native memory allocation Modern memory allocators like jemalloc will not return the just freed memory to system to achieve better performance, also a set of memory allocation pools will be used to reduce the lock contention. Recent version of glibc also used similar design - by default it has 8 * cores pools, each with 64MB size. This will introduce a big memory footprint. Jemalloc's pool is smaller relatively - it has 4 * cores pools, each with 2MB size. Both glibc/jemalloc provides turning knobs to control these behaviors. Using lower number of pools can reduce the memory footprint, but also may impact the performance. It's a tradeoff on memory vs. performance. Turnings to reduce memory footprint Gazelle Plugin 1.2+ Using jemalloc in Arrow build -DARROW_USEJEMALLOC=True Build Gazelle Plugin with arrow-unsafe profile mvn clean pacakge -DskipTests -Parrow-unsafe LD_PRELOAD jemalloc in each Spark executor --conf spark.executorEnv.LD_PRELOAD=/path/to/libjemalloc.so Reduce number memory pools in jemalloc --conf spark.executorEnv.MALLOC_CONF=narenas:2 Reduce number memory pools in glibc --conf spark.executorEnv.MALLOC_ARENA_MAX=2","title":"Memory Allocation in Gazelle Plugin"},{"location":"memory/#memory-allocation-in-gazelle-plugin","text":"","title":"Memory Allocation in Gazelle Plugin"},{"location":"memory/#java-memory-allocation","text":"By default, Arrow columnar vector Java API is using netty pooledbytebuffer allocator , which will try to hold on the \"free memory\" by not returning back to System immediately for better performance. This will result big memory footprint on operators relying on this API, e.g., CoalesceBatches . We changed to use unsafe API since 1.2 release, which means the freed memory will be returned to system directly. Performance tests showed the performance of this change is negatable.","title":"Java memory allocation"},{"location":"memory/#native-memory-allocation","text":"Modern memory allocators like jemalloc will not return the just freed memory to system to achieve better performance, also a set of memory allocation pools will be used to reduce the lock contention. Recent version of glibc also used similar design - by default it has 8 * cores pools, each with 64MB size. This will introduce a big memory footprint. Jemalloc's pool is smaller relatively - it has 4 * cores pools, each with 2MB size. Both glibc/jemalloc provides turning knobs to control these behaviors. Using lower number of pools can reduce the memory footprint, but also may impact the performance. It's a tradeoff on memory vs. performance.","title":"Native memory allocation"},{"location":"memory/#turnings-to-reduce-memory-footprint","text":"Gazelle Plugin 1.2+ Using jemalloc in Arrow build -DARROW_USEJEMALLOC=True Build Gazelle Plugin with arrow-unsafe profile mvn clean pacakge -DskipTests -Parrow-unsafe LD_PRELOAD jemalloc in each Spark executor --conf spark.executorEnv.LD_PRELOAD=/path/to/libjemalloc.so Reduce number memory pools in jemalloc --conf spark.executorEnv.MALLOC_CONF=narenas:2 Reduce number memory pools in glibc --conf spark.executorEnv.MALLOC_ARENA_MAX=2","title":"Turnings to reduce memory footprint"},{"location":"operators/","text":"No. Executor Description Notes BOOLEAN BYTE SHORT INT LONG FLOAT DOUBLE STRING DECIMAL DATE TIMESTAMP NULL BINARY CALENDAR ARRAY MAP STRUCT UDT 1 CoalesceExec y y y y y y y y y y 2 CollectLimitExec using row version 3 ExpandExec y y y y y y y y y y 4 FileSourceScanExec y y y y y y y y y y 5 FilterExec y y y y y y y y y y 6 GenerateExec using row version 7 GlobalLimitExec using row version 8 LocalLimitExec 9 ProjectExec y y y y y y y y y y 10 RangeExec using row version 11 SortExec y y y y y y y y y y 12 TakeOrderedAndPorjectExec using row version 13 UnionExec y y y y y y y y y y 14 CustomShuffleReaderExec y y y y y y y y y y 15 HashAggregateExec y y y y y y y y y y 16 SortAggregateExec y y y y y y y y y y 17 DataWritingCommandExec using row version 18 BatchScanExec y y y y y y y y y y 19 BroadcastExchangeExec y y y y y y y y y y 20 ShuffleExchangeExec y y y y y y y y y y 21 BroadcastHashJoinExec y y y y y y y y y y 22 BroadcastNestedLoopJoinExec using row version 23 CartesianProductExec using row version 24 ShuffledHashJoinExec y y y y y y y y y y 25 SortMergeJoinExec y y y y y y y y y y 26 ArrowEvalPythonExec using row version 27 WindowINPandasExec using row version 28 WindowExec y y y y y y y y y y","title":"Operators"},{"location":"performance/","text":"Performance Tuning for Gazelle Plugin It is complicated to tune for Spark workloads as each varies a lot. Here are several general tuning options on the most popular TPCH/TPC-DS benchmarking. Columnar Batch size Spark have several options to control the batch size at different operators. We suggest to use bigger value as this will bring better cache efficiency especially for columnar processing. --conf spark.sql.inMemoryColumnarStorage.batchSize=20480 --conf spark.sql.execution.arrow.maxRecordsPerBatch=20480 --conf spark.sql.parquet.columnarReaderBatchSize=20480 Shuffle Partitions As Gazelle currently only supports hash based shuffle, it's recommended to use 1 or 2 times shuffle partitions of the total HT cores in the cluster. e.g., there are 384 cores in the cluster, it's better to use spark.sql.shuffle.partitions = 384 or 768. It this way it's most efficient for Gazelle. On-heap/Off-heap Memory Size Unlike Spark, most of the memory usage in Gazelle would be off-heap based. So a big off-heap is recomended. There are still some small objects in On-heap thus a proper sized on-heap is also required. We are recommending below configurations for memory related settings. --executor-cores 6 --executor-memory 6g // on-heap memory: 1G per core --conf spark.executor.memoryOverhead=384 // not used, 384M should be enough --conf spark.memory.offHeap.enabled=true // enable off-heap thus Spark can control the memory --conf spark.memory.offHeap.size=15g // a big off-heap is required Data Generating On non-partiton tables, it's better to set the number of files to times of total HT cores in your cluster, e.g., there are 384 cores in your cluster, we'd better to set the file numbers to 2x384 or 3x384 to ensure. In this way Spark would only issue one task for each file(together with spark.sql.files.maxPartitionBytes option). Note the spark.sql.files.maxRecordsPerFile will also split the files in to smaller ones. We may just disable this feature to -1.","title":"Performance Tuning for Gazelle Plugin"},{"location":"performance/#performance-tuning-for-gazelle-plugin","text":"It is complicated to tune for Spark workloads as each varies a lot. Here are several general tuning options on the most popular TPCH/TPC-DS benchmarking.","title":"Performance Tuning for Gazelle Plugin"},{"location":"performance/#columnar-batch-size","text":"Spark have several options to control the batch size at different operators. We suggest to use bigger value as this will bring better cache efficiency especially for columnar processing. --conf spark.sql.inMemoryColumnarStorage.batchSize=20480 --conf spark.sql.execution.arrow.maxRecordsPerBatch=20480 --conf spark.sql.parquet.columnarReaderBatchSize=20480","title":"Columnar Batch size"},{"location":"performance/#shuffle-partitions","text":"As Gazelle currently only supports hash based shuffle, it's recommended to use 1 or 2 times shuffle partitions of the total HT cores in the cluster. e.g., there are 384 cores in the cluster, it's better to use spark.sql.shuffle.partitions = 384 or 768. It this way it's most efficient for Gazelle.","title":"Shuffle Partitions"},{"location":"performance/#on-heapoff-heap-memory-size","text":"Unlike Spark, most of the memory usage in Gazelle would be off-heap based. So a big off-heap is recomended. There are still some small objects in On-heap thus a proper sized on-heap is also required. We are recommending below configurations for memory related settings. --executor-cores 6 --executor-memory 6g // on-heap memory: 1G per core --conf spark.executor.memoryOverhead=384 // not used, 384M should be enough --conf spark.memory.offHeap.enabled=true // enable off-heap thus Spark can control the memory --conf spark.memory.offHeap.size=15g // a big off-heap is required","title":"On-heap/Off-heap Memory Size"},{"location":"performance/#data-generating","text":"On non-partiton tables, it's better to set the number of files to times of total HT cores in your cluster, e.g., there are 384 cores in your cluster, we'd better to set the file numbers to 2x384 or 3x384 to ensure. In this way Spark would only issue one task for each file(together with spark.sql.files.maxPartitionBytes option). Note the spark.sql.files.maxRecordsPerFile will also split the files in to smaller ones. We may just disable this feature to -1.","title":"Data Generating"}]}