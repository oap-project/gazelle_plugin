-- Automatically generated by SQLQueryTestSuite
-- Number of queries: 122


-- !query
select TIMESTAMP_SECONDS(1230219000),TIMESTAMP_SECONDS(-1230219000),TIMESTAMP_SECONDS(null)
-- !query schema
struct<timestamp_seconds(1230219000):timestamp,timestamp_seconds(-1230219000):timestamp,timestamp_seconds(CAST(NULL AS DOUBLE)):timestamp>
-- !query output
2008-12-25 07:30:00	1931-01-07 00:30:00	NULL


-- !query
select TIMESTAMP_SECONDS(1.23), TIMESTAMP_SECONDS(1.23d), TIMESTAMP_SECONDS(FLOAT(1.23))
-- !query schema
struct<timestamp_seconds(1.23):timestamp,timestamp_seconds(1.23):timestamp,timestamp_seconds(CAST(1.23 AS FLOAT)):timestamp>
-- !query output
1969-12-31 16:00:01.23	1969-12-31 16:00:01.23	1969-12-31 16:00:01.23


-- !query
select TIMESTAMP_MILLIS(1230219000123),TIMESTAMP_MILLIS(-1230219000123),TIMESTAMP_MILLIS(null)
-- !query schema
struct<timestamp_millis(1230219000123):timestamp,timestamp_millis(-1230219000123):timestamp,timestamp_millis(CAST(NULL AS INT)):timestamp>
-- !query output
2008-12-25 07:30:00.123	1931-01-07 00:29:59.877	NULL


-- !query
select TIMESTAMP_MICROS(1230219000123123),TIMESTAMP_MICROS(-1230219000123123),TIMESTAMP_MICROS(null)
-- !query schema
struct<timestamp_micros(1230219000123123):timestamp,timestamp_micros(-1230219000123123):timestamp,timestamp_micros(CAST(NULL AS INT)):timestamp>
-- !query output
2008-12-25 07:30:00.123123	1931-01-07 00:29:59.876877	NULL


-- !query
select TIMESTAMP_SECONDS(1230219000123123)
-- !query schema
struct<>
-- !query output
java.lang.ArithmeticException
long overflow


-- !query
select TIMESTAMP_SECONDS(-1230219000123123)
-- !query schema
struct<>
-- !query output
java.lang.ArithmeticException
long overflow


-- !query
select TIMESTAMP_MILLIS(92233720368547758)
-- !query schema
struct<>
-- !query output
java.lang.ArithmeticException
long overflow


-- !query
select TIMESTAMP_MILLIS(-92233720368547758)
-- !query schema
struct<>
-- !query output
java.lang.ArithmeticException
long overflow


-- !query
select TIMESTAMP_SECONDS(0.1234567)
-- !query schema
struct<>
-- !query output
java.lang.ArithmeticException
Rounding necessary


-- !query
select TIMESTAMP_SECONDS(0.1234567d), TIMESTAMP_SECONDS(FLOAT(0.1234567))
-- !query schema
struct<timestamp_seconds(0.1234567):timestamp,timestamp_seconds(CAST(0.1234567 AS FLOAT)):timestamp>
-- !query output
1969-12-31 16:00:00.123456	1969-12-31 16:00:00.123456


-- !query
select UNIX_SECONDS(TIMESTAMP('2020-12-01 14:30:08Z')), UNIX_SECONDS(TIMESTAMP('2020-12-01 14:30:08.999999Z')), UNIX_SECONDS(null)
-- !query schema
struct<unix_seconds(CAST(2020-12-01 14:30:08Z AS TIMESTAMP)):bigint,unix_seconds(CAST(2020-12-01 14:30:08.999999Z AS TIMESTAMP)):bigint,unix_seconds(CAST(NULL AS TIMESTAMP)):bigint>
-- !query output
1606833008	1606833008	NULL


-- !query
select UNIX_MILLIS(TIMESTAMP('2020-12-01 14:30:08Z')), UNIX_MILLIS(TIMESTAMP('2020-12-01 14:30:08.999999Z')), UNIX_MILLIS(null)
-- !query schema
struct<unix_millis(CAST(2020-12-01 14:30:08Z AS TIMESTAMP)):bigint,unix_millis(CAST(2020-12-01 14:30:08.999999Z AS TIMESTAMP)):bigint,unix_millis(CAST(NULL AS TIMESTAMP)):bigint>
-- !query output
1606833008000	1606833008999	NULL


-- !query
select UNIX_MICROS(TIMESTAMP('2020-12-01 14:30:08Z')), UNIX_MICROS(TIMESTAMP('2020-12-01 14:30:08.999999Z')), UNIX_MICROS(null)
-- !query schema
struct<unix_micros(CAST(2020-12-01 14:30:08Z AS TIMESTAMP)):bigint,unix_micros(CAST(2020-12-01 14:30:08.999999Z AS TIMESTAMP)):bigint,unix_micros(CAST(NULL AS TIMESTAMP)):bigint>
-- !query output
1606833008000000	1606833008999999	NULL


-- !query
select DATE_FROM_UNIX_DATE(0), DATE_FROM_UNIX_DATE(1000), DATE_FROM_UNIX_DATE(null)
-- !query schema
struct<date_from_unix_date(0):date,date_from_unix_date(1000):date,date_from_unix_date(CAST(NULL AS INT)):date>
-- !query output
1970-01-01	1972-09-27	NULL


-- !query
select UNIX_DATE(DATE('1970-01-01')), UNIX_DATE(DATE('2020-12-04')), UNIX_DATE(null)
-- !query schema
struct<unix_date(CAST(1970-01-01 AS DATE)):int,unix_date(CAST(2020-12-04 AS DATE)):int,unix_date(CAST(NULL AS DATE)):int>
-- !query output
0	18600	NULL


-- !query
select current_date = current_date(), current_timestamp = current_timestamp()
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

no viable alternative at input 'current_date'(line 1, pos 22)

== SQL ==
select current_date = current_date(), current_timestamp = current_timestamp()
----------------------^^^


-- !query
select to_date(null), to_date('2016-12-31'), to_date('2016-12-31', 'yyyy-MM-dd')
-- !query schema
struct<to_date(NULL):date,to_date(2016-12-31):date,to_date(2016-12-31, yyyy-MM-dd):date>
-- !query output
NULL	2016-12-31	2016-12-31


-- !query
select to_timestamp(null), to_timestamp('2016-12-31 00:12:00'), to_timestamp('2016-12-31', 'yyyy-MM-dd')
-- !query schema
struct<to_timestamp(NULL):timestamp,to_timestamp(2016-12-31 00:12:00):timestamp,to_timestamp(2016-12-31, yyyy-MM-dd):timestamp>
-- !query output
NULL	2016-12-31 00:12:00	2016-12-31 00:00:00


-- !query
select dayofweek('2007-02-03'), dayofweek('2009-07-30'), dayofweek('2017-05-27'), dayofweek(null), dayofweek('1582-10-15 13:10:15')
-- !query schema
struct<dayofweek(CAST(2007-02-03 AS DATE)):int,dayofweek(CAST(2009-07-30 AS DATE)):int,dayofweek(CAST(2017-05-27 AS DATE)):int,dayofweek(CAST(NULL AS DATE)):int,dayofweek(CAST(1582-10-15 13:10:15 AS DATE)):int>
-- !query output
7	5	7	NULL	6


-- !query
create temporary view ttf1 as select * from values
  (1, 2),
  (2, 3)
  as ttf1(current_date, current_timestamp)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

no viable alternative at input 'current_date'(line 4, pos 10)

== SQL ==
create temporary view ttf1 as select * from values
  (1, 2),
  (2, 3)
  as ttf1(current_date, current_timestamp)
----------^^^


-- !query
select current_date, current_timestamp from ttf1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
Table or view not found: ttf1; line 1 pos 44


-- !query
create temporary view ttf2 as select * from values
  (1, 2),
  (2, 3)
  as ttf2(a, b)
-- !query schema
struct<>
-- !query output



-- !query
select current_date = current_date(), current_timestamp = current_timestamp(), a, b from ttf2
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

no viable alternative at input 'current_date'(line 1, pos 22)

== SQL ==
select current_date = current_date(), current_timestamp = current_timestamp(), a, b from ttf2
----------------------^^^


-- !query
select a, b from ttf2 order by a, current_date
-- !query schema
struct<a:int,b:int>
-- !query output
1	2
2	3


-- !query
select weekday('2007-02-03'), weekday('2009-07-30'), weekday('2017-05-27'), weekday(null), weekday('1582-10-15 13:10:15')
-- !query schema
struct<weekday(CAST(2007-02-03 AS DATE)):int,weekday(CAST(2009-07-30 AS DATE)):int,weekday(CAST(2017-05-27 AS DATE)):int,weekday(CAST(NULL AS DATE)):int,weekday(CAST(1582-10-15 13:10:15 AS DATE)):int>
-- !query output
5	3	5	NULL	4


-- !query
select year('1500-01-01'), month('1500-01-01'), dayOfYear('1500-01-01')
-- !query schema
struct<year(CAST(1500-01-01 AS DATE)):int,month(CAST(1500-01-01 AS DATE)):int,dayofyear(CAST(1500-01-01 AS DATE)):int>
-- !query output
1500	1	1


-- !query
select date '2019-01-01\t'
-- !query schema
struct<DATE '2019-01-01':date>
-- !query output
2019-01-01


-- !query
select timestamp '2019-01-01\t'
-- !query schema
struct<TIMESTAMP '2019-01-01 00:00:00':timestamp>
-- !query output
2019-01-01 00:00:00


-- !query
select date '2020-01-01中文'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

Cannot parse the DATE value: 2020-01-01中文(line 1, pos 7)

== SQL ==
select date '2020-01-01中文'
-------^^^


-- !query
select timestamp '2019-01-01中文'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

Cannot parse the TIMESTAMP value: 2019-01-01中文(line 1, pos 7)

== SQL ==
select timestamp '2019-01-01中文'
-------^^^


-- !query
select timestamp'2011-11-11 11:11:11' + interval '2' day
-- !query schema
struct<CAST(TIMESTAMP '2011-11-11 11:11:11' + INTERVAL '2 days' AS TIMESTAMP):timestamp>
-- !query output
2011-11-13 11:11:11


-- !query
select timestamp'2011-11-11 11:11:11' - interval '2' day
-- !query schema
struct<CAST(TIMESTAMP '2011-11-11 11:11:11' - INTERVAL '2 days' AS TIMESTAMP):timestamp>
-- !query output
2011-11-09 11:11:11


-- !query
select date'2011-11-11 11:11:11' + interval '2' second
-- !query schema
struct<>
-- !query output
java.lang.IllegalArgumentException
requirement failed: Cannot add hours, minutes or seconds, milliseconds, microseconds to a date


-- !query
select date'2011-11-11 11:11:11' - interval '2' second
-- !query schema
struct<>
-- !query output
java.lang.IllegalArgumentException
requirement failed: Cannot add hours, minutes or seconds, milliseconds, microseconds to a date


-- !query
select '2011-11-11' - interval '2' day
-- !query schema
struct<CAST(2011-11-11 - INTERVAL '2 days' AS STRING):string>
-- !query output
2011-11-09 00:00:00


-- !query
select '2011-11-11 11:11:11' - interval '2' second
-- !query schema
struct<CAST(2011-11-11 11:11:11 - INTERVAL '2 seconds' AS STRING):string>
-- !query output
2011-11-11 11:11:09


-- !query
select '1' - interval '2' second
-- !query schema
struct<>
-- !query output
java.time.DateTimeException
Cannot cast 1 to TimestampType.


-- !query
select 1 - interval '2' second
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve '1 + (- INTERVAL '2 seconds')' due to data type mismatch: argument 1 requires timestamp type, however, '1' is of int type.; line 1 pos 7


-- !query
select date'2020-01-01' - timestamp'2019-10-06 10:11:12.345678'
-- !query schema
struct<subtracttimestamps(CAST(DATE '2020-01-01' AS TIMESTAMP), TIMESTAMP '2019-10-06 10:11:12.345678'):interval>
-- !query output
2078 hours 48 minutes 47.654322 seconds


-- !query
select timestamp'2019-10-06 10:11:12.345678' - date'2020-01-01'
-- !query schema
struct<subtracttimestamps(TIMESTAMP '2019-10-06 10:11:12.345678', CAST(DATE '2020-01-01' AS TIMESTAMP)):interval>
-- !query output
-2078 hours -48 minutes -47.654322 seconds


-- !query
select timestamp'2019-10-06 10:11:12.345678' - null
-- !query schema
struct<subtracttimestamps(TIMESTAMP '2019-10-06 10:11:12.345678', CAST(NULL AS TIMESTAMP)):interval>
-- !query output
NULL


-- !query
select null - timestamp'2019-10-06 10:11:12.345678'
-- !query schema
struct<subtracttimestamps(CAST(NULL AS TIMESTAMP), TIMESTAMP '2019-10-06 10:11:12.345678'):interval>
-- !query output
NULL


-- !query
select date_add('2011-11-11', 1Y)
-- !query schema
struct<date_add(CAST(2011-11-11 AS DATE), 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', 1S)
-- !query schema
struct<date_add(CAST(2011-11-11 AS DATE), 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', 1)
-- !query schema
struct<date_add(CAST(2011-11-11 AS DATE), 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', 1L)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('2011-11-11' AS DATE), 1L)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '1L' is of bigint type.; line 1 pos 7


-- !query
select date_add('2011-11-11', 1.0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('2011-11-11' AS DATE), 1.0BD)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '1.0BD' is of decimal(2,1) type.; line 1 pos 7


-- !query
select date_add('2011-11-11', 1E1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('2011-11-11' AS DATE), 10.0D)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '10.0D' is of double type.; line 1 pos 7


-- !query
select date_add('2011-11-11', '1')
-- !query schema
struct<date_add(CAST(2011-11-11 AS DATE), 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', '1.2')
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
The second argument of 'date_add' function needs to be an integer.


-- !query
select date_add(date'2011-11-11', 1)
-- !query schema
struct<date_add(DATE '2011-11-11', 1):date>
-- !query output
2011-11-12


-- !query
select date_add(timestamp'2011-11-11', 1)
-- !query schema
struct<date_add(CAST(TIMESTAMP '2011-11-11 00:00:00' AS DATE), 1):date>
-- !query output
2011-11-12


-- !query
select date_sub(date'2011-11-11', 1)
-- !query schema
struct<date_sub(DATE '2011-11-11', 1):date>
-- !query output
2011-11-10


-- !query
select date_sub(date'2011-11-11', '1')
-- !query schema
struct<date_sub(DATE '2011-11-11', 1):date>
-- !query output
2011-11-10


-- !query
select date_sub(date'2011-11-11', '1.2')
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
The second argument of 'date_sub' function needs to be an integer.


-- !query
select date_sub(timestamp'2011-11-11', 1)
-- !query schema
struct<date_sub(CAST(TIMESTAMP '2011-11-11 00:00:00' AS DATE), 1):date>
-- !query output
2011-11-10


-- !query
select date_sub(null, 1)
-- !query schema
struct<date_sub(CAST(NULL AS DATE), 1):date>
-- !query output
NULL


-- !query
select date_sub(date'2011-11-11', null)
-- !query schema
struct<date_sub(DATE '2011-11-11', CAST(NULL AS INT)):date>
-- !query output
NULL


-- !query
select date'2011-11-11' + 1E1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(DATE '2011-11-11', 10.0D)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '10.0D' is of double type.; line 1 pos 7


-- !query
select date'2011-11-11' + '1'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(DATE '2011-11-11', CAST('1' AS DOUBLE))' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'CAST('1' AS DOUBLE)' is of double type.; line 1 pos 7


-- !query
select null + date '2001-09-28'
-- !query schema
struct<date_add(DATE '2001-09-28', CAST(NULL AS INT)):date>
-- !query output
NULL


-- !query
select date '2001-09-28' + 7Y
-- !query schema
struct<date_add(DATE '2001-09-28', 7):date>
-- !query output
2001-10-05


-- !query
select 7S + date '2001-09-28'
-- !query schema
struct<date_add(DATE '2001-09-28', 7):date>
-- !query output
2001-10-05


-- !query
select date '2001-10-01' - 7
-- !query schema
struct<date_sub(DATE '2001-10-01', 7):date>
-- !query output
2001-09-24


-- !query
select date '2001-10-01' - '7'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_sub(DATE '2001-10-01', CAST('7' AS DOUBLE))' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'CAST('7' AS DOUBLE)' is of double type.; line 1 pos 7


-- !query
select date '2001-09-28' + null
-- !query schema
struct<date_add(DATE '2001-09-28', CAST(NULL AS INT)):date>
-- !query output
NULL


-- !query
select date '2001-09-28' - null
-- !query schema
struct<date_sub(DATE '2001-09-28', CAST(NULL AS INT)):date>
-- !query output
NULL


-- !query
create temp view v as select '1' str
-- !query schema
struct<>
-- !query output



-- !query
select date_add('2011-11-11', str) from v
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('2011-11-11' AS DATE), v.`str`)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'v.`str`' is of string type.; line 1 pos 7


-- !query
select date_sub('2011-11-11', str) from v
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_sub(CAST('2011-11-11' AS DATE), v.`str`)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'v.`str`' is of string type.; line 1 pos 7


-- !query
select null - date '2019-10-06'
-- !query schema
struct<subtractdates(CAST(NULL AS DATE), DATE '2019-10-06'):interval>
-- !query output
NULL


-- !query
select date '2001-10-01' - date '2001-09-28'
-- !query schema
struct<subtractdates(DATE '2001-10-01', DATE '2001-09-28'):interval>
-- !query output
3 days


-- !query
select to_timestamp('2019-10-06 10:11:12.', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '2019-10-06 10:11:12.' could not be parsed at index 20


-- !query
select to_timestamp('2019-10-06 10:11:12.0', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.0, yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 10:11:12


-- !query
select to_timestamp('2019-10-06 10:11:12.1', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.1, yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 10:11:12.1


-- !query
select to_timestamp('2019-10-06 10:11:12.12', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.12, yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 10:11:12.12


-- !query
select to_timestamp('2019-10-06 10:11:12.123UTC', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.123UTC, yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 03:11:12.123


-- !query
select to_timestamp('2019-10-06 10:11:12.1234', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.1234, yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 10:11:12.1234


-- !query
select to_timestamp('2019-10-06 10:11:12.12345CST', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.12345CST, yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 08:11:12.12345


-- !query
select to_timestamp('2019-10-06 10:11:12.123456PST', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.123456PST, yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 10:11:12.123456


-- !query
select to_timestamp('2019-10-06 10:11:12.1234567PST', 'yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '2019-10-06 10:11:12.1234567PST' could not be parsed, unparsed text found at index 26


-- !query
select to_timestamp('123456 2019-10-06 10:11:12.123456PST', 'SSSSSS yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<to_timestamp(123456 2019-10-06 10:11:12.123456PST, SSSSSS yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]):timestamp>
-- !query output
2019-10-06 10:11:12.123456


-- !query
select to_timestamp('223456 2019-10-06 10:11:12.123456PST', 'SSSSSS yyyy-MM-dd HH:mm:ss.SSSSSS[zzz]')
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '223456 2019-10-06 10:11:12.123456PST' could not be parsed at index 27


-- !query
select to_timestamp('2019-10-06 10:11:12.1234', 'yyyy-MM-dd HH:mm:ss.[SSSSSS]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.1234, yyyy-MM-dd HH:mm:ss.[SSSSSS]):timestamp>
-- !query output
2019-10-06 10:11:12.1234


-- !query
select to_timestamp('2019-10-06 10:11:12.123', 'yyyy-MM-dd HH:mm:ss[.SSSSSS]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.123, yyyy-MM-dd HH:mm:ss[.SSSSSS]):timestamp>
-- !query output
2019-10-06 10:11:12.123


-- !query
select to_timestamp('2019-10-06 10:11:12', 'yyyy-MM-dd HH:mm:ss[.SSSSSS]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12, yyyy-MM-dd HH:mm:ss[.SSSSSS]):timestamp>
-- !query output
2019-10-06 10:11:12


-- !query
select to_timestamp('2019-10-06 10:11:12.12', 'yyyy-MM-dd HH:mm[:ss.SSSSSS]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11:12.12, yyyy-MM-dd HH:mm[:ss.SSSSSS]):timestamp>
-- !query output
2019-10-06 10:11:12.12


-- !query
select to_timestamp('2019-10-06 10:11', 'yyyy-MM-dd HH:mm[:ss.SSSSSS]')
-- !query schema
struct<to_timestamp(2019-10-06 10:11, yyyy-MM-dd HH:mm[:ss.SSSSSS]):timestamp>
-- !query output
2019-10-06 10:11:00


-- !query
select to_timestamp("2019-10-06S10:11:12.12345", "yyyy-MM-dd'S'HH:mm:ss.SSSSSS")
-- !query schema
struct<to_timestamp(2019-10-06S10:11:12.12345, yyyy-MM-dd'S'HH:mm:ss.SSSSSS):timestamp>
-- !query output
2019-10-06 10:11:12.12345


-- !query
select to_timestamp("12.12342019-10-06S10:11", "ss.SSSSyyyy-MM-dd'S'HH:mm")
-- !query schema
struct<to_timestamp(12.12342019-10-06S10:11, ss.SSSSyyyy-MM-dd'S'HH:mm):timestamp>
-- !query output
2019-10-06 10:11:12.1234


-- !query
select to_timestamp("12.1232019-10-06S10:11", "ss.SSSSyyyy-MM-dd'S'HH:mm")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '12.1232019-10-06S10:11' could not be parsed at index 7


-- !query
select to_timestamp("12.1232019-10-06S10:11", "ss.SSSSyy-MM-dd'S'HH:mm")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '12.1232019-10-06S10:11' could not be parsed at index 9


-- !query
select to_timestamp("12.1234019-10-06S10:11", "ss.SSSSy-MM-dd'S'HH:mm")
-- !query schema
struct<to_timestamp(12.1234019-10-06S10:11, ss.SSSSy-MM-dd'S'HH:mm):timestamp>
-- !query output
0019-10-06 10:11:12.1234


-- !query
select to_timestamp("2019-10-06S", "yyyy-MM-dd'S'")
-- !query schema
struct<to_timestamp(2019-10-06S, yyyy-MM-dd'S'):timestamp>
-- !query output
2019-10-06 00:00:00


-- !query
select to_timestamp("S2019-10-06", "'S'yyyy-MM-dd")
-- !query schema
struct<to_timestamp(S2019-10-06, 'S'yyyy-MM-dd):timestamp>
-- !query output
2019-10-06 00:00:00


-- !query
select to_timestamp("2019-10-06T10:11:12'12", "yyyy-MM-dd'T'HH:mm:ss''SSSS")
-- !query schema
struct<to_timestamp(2019-10-06T10:11:12'12, yyyy-MM-dd'T'HH:mm:ss''SSSS):timestamp>
-- !query output
2019-10-06 10:11:12.12


-- !query
select to_timestamp("2019-10-06T10:11:12'", "yyyy-MM-dd'T'HH:mm:ss''")
-- !query schema
struct<to_timestamp(2019-10-06T10:11:12', yyyy-MM-dd'T'HH:mm:ss''):timestamp>
-- !query output
2019-10-06 10:11:12


-- !query
select to_timestamp("'2019-10-06T10:11:12", "''yyyy-MM-dd'T'HH:mm:ss")
-- !query schema
struct<to_timestamp('2019-10-06T10:11:12, ''yyyy-MM-dd'T'HH:mm:ss):timestamp>
-- !query output
2019-10-06 10:11:12


-- !query
select to_timestamp("P2019-10-06T10:11:12", "'P'yyyy-MM-dd'T'HH:mm:ss")
-- !query schema
struct<to_timestamp(P2019-10-06T10:11:12, 'P'yyyy-MM-dd'T'HH:mm:ss):timestamp>
-- !query output
2019-10-06 10:11:12


-- !query
select to_timestamp("16", "dd")
-- !query schema
struct<to_timestamp(16, dd):timestamp>
-- !query output
1970-01-16 00:00:00


-- !query
select to_timestamp("02-29", "MM-dd")
-- !query schema
struct<>
-- !query output
java.time.DateTimeException
Invalid date 'February 29' as '1970' is not a leap year


-- !query
select to_date("16", "dd")
-- !query schema
struct<to_date(16, dd):date>
-- !query output
1970-01-16


-- !query
select to_date("02-29", "MM-dd")
-- !query schema
struct<>
-- !query output
java.time.DateTimeException
Invalid date 'February 29' as '1970' is not a leap year


-- !query
select to_timestamp("2019 40", "yyyy mm")
-- !query schema
struct<to_timestamp(2019 40, yyyy mm):timestamp>
-- !query output
2019-01-01 00:40:00


-- !query
select to_timestamp("2019 10:10:10", "yyyy hh:mm:ss")
-- !query schema
struct<to_timestamp(2019 10:10:10, yyyy hh:mm:ss):timestamp>
-- !query output
2019-01-01 10:10:10


-- !query
select to_timestamp('2019-10-06 A', 'yyyy-MM-dd GGGGG')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'yyyy-MM-dd GGGGG' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select to_timestamp('22 05 2020 Friday', 'dd MM yyyy EEEEEE')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd MM yyyy EEEEEE' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select to_timestamp('22 05 2020 Friday', 'dd MM yyyy EEEEE')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd MM yyyy EEEEE' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select unix_timestamp('22 05 2020 Friday', 'dd MM yyyy EEEEE')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd MM yyyy EEEEE' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select from_json('{"t":"26/October/2015"}', 't Timestamp', map('timestampFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd/MMMMM/yyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select from_json('{"d":"26/October/2015"}', 'd Date', map('dateFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd/MMMMM/yyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select from_csv('26/October/2015', 't Timestamp', map('timestampFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd/MMMMM/yyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select from_csv('26/October/2015', 'd Date', map('dateFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd/MMMMM/yyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select to_date("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '2020-01-27T20:06:11.847' could not be parsed at index 10


-- !query
select to_date("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text 'Unparseable' could not be parsed at index 0


-- !query
select to_timestamp("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '2020-01-27T20:06:11.847' could not be parsed at index 10


-- !query
select to_timestamp("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text 'Unparseable' could not be parsed at index 0


-- !query
select unix_timestamp("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '2020-01-27T20:06:11.847' could not be parsed at index 10


-- !query
select unix_timestamp("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text 'Unparseable' could not be parsed at index 0


-- !query
select to_unix_timestamp("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text '2020-01-27T20:06:11.847' could not be parsed at index 10


-- !query
select to_unix_timestamp("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<>
-- !query output
java.time.format.DateTimeParseException
Text 'Unparseable' could not be parsed at index 0


-- !query
select cast("Unparseable" as timestamp)
-- !query schema
struct<>
-- !query output
java.time.DateTimeException
Cannot cast Unparseable to TimestampType.
