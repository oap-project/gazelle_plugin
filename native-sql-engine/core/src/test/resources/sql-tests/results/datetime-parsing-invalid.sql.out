-- Automatically generated by SQLQueryTestSuite
-- Number of queries: 19


-- !query
select to_timestamp('294248', 'y')
-- !query schema
struct<>
-- !query output
java.lang.ArithmeticException
long overflow


-- !query
select to_timestamp('1', 'yy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to parse '1' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.


-- !query
select to_timestamp('-12', 'yy')
-- !query schema
struct<to_timestamp(-12, yy):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('123', 'yy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to parse '123' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.


-- !query
select to_timestamp('1', 'yyy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to parse '1' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.


-- !query
select to_timestamp('1234567', 'yyyyyyy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'yyyyyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select to_timestamp('366', 'D')
-- !query schema
struct<to_timestamp(366, D):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('9', 'DD')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to parse '9' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.


-- !query
select to_timestamp('366', 'DD')
-- !query schema
struct<to_timestamp(366, DD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('9', 'DDD')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to parse '9' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.


-- !query
select to_timestamp('99', 'DDD')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to parse '99' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.


-- !query
select to_timestamp('30-365', 'dd-DDD')
-- !query schema
struct<to_timestamp(30-365, dd-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('11-365', 'MM-DDD')
-- !query schema
struct<to_timestamp(11-365, MM-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2019-366', 'yyyy-DDD')
-- !query schema
struct<to_timestamp(2019-366, yyyy-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('12-30-365', 'MM-dd-DDD')
-- !query schema
struct<to_timestamp(12-30-365, MM-dd-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2020-01-365', 'yyyy-dd-DDD')
-- !query schema
struct<to_timestamp(2020-01-365, yyyy-dd-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2020-10-350', 'yyyy-MM-DDD')
-- !query schema
struct<to_timestamp(2020-10-350, yyyy-MM-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2020-11-31-366', 'yyyy-MM-dd-DDD')
-- !query schema
struct<to_timestamp(2020-11-31-366, yyyy-MM-dd-DDD):timestamp>
-- !query output
NULL


-- !query
select from_csv('2018-366', 'date Date', map('dateFormat', 'yyyy-DDD'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
You may get a different result due to the upgrading of Spark 3.0: Fail to parse '2018-366' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.
