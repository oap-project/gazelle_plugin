<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
      http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.intel.oap</groupId>
  <artifactId>native-sql-engine-parent</artifactId>
  <version>1.5.0-SNAPSHOT</version>
  <packaging>pom</packaging>

  <name>Native SQL Engine Parent Pom</name>
  <url>https://github.com/oap-project/native-sql-engine.git</url>

  <licenses>
    <license>
      <name>The Apache Software License, Version 2.0</name>
      <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>
    </license>
  </licenses>

  <modules>
    <module>arrow-data-source</module>
    <module>native-sql-engine/core</module>
    <module>shims</module>
    <module>gazelle-dist</module>
  </modules>

  <profiles>
    <profile>
      <id>spark-3.1</id>
      <activation>
        <activeByDefault>true</activeByDefault>
      </activation>
      <properties>
        <spark.version>${spark311.version}</spark.version>
        <scala.version>2.12.10</scala.version>
        <jackson.version>2.10.0</jackson.version>
      </properties>
    </profile>
    <profile>
      <id>spark-3.2</id>
      <properties>
        <spark.version>${spark321.version}</spark.version>
        <scala.version>2.12.15</scala.version>
        <!--Jackson may be directly used in future UT. Align with the version in spark 3.2.-->
        <jackson.version>2.12.0</jackson.version>
        <maven.test.skip>true</maven.test.skip>
      </properties>
    </profile>
    <profile>
      <id>spark-3.2.1</id>
      <properties>
        <spark.version>${spark321.version}</spark.version>
        <scala.version>2.12.15</scala.version>
        <!--Jackson may be directly used in future UT. Align with the version in spark 3.2.-->
        <jackson.version>2.12.0</jackson.version>
        <maven.test.skip>true</maven.test.skip>
      </properties>
    </profile>
    <profile>
      <id>spark-3.2.2</id>
      <properties>
        <spark.version>${spark322.version}</spark.version>
        <scala.version>2.12.15</scala.version>
        <!--Jackson may be directly used in future UT. Align with the version in spark 3.2.-->
        <jackson.version>2.12.0</jackson.version>
        <maven.test.skip>true</maven.test.skip>
      </properties>
    </profile>
    <profile>
      <id>hadoop-2.7.4</id>
      <properties>
        <hadoop.version>2.7.4</hadoop.version>
      </properties>
    </profile>
    <profile>
      <id>hadoop-3.2</id>
      <activation>
        <property>
          <name>!hadoop.version</name>
        </property>
      </activation>
      <properties>
        <hadoop.version>3.2.0</hadoop.version>
      </properties>
    </profile>
    <profile>
      <id>dataproc-2.0</id>
      <properties>
        <hadoop.version>3.2.2</hadoop.version>
      </properties>
    </profile>
    <profile>
      <id>emr-6.3.0</id>
      <properties>
        <hadoop.version>3.2.1</hadoop.version>
      </properties>
    </profile>
    <profile>
      <id>incremental-scala-compiler</id>
      <activation>
        <activeByDefault>true</activeByDefault>
      </activation>
      <properties>
        <scala.recompile.mode>incremental</scala.recompile.mode>
      </properties>
    </profile>
    <profile>
      <id>full-scala-compiler</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <scala.recompile.mode>all</scala.recompile.mode>
      </properties>
    </profile>
    <profile>
      <id>arrow-netty</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <id>arrow-netty</id>
        <arrow-memory.artifact>arrow-memory-netty</arrow-memory.artifact>
      </properties>
    </profile>
  </profiles>

  <properties>
    <!--TODO: need to verify whether the settings will be overridden in profile-->
    <spark.version>3.1.1</spark.version>
    <spark311.version>3.1.1</spark311.version>
    <spark321.version>3.2.1</spark321.version>
    <spark322.version>3.2.2</spark322.version>
    <!-- Scala 2.12.10 is the version for default spark 3.1 -->
    <scala.version>2.12.10</scala.version>
    <java.version>1.8</java.version>
    <maven.compiler.source>${java.version}</maven.compiler.source>
    <maven.compiler.target>${java.version}</maven.compiler.target>
    <jackson.version>2.10.0</jackson.version>
    <scala.binary.version>2.12</scala.binary.version>
    <arrow.version>4.0.0</arrow.version>
    <log4j.version>2.17.1</log4j.version>
    <arrow-memory.artifact>arrow-memory-unsafe</arrow-memory.artifact>
    <hadoop.version>${hadoop.version}</hadoop.version>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    <arrow.script.dir>${project.basedir}/script</arrow.script.dir>
    <cpp_tests>OFF</cpp_tests>
    <build_arrow>ON</build_arrow>
    <static_arrow>OFF</static_arrow>
    <arrow.install.dir>${arrow.script.dir}/build/arrow_install</arrow.install.dir>
    <arrow_root>/usr/local</arrow_root>
    <build_protobuf>ON</build_protobuf>
    <build_jemalloc>ON</build_jemalloc>
    <native_avx512>ON</native_avx512>
    <project.prefix>spark-sql-columnar</project.prefix>
    <project.name.prefix>OAP Project Gazelle</project.name.prefix>
  </properties>

  <dependencyManagement>
    <dependencies>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <exclusions>
          <exclusion>
            <groupId>org.apache.arrow</groupId>
            <artifactId>*</artifactId>
          </exclusion>
        </exclusions>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <scope>provided</scope>
        <exclusions>
          <exclusion>
            <groupId>org.apache.arrow</groupId>
            <artifactId>arrow-vector</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
          </exclusion>
          <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-recipes</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
      <!-- test dependencies -->
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
        <scope>test</scope>
        <exclusions>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-recipes</artifactId>
          </exclusion>
          <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <exclusions>
          <exclusion>
            <groupId>org.apache.arrow</groupId>
            <artifactId>*</artifactId>
          </exclusion>
          <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
          </exclusion>
        </exclusions>
        <type>test-jar</type>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <exclusions>
          <exclusion>
            <groupId>org.apache.arrow</groupId>
            <artifactId>*</artifactId>
          </exclusion>
          <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
          </exclusion>
        </exclusions>
        <type>test-jar</type>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
        <scope>provided</scope>
        <exclusions>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
          </exclusion>
          <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
          </exclusion>
          <exclusion>
            <groupId>io.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
      <dependency>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest_${scala.binary.version}</artifactId>
        <version>3.2.3</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.logging.log4j</groupId>
        <artifactId>log4j-1.2-api</artifactId>
        <version>${log4j.version}</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.logging.log4j</groupId>
        <artifactId>log4j-api</artifactId>
        <version>${log4j.version}</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.logging.log4j</groupId>
        <artifactId>log4j-core</artifactId>
        <version>${log4j.version}</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.logging.log4j</groupId>
        <artifactId>log4j-slf4j-impl</artifactId>
        <version>${log4j.version}</version>
        <scope>test</scope>
      </dependency>
    </dependencies>
  </dependencyManagement>

  <build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-jar-plugin</artifactId>
        <version>3.2.0</version>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-enforcer-plugin</artifactId>
        <version>1.0.1</version>
        <executions>
          <execution>
            <id>enforce-versions</id>
            <goals>
              <goal>enforce</goal>
            </goals>
            <configuration>
              <rules>
                <requireMavenVersion>
                  <version>3.6.3</version>
                </requireMavenVersion>
              </rules>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>

</project>
