/*
 * Copyright 2020 Intel Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.intel.oap.sql.shims.spark311

import com.intel.oap.sql.shims.{SparkShims, ShimDescriptor}

import org.apache.spark.unsafe.map.BytesToBytesMap

class Spark320Shims extends SparkShims {

  override def getShimDescriptor: ShimDescriptor = SparkShimProvider.DESCRIPTOR

  override def shuffleBlockResolverWriteAndCommit(shuffleBlockResolver: IndexShuffleBlockResolver,
                                                  shuffleId: int, mapId: long, partitionLengths: Array[Long], dataTmp: File): Unit =
    shuffleBlockResolver.writeMetadataFileAndCommit(shuffleId, mapId, partitionLengths, null, dataTmp)

  override def getDatetimeRebaseMode(fileMetaData: FileMetaData, parquetOptions: ParquetOptions):
  SQLConf.LegacyBehaviorPolicy.Value = {
    val datetimeRebaseModeInRead = parquetOptions.datetimeRebaseModeInRead
    DataSourceUtils.datetimeRebaseMode(
      footerFileMetaData.getKeyValueMetaData.get,
      datetimeRebaseModeInRead)
  }

  override def newParquetFilters(parquetSchema: MessageType,
                                    pushDownDate: Boolean,
                                    pushDownTimestamp: Boolean,
                                    pushDownDecimal: Boolean,
                                    pushDownStringStartWith: Boolean,
                                    pushDownInFilterThreshold: Int,
                                    isCaseSensitive: Boolean,
                                    datetimeRebaseMode: LegacyBehaviorPolicy.Value): ParquetFilters = {
    return new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,
      pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive, datetimeRebaseMode)
  }

  override def newOutputWriter(writeQueue: ArrowWriteQueue, path: String): OutputWriter = {
    new OutputWriter {
      override def write(row: InternalRow): Unit = {
        val batch = row.asInstanceOf[FakeRow].batch
        writeQueue.enqueue(SparkVectorUtils
          .toArrowRecordBatch(batch))
      }

      override def close(): Unit = {
        writeQueue.close()
      }

      override def path(): String = {
        path
      }
    }
  }

  override def getBroadcastHashJoinOutputPartitioningExpandLimit(sqlContext: SQLContext, conf: SQLConf): Int = {
    conf.broadcastHashJoinOutputPartitioningExpandLimit
  }

  override def newSortShuffleWriter(resolver: IndexShuffleBlockResolver, BaseShuffleHandle,
                                    mapId: Long, context: TaskContext,
                                    shuffleExecutorComponents: ShuffleExecutorComponents): SortShuffleWriter = {
    new SortShuffleWriter(
      other,
      mapId,
      context,
      shuffleExecutorComponents)
  }

  // TODO: to see whether the below piece of code can be used for both spark 3.1/3.2.
  override def getMaxBroadcastRows(mode: BroadcastMode): Long = {
    // The below code is ported from BroadcastExchangeExec of spark 3.2.
    private lazy val maxBroadcastRows = mode match {
      case HashedRelationBroadcastMode(key, _)
        // NOTE: LongHashedRelation is used for single key with LongType. This should be kept
        // consistent with HashedRelation.apply.
        if !(key.length == 1 && key.head.dataType == LongType) =>
        // Since the maximum number of keys that BytesToBytesMap supports is 1 << 29,
        // and only 70% of the slots can be used before growing in UnsafeHashedRelation,
        // here the limitation should not be over 341 million.
        (BytesToBytesMap.MAX_CAPACITY / 1.5).toLong
      case _ => 512000000
    }
    maxBroadcastRows
  }

  override def getSparkSession(plan: SparkPlan): SparkSession = {
    plan.session
  }
}