{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import nested_scopes\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML('<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib import colors\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.sans-serif'] =  'Courier New'\n",
    "rcParams['font.family'] = 'Courier New'\n",
    "rcParams['font.size'] = '12'\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display,HTML\n",
    "import threading\n",
    "import collections\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress,Layout\n",
    "import time\n",
    "import threading\n",
    "import gzip\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, DateType,\n",
    "    TimestampType, StringType, LongType, IntegerType, DoubleType,FloatType)\n",
    "from pyspark.sql.functions import to_date, floor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import lit\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "\n",
    "import re\n",
    "import math\n",
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start analysis cluster and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executor per node: 4\n",
      "parallelism: 32\n",
      "memory: 58376m\n",
      "offheap:25600m\n"
     ]
    }
   ],
   "source": [
    "executors_per_node = 4\n",
    "nodes=1\n",
    "cores_per_executor=4\n",
    "task_per_core=2\n",
    "\n",
    "\n",
    "cache_size=100\n",
    "total_size=340000\n",
    "print('executor per node: {:d}\\nparallelism: {:d}\\nmemory: {:d}m\\noffheap:{:d}m'.format(executors_per_node,nodes*executors_per_node*cores_per_executor*task_per_core,int(math.floor(nodes*total_size/(nodes*executors_per_node)))-1024-int(math.floor(cache_size*1024/(nodes*executors_per_node))),int(math.floor(cache_size*1024/(nodes*executors_per_node)))))\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.default.parallelism\", \"{:d}\".format(nodes*executors_per_node))\n",
    "    .set('spark.executor.instances', '{:d}'.format(nodes*executors_per_node))\n",
    "    .set('spark.files.maxPartitionBytes', '256m')\n",
    "    .set('spark.app.name', 'pyspark_final')\n",
    "    .set('spark.rdd.compress', 'False')\n",
    "    .set('spark.serializer','org.apache.spark.serializer.KryoSerializer')    \n",
    "    .set('spark.executor.cores','{:d}'.format(cores_per_executor))\n",
    "    .set('spark.sql.shuffle.partitions','{:d}'.format(nodes*executors_per_node))\n",
    "    .set('spark.executor.memory', '{:d}m'.format(int(math.floor(nodes*total_size/(nodes*executors_per_node)))-1024-int(math.floor(cache_size*1024/(nodes*executors_per_node)))))\n",
    "    .set('spark.task.cpus','1')\n",
    "    .set('spark.driver.memory','128g')\n",
    "    .set('spark.sql.inMemoryColumnarStorage.compressed','False')\n",
    "    .set('spark.sql.inMemoryColumnarStorage.batchSize','100000')\n",
    "#    .set('spark.memory.storageFraction','0.8')\n",
    "#    .set('spark.memory.fraction','0.7')\n",
    "        \n",
    "    .set('spark.sql.execution.arrow.fallback.enabled','True')\n",
    "    .set('spark.sql.execution.arrow.enabled','True')\n",
    "    .set('spark.sql.execution.arrow.maxRecordsPerBatch','100000')\n",
    "    .set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        \n",
    "    .set('spark.memory.offHeap.enabled','True')\n",
    "    .set('spark.memory.offHeap.size','{:d}m'.format(int(math.floor(cache_size*1024/(nodes*executors_per_node)))))\n",
    "    .set('spark.executor.memoryOverhead','{:d}m'.format(int(math.floor(cache_size*1024/(nodes*executors_per_node)))+3000))\n",
    "    .set('spark.sql.join.preferSortMergeJoin','False')\n",
    "#    .set('spark.executor.extraJavaOptions',\n",
    "#          '-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:InitiatingHeapOccupancyPercent=35')\n",
    "    .set('spark.executor.extraJavaOptions',\n",
    "          '-XX:+UseParallelGC -XX:+UseParallelOldGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps')\n",
    "    .set('spark.dynamicAllocation.enabled', 'False')\n",
    "    .set('spark.shuffle.service.enabled', 'False')\n",
    "    .set('spark.dynamicAllocation.shuffleTracking.enabled', 'False')\n",
    "    .set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "        \n",
    "#    .set(\"spark.sql.session.timeZone\", \"PST\")\n",
    "     .set(\"spark.sql.session.timeZone\", \"Etc/GMT-8\")\n",
    "#     .set(\"spark.driver.extraJavaOptions\",\"-Duser.timezone=UTC+8\")\n",
    "    \n",
    "       )\n",
    "\n",
    "sc = SparkContext(conf=conf,master='yarn')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SQLContext(sc)\n",
    "time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparklog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run sparklog.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "appals=App_Log_Analysis(\"/user/root/spark_logs/application_1645145794788_0014\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data  /user/root/spark_logs/application_1645145794788_0014\n"
     ]
    }
   ],
   "source": [
    "appals.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appals.get_basic_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appals.get_app_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_df, dfx=appals.get_shuffle_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TraceView"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The command generate the traceview as json format. It can be open by traceviewer in https://chromium.googlesource.com/catapult.\n",
    "1. clone catapult\n",
    "2. copy/generate the traceview json file to a folder like /home/xxx/trace_result\n",
    "3. cd catapult/bin/\n",
    "4. python2.7 ./run_dev_server --no-install-hooks -d /home/xxx/trace_result -p1088\n",
    "5. open like in browser: http://127.0.0.1:1088/tracing_examples/trace_viewer.html#/tracing/test_data/application_1647347981137_0221_traceview.json\n",
    "\n",
    "if you need to open the traceview from hostip instead of 127.0.0.1, hack catapult :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The command generate the traceview as json format. It can be open by traceviewer in https://chromium.googlesource.com/catapult.\n",
    "#  1. clone catapult\n",
    "#  2. copy/generate the traceview json file to a folder like /home/xxx/trace_result\n",
    "#  3. cd catapult/bin/\n",
    "#  4. python2.7 ./run_dev_server --no-install-hooks -d /home/xxx/trace_result -p1088\n",
    "#  5. open like in browser: http://127.0.0.1:1088/tracing_examples/trace_viewer.html#/tracing/test_data/application_1647347981137_0221_traceview.json\n",
    "\n",
    "#  if you need to open the traceview from hostip instead of 127.0.0.1, hack catapult :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appals.generate_trace_view(\"/home/xxx/\" + appals.appid + \"_traceview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hot queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# querys which has biggest gap compared to vanilla\n",
    "for q in []:\n",
    "    appals.get_query_plan(queryid=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColumnarToRow Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the node name to ColumnarToRow if native columnarToRow is disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2r=appals.get_metrics_by_node(\"ArrowColumnarToRow\")\n",
    "c2rstagedf=c2r.where(\"`number of output rows` > 1000000\").where(\"ElapsedTime>100\").select(\"real_queryid\",\"Stage ID\").distinct().orderBy(\"Stage ID\").collect()\n",
    "c2rstage=[l['Stage ID'] for l in c2rstagedf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c2rstage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in c2rstage:\n",
    "    v=appals.get_query_plan(stageid=s,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2rquery=set([l['real_queryid'] for l in c2rstagedf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in c2rquery:\n",
    "    appals.get_query_plan(queryid=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for q in c2rquery:\n",
    "#    ps=appals.get_physical_plan(queryid=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RowToColumnar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the node name to RowToArrowColumnar if native RowToColumnar is disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2c=appals.get_metrics_by_node(\"ArrowRowToColumnarExec\")\n",
    "r2cstage=r2c.where(\"`number of input rows` > 1000000\").where(\"ElapsedTime>100\").select(\"Stage ID\").distinct().orderBy(\"Stage ID\").collect()\n",
    "r2cstage=[l['Stage ID'] for l in r2cstage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r2cstage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [l for l in r2cstage if l not in c2rstage]:\n",
    "    v=appals.get_query_plan(stageid=s,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=appals.get_physical_plan(shownops=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fallbacks in query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=appals.get_physical_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "197px",
    "left": "2188px",
    "top": "111px",
    "width": "269px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
