{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML('<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import builtins\n",
    "from itertools import chain\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, DateType,\n",
    "    TimestampType, StringType, LongType, IntegerType, DoubleType,FloatType)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "import re\n",
    "import collections\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas\n",
    "import numpy\n",
    "import time\n",
    "from pandasql import sqldf\n",
    "import html\n",
    "\n",
    "pandas.options.display.max_rows=1000\n",
    "pandas.options.display.max_columns=200\n",
    "pandas.options.display.float_format = '{:,}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress,Layout\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhdfs\n",
    "\n",
    "\n",
    "fs = pyhdfs.HdfsClient(hosts='10.0.2.125:50070', user_name='yuzhou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SparkLog_Analysis:\n",
    "    def __init__(self, appid,jobids,clients):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self,file):\n",
    "        self.file=file\n",
    "        self.starttime=0\n",
    "        self.df=None\n",
    "    \n",
    "    def load_data(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        if self.df==None:\n",
    "            self.load_data()\n",
    "        trace_events=[]\n",
    "        node=kwargs.get('node',\"node\")\n",
    "        trace_events.append(json.dumps({\"name\": \"process_name\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"name\":\" \"+node}}))\n",
    "        return trace_events\n",
    "\n",
    "    \n",
    "    def generate_trace_view(self, trace_output, **kwargs):\n",
    "        traces=[]\n",
    "        traces.extend(self.generate_trace_view_list(0,**kwargs))\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ],\n",
    "            \"displayTimeUnit\": \"ns\"\n",
    "        }'''\n",
    "\n",
    "        with open(trace_output+'.json', 'w') as outfile: \n",
    "            outfile.write(output)\n",
    "\n",
    "        display(HTML(\"<a href=http://xxx:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+trace_output+\".json>http://xxx:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+trace_output+\".json</a>\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app log analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf(\"long\")\n",
    "def isfinish_udf(s):\n",
    "    import json\n",
    "    s=json.loads(s)\n",
    "    def isfinish(root):\n",
    "        if \"isFinalPlan=false\" in root['simpleString'] or root['children'] is None:\n",
    "            return 0\n",
    "        for c in root[\"children\"]:\n",
    "            if isfinish(c)==0:\n",
    "                return 0\n",
    "        return 1\n",
    "    if len(s)>0:\n",
    "        return isfinish(s[0])\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "@pandas_udf(\"taskid long, start long, dur long, name string\", PandasUDFType.GROUPED_MAP)\n",
    "def time_breakdown(pdf):\n",
    "    ltime=pdf['Launch Time'][0]+2\n",
    "    pdf['start']=0\n",
    "    pdf['dur']=0\n",
    "    outpdf=[]\n",
    "    ratio=(pdf[\"Finish Time\"][0]-pdf[\"Launch Time\"][0])/pdf[\"Update\"].sum()\n",
    "    ratio=1 if ratio>1 else ratio\n",
    "    for idx,l in pdf.iterrows():\n",
    "        if(l[\"Update\"]*ratio>1):\n",
    "            outpdf.append([l[\"Task ID\"],ltime,int(l[\"Update\"]*ratio),l[\"mname\"]])\n",
    "            ltime=ltime+int(l[\"Update\"]*ratio)\n",
    "    if len(outpdf)>0:\n",
    "        return pandas.DataFrame(outpdf)\n",
    "    else:\n",
    "        return pandas.DataFrame({'taskid': pandas.Series([], dtype='long'),\n",
    "                   'start': pandas.Series([], dtype='long'),\n",
    "                   'dur': pandas.Series([], dtype='long'),\n",
    "                   'name': pandas.Series([], dtype='str'),\n",
    "                                })\n",
    "    \n",
    "class App_Log_Analysis(Analysis):\n",
    "    def __init__(self, file, jobids):\n",
    "        Analysis.__init__(self,file)\n",
    "        self.jobids=[] if jobids is None else [str(l) for l in jobids]\n",
    "        self.df=None\n",
    "        self.pids=[]\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"load data \", self.file)\n",
    "        jobids=self.jobids\n",
    "        df=spark.read.json(self.file)\n",
    "        \n",
    "        if 'App ID' in df.columns:\n",
    "            self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        else:\n",
    "            self.appid=\"Application-00000000\"\n",
    "                \n",
    "        if df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").count()>0:\n",
    "            self.dfacc=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").select(F.col(\"executionId\").alias(\"queryid\"),F.explode(\"accumUpdates\"))\n",
    "        else:\n",
    "            self.dfacc = None\n",
    "            \n",
    "        if \"sparkPlanInfo\" in df.columns:\n",
    "            self.queryplans=df.where(\"(Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart' or Event='org.apache.spark.sql.execution.ui.SparkListenerSQLAdaptiveExecutionUpdate') \\\n",
    "                                  and (sparkPlanInfo.nodeName!='AdaptiveSparkPlan' or sparkPlanInfo.simpleString='AdaptiveSparkPlan isFinalPlan=true') \").select(F.col(\"executionId\").alias(\"queryid\"),'physicalPlanDescription',\"sparkPlanInfo.*\")\n",
    "        else:\n",
    "            self.queryplans=None\n",
    "        \n",
    "        seen = set()\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.queryplans=self.queryplans.where(isfinish_udf(F.to_json(\"children\"))==1)\n",
    "        \n",
    "            self.allmetrics=[]\n",
    "            if self.queryplans.count() > 0:\n",
    "                metrics=self.queryplans.collect()\n",
    "                def get_metric(root):\n",
    "                    for l in root[\"metrics\"]:\n",
    "                        if l['accumulatorId'] not in seen:\n",
    "                            seen.add(l['accumulatorId'])\n",
    "                            self.allmetrics.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"]])\n",
    "                    if root['children'] is not None:\n",
    "                        for c in root[\"children\"]:\n",
    "                            get_metric(c)\n",
    "                for c in metrics:\n",
    "                    get_metric(c)\n",
    "        \n",
    "            amsdf=spark.createDataFrame(self.allmetrics)\n",
    "            amsdf=amsdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"type\").withColumnRenamed(\"_3\",\"Name\").withColumnRenamed(\"_4\",\"nodeName\")\n",
    "        \n",
    "        \n",
    "        if self.dfacc is not None:\n",
    "            self.dfacc=self.dfacc.select(\"queryid\",(F.col(\"col\")[0]).alias(\"ID\"),(F.col(\"col\")[1]).alias(\"Update\")).join(amsdf,on=[\"ID\"])\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.metricscollect=[l for l in self.allmetrics if l[1] in ['nsTiming','timing'] and (l[2].startswith(\"totaltime_\") or l[2].startswith(\"scan time\") or l[2].startswith(\"shuffle write time\") or l[2].startswith(\"shuffle spill time\")) and l[2] not in(\"totaltime_collectbatch\") ]\n",
    "        \n",
    "        #config=df.where(\"event='SparkListenerJobStart' and Properties.`spark.executor.cores` is not null\").select(\"Properties.*\").limit(1).collect()\n",
    "        config=df.select(\"`Spark Properties`.*\").where(\"`spark.app.id` is not null\").limit(1).collect()\n",
    "    \n",
    "        configdic=config[0].asDict()\n",
    "        self.parallelism=int(configdic['spark.sql.shuffle.partitions']) if 'spark.sql.shuffle.partitions' in configdic else 1\n",
    "        self.executor_cores=int(configdic['spark.executor.cores']) if 'spark.executor.cores' in configdic else 1\n",
    "        self.executor_instances=int(configdic['spark.executor.instances']) if 'spark.executor.instances' in configdic else 1\n",
    "        self.taskcpus= int(configdic['spark.task.cpus'])if 'spark.task.cpus' in configdic else 1\n",
    "        self.batchsize= int(configdic['spark.sql.execution.arrow.maxRecordsPerBatch'])if 'spark.sql.execution.arrow.maxRecordsPerBatch' in configdic else 1\n",
    "        \n",
    "        self.realexecutors = df.where(~F.isnull(F.col(\"Executor ID\"))).select(\"Executor ID\").distinct().count()\n",
    "\n",
    "        if \"spark.sql.execution.id\" in df.where(\"Event='SparkListenerJobStart'\").select(\"Properties.*\").columns:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.col(\"Properties.`spark.sql.execution.id`\").alias(\"queryid\"),\"Stage IDs\")\n",
    "        else:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.lit(0).alias(\"queryid\"),\"Stage IDs\")\n",
    "        df_jobend=df.where(\"Event='SparkListenerJobEnd'\").select(\"`Job ID`\",\"Completion Time\")\n",
    "        df_job=df_jobstart.join(df_jobend,\"Job ID\")\n",
    "        df_job=df_job.withColumnRenamed(\"Submission Time\",\"job_start_time\")\n",
    "        df_job=df_job.withColumnRenamed(\"Completion Time\",\"job_stop_time\")\n",
    "        self.df_job=df_job\n",
    "        \n",
    "        jobstage=df_job.select(\"*\",F.explode(\"Stage IDs\").alias(\"Stage ID\"))\n",
    "        task=df.where(\"(Event='SparkListenerTaskEnd' or Event='SparkListenerTaskStart') \").select(\"Event\",\"Stage ID\",\"task info.*\",\"task metrics.*\")\n",
    "        \n",
    "        self.failed_stages = [str(l['Stage ID']) for l in task.where(\"Failed='true'\").select(\"Stage ID\").distinct().collect()]\n",
    "        \n",
    "        self.speculativetask = task.where(\"speculative = 'true'\").count()\n",
    "        self.speculativekilledtask = task.where(\"speculative = true and killed='true'\").count()\n",
    "        self.speculativestage = task.where(\"speculative = true and killed='true'\").select(\"`Stage ID`\").distinct().count()\n",
    "        \n",
    "        validtsk = task.where(\"Event = 'SparkListenerTaskEnd' and (Failed<>'true' or killed<>'true')\").select(\"`Task ID`\")\n",
    "        task=task.join(validtsk,on='Task ID',how='inner')\n",
    "        \n",
    "        taskjob=task.\\\n",
    "            select(\"Host\",\"`Event`\",\"`Launch Time`\",\"`Executor ID`\",\"`Task ID`\",\"`Finish Time`\",\n",
    "                    \"`Stage ID`\",\"`Input Metrics`.`Bytes Read`\",\"`Disk Bytes Spilled`\",\"`Memory Bytes Spilled`\",\"`Shuffle Read Metrics`.`Local Bytes Read`\",\"`Shuffle Read Metrics`.`Remote Bytes Read`\",\n",
    "                   \"`Shuffle Write Metrics`.`Shuffle Bytes Written`\",\"`Executor Deserialize Time`\",\"`Shuffle Read Metrics`.`Fetch Wait Time`\",\"`Executor Run Time`\",\"`Shuffle Write Metrics`.`Shuffle Write Time`\",\n",
    "                   \"`Result Serialization Time`\",\"`Getting Result Time`\",\"`JVM GC Time`\",\"`Executor CPU Time`\",\"Accumulables\",\"Peak Execution Memory\",\n",
    "                    F.when(task['Finish Time']==0,task['Launch Time']).otherwise(task['Finish Time']).alias('eventtime')\n",
    "        ).join(jobstage,\"Stage ID\").where(\"`Finish Time` is null or `Finish Time` <=job_stop_time+5\")\n",
    "        \n",
    "        self.df=taskjob\n",
    "        \n",
    "        if len(jobids)>0:\n",
    "            self.df=self.df.where('`Job ID` in ({:s})'.format(','.join(jobids)))\n",
    "        \n",
    "        queryids=self.df.select(F.col(\"queryid\").astype(IntegerType())).distinct().where(\"queryid is not null\").orderBy(\"queryid\").toPandas()\n",
    "        \n",
    "        self.query_num=len(queryids)\n",
    "        if self.query_num>0:\n",
    "            queryidx=queryids.reset_index()\n",
    "            queryidx['index']=queryidx['index']+1\n",
    "            #tpcds query\n",
    "            if self.query_num==103:\n",
    "                queryidx['index']=queryidx['index'].map(tpcds_query_map)\n",
    "            qidx=spark.createDataFrame(queryidx)\n",
    "            qidx=qidx.withColumnRenamed(\"index\",\"real_queryid\")\n",
    "            self.df=self.df.join(qidx,on=\"queryid\",how=\"left\")\n",
    "            if self.dfacc is not None:\n",
    "                self.dfacc=self.dfacc.join(qidx,on=\"queryid\",how='left')\n",
    "\n",
    "            if self.queryplans:\n",
    "                self.queryplans=self.queryplans.join(qidx,\"queryid\",how=\"right\")\n",
    "        \n",
    "        self.df=self.df.fillna(0)\n",
    "        self.df=self.df.withColumn('Executor ID',F.when(F.col(\"Executor ID\")==\"driver\",1).otherwise(F.col(\"Executor ID\")))\n",
    "        self.df.cache()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##############################\n",
    "        \n",
    "        dfx=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"Stage ID\",\"Launch Time\",\"Finish Time\",\"Task ID\")\n",
    "        dfxpds=dfx.toPandas()\n",
    "        dfxpds.columns=[l.replace(\" \",\"_\") for l in dfxpds.columns]\n",
    "        dfxpds_ods=sqldf('''select * from dfxpds order by finish_time desc''')\n",
    "        criticaltasks=[]\n",
    "        idx=0\n",
    "        prefinish=0\n",
    "        launchtime=dfxpds_ods[\"Launch_Time\"][0]\n",
    "        criticaltasks.append([dfxpds_ods[\"Task_ID\"][0],launchtime,dfxpds_ods[\"Finish_Time\"][0]])\n",
    "        total_row=len(dfxpds_ods)\n",
    "\n",
    "        while True:\n",
    "            while idx<total_row:\n",
    "                if dfxpds_ods[\"Finish_Time\"][idx]-2<launchtime:\n",
    "                    break\n",
    "                idx=idx+1\n",
    "            else:\n",
    "                break\n",
    "            cur_finish=dfxpds_ods[\"Finish_Time\"][idx]\n",
    "            cur_finish=launchtime-1 if cur_finish>=launchtime else cur_finish\n",
    "            launchtime=dfxpds_ods[\"Launch_Time\"][idx]\n",
    "            criticaltasks.append([dfxpds_ods[\"Task_ID\"][idx],launchtime,cur_finish])\n",
    "        self.criticaltasks=criticaltasks\n",
    "\n",
    "    def get_physical_plan(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        queryid=kwargs.get('queryid',None)\n",
    "        shownops=kwargs.get(\"shownops\",['ArrowRowToColumnarExec','ColumnarToRow','RowToArrowColumnar','ArrowColumnarToRow','Filter','HashAggregate','Project','SortAggregate','SortMergeJoin','window'])\n",
    "        \n",
    "        desensitization=kwargs.get('desensitization',True)\n",
    "        \n",
    "        def get_fields(colss):\n",
    "            lvls=0\n",
    "            colns=[]\n",
    "            ks=\"\"\n",
    "            for c in colss:\n",
    "                if c==\",\" and lvls==0:\n",
    "                    colns.append(ks)\n",
    "                    ks=\"\"\n",
    "                    continue\n",
    "                if c==\" \" and ks==\"\":\n",
    "                    continue\n",
    "                if c==\"(\":\n",
    "                    lvls+=1\n",
    "                if c==\")\":\n",
    "                    lvls-=1\n",
    "                ks+=c\n",
    "            if ks!=\"\":\n",
    "                colns.append(ks)\n",
    "            return colns\n",
    "        \n",
    "        def get_column_names(s, opname, resultname, prefix, columns, funcs):\n",
    "            p=re.search(r\" \"+opname+\" \",s[0])\n",
    "            if p:\n",
    "                for v in s[1].split(\"\\n\"):\n",
    "                    if v.startswith(resultname):\n",
    "                        cols=re.search(\"\\[([^0-9].+)\\]\",v)\n",
    "                        if cols:\n",
    "                            colss=cols.group(1)\n",
    "                            colns=get_fields(colss)\n",
    "                            if opname+str(len(columns)) not in funcs:\n",
    "                                funcs[opname+str(len(columns))]=[]\n",
    "                            funcs[opname+str(len(columns))].extend(colns)\n",
    "                            for c in colns:\n",
    "                                if \" AS \" in c:\n",
    "                                    c=re.sub(\"#\\d+L*\",\"\",c)\n",
    "                                    colname=re.search(r\" AS (.+)\",c).group(1)\n",
    "                                    if colname not in columns:\n",
    "                                        columns[colname]=prefix\n",
    "        \n",
    "        plans=appals.queryplans.select('real_queryid','physicalPlanDescription').collect() if queryid is None else appals.queryplans.where(f\"real_queryid='{queryid}'\").select(\"physicalPlanDescription\").collect()\n",
    "        \n",
    "        for pr in range(0,len(plans)):\n",
    "            plan=plans[pr]['physicalPlanDescription']\n",
    "            nodes={}\n",
    "            lines=plan.split(\"\\n\")\n",
    "            for idx in range(0,len(lines)):\n",
    "                l=lines[idx]\n",
    "                if l=='+- == Final Plan ==':\n",
    "                    while l!='+- == Initial Plan ==':\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"\\(\\d+\\)$\",l).group(0)\n",
    "                        nodes[idv]=[l]\n",
    "                if l==\"== Physical Plan ==\":\n",
    "                    while not lines[idx+1].startswith(\"(\"):\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"\\(\\d+\\)$\",l).group(0)\n",
    "                        nodes[idv]=[l]\n",
    "                        \n",
    "                if l.startswith(\"(\"):\n",
    "                    idv=re.search(\"^\\(\\d+\\)\",l).group(0)\n",
    "                    if idv in nodes:\n",
    "                        desc=\"\"\n",
    "                        while l.strip()!=\"\":\n",
    "                            desc+=l+\"\\n\"\n",
    "                            idx+=1\n",
    "                            l=lines[idx]\n",
    "                        desc=re.sub(r\"#\\d+L*\",r\"\",desc)\n",
    "                        desc=re.sub(r\"= [^)]+\",r\"=\",desc)\n",
    "                        desc=re.sub(r\"IN \\([^)]\\)\",r\"IN ()\",desc)\n",
    "                        desc=re.sub(r\"In\\([^)]\\)\",r\"In()\",desc)\n",
    "                        desc=re.sub(r\"EqualTo\\(([^,]+),[^)]+\\)\",r\"EqualTo(\\1,)\",desc)\n",
    "                        desc=re.sub(r\"搜索广告\",r\"xxx\",desc)\n",
    "                        ## add all keyword replace here\n",
    "                        nodes[idv].append(desc)\n",
    "            tables={}\n",
    "            columns={}\n",
    "            functions={}\n",
    "            for s in nodes.values():\n",
    "                p=re.search(r\"Scan arrow [^.]*\\.([^ ]+)\",s[0])\n",
    "                if p:\n",
    "                    tn=p.group(1)\n",
    "                    if not tn in tables:\n",
    "                        tables[tn]=\"table\"\n",
    "                    if desensitization:\n",
    "                        s[0]=s[0].replace(tn,tables[tn])\n",
    "                        s[1]=s[1].replace(tn,tables[tn])\n",
    "                    colsv=[]\n",
    "                    schema=[]\n",
    "                    for v in s[1].split(\"\\n\"):\n",
    "                        if v.startswith(\"ReadSchema\"):\n",
    "                            cols=re.search(\"<(.*)>\",v)\n",
    "                            if cols:\n",
    "                                colss=cols.group(1).split(\",\")\n",
    "                                for c in colss:\n",
    "                                    cts=c.split(\":\")\n",
    "                                    ct=cts[0]\n",
    "                                    if not ct in columns:\n",
    "                                        if len(cts)==2:\n",
    "                                            cts[1]=cts[1]\n",
    "                                            columns[ct]=cts[1]+\"_\"\n",
    "                                        else:\n",
    "                                            columns[ct]=\"c_\"\n",
    "                        if v.startswith(\"Location\") and desensitization:\n",
    "                            s[1]=s[1].replace(v+\"\\n\",\"\")\n",
    "                            \n",
    "                get_column_names(s, \"Project\", \"Output\", \"proj_\", columns, functions)\n",
    "                get_column_names(s, \"HashAggregate\", \"Results\", \"shagg_\", columns, functions)\n",
    "                get_column_names(s, \"SortAggregate\", \"Results\", \"stagg_\", columns, functions)\n",
    "                get_column_names(s, \"ColumnarConditionProject\", \"Arguments\", \"cproj_\", columns, functions)\n",
    "                get_column_names(s, \"ColumnarHashAggregate\", \"Results\", \"cshagg_\", columns, functions)\n",
    "                get_column_names(s, \"Window\", \"Arguments\", \"window_\", columns, functions)\n",
    "\n",
    "            keys=[]\n",
    "            ckeys=list(columns.keys())\n",
    "            for l in range(0,len(ckeys)):\n",
    "                k1=ckeys[l]\n",
    "                for k in range(0,len(keys)):\n",
    "                    if keys[k] in k1:\n",
    "                        keys.insert(k,k1)\n",
    "                        break\n",
    "                else:\n",
    "                    keys.append(k1)\n",
    "                \n",
    "            for s in nodes.values():\n",
    "                s[1]=html.escape(s[1])\n",
    "                if desensitization:\n",
    "                    for c in keys:\n",
    "                        v=columns[c]\n",
    "                        if v.startswith(\"array\") or v.startswith(\"map\") or v.startswith(\"struct\"):\n",
    "                            s[1]=re.sub(c, '<span style=\"color:red;background-color:yellow\">'+html.escape(v)+\"</span>\",s[1])\n",
    "                        else:\n",
    "                            s[1]=re.sub(c, \"<font color=#33cc33>\"+html.escape(v)+\"</font>\",s[1])\n",
    "\n",
    "\n",
    "            htmls=['''<table style=\"table-layout:fixed;max-width: 100%;\">''']\n",
    "            qid=pr+1 if queryid is None else queryid\n",
    "            htmls.append(f\"<tr><td colspan=2>{qid}</td></tr>\")\n",
    "            for l in nodes.values():\n",
    "                if shownops is not None:\n",
    "                    for k in shownops:\n",
    "                        if \" \"+k+\" \" in l[0]:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                htmls.append(\"<tr>\")\n",
    "                htmls.append('<td width=33%><div align=\"left\" style=\"font-family:Courier New;overflow-wrap: anywhere\">')\n",
    "                htmls.append(l[0].replace(\" \",\"_\")\n",
    "                             .replace(\"ColumnarToRow\",\"<font color=blue>ColumnarToRow</font>\")\n",
    "                             .replace(\"RowToArrowColumnar\",\"<font color=blue>RowToArrowColumnar</font>\")\n",
    "                             .replace(\"ArrowColumnarToRow\",\"<font color=blue>ArrowColumnarToRow</font>\")\n",
    "                             .replace(\"ArrowRowToColumnar\",\"<font color=blue>ArrowRowToColumnar</font>\")\n",
    "                            )\n",
    "                htmls.append(\"</div></td>\")\n",
    "                htmls.append('<td width=66%><div align=\"left\" style=\"font-family:Courier New;overflow-wrap: anywhere\">')\n",
    "                ls=l[1].split(\"\\n\")\n",
    "                lsx=[]\n",
    "                for t in ls:\n",
    "                    cols=re.search(\"\\[([^0-9].+)\\]\",t)\n",
    "                    if cols:\n",
    "                        colss=cols.group(1)\n",
    "                        colns=get_fields(colss)\n",
    "                        t=re.sub(\"\\[([^0-9].+)\\]\",\"\",t)\n",
    "                        t+=\"[\"+'<span style=\"background-color:#ededed;\">;</span>'.join(colns)+\"]\"                        \n",
    "                    if \":\" in t:\n",
    "                        lsx.append(re.sub(r'^([^:]+:)',r'<font color=blue>\\1</font>',t))\n",
    "                    else:\n",
    "                        lsx.append(t)\n",
    "                htmls.append(\"<br>\".join(lsx))\n",
    "                htmls.append(\"</div></td>\")\n",
    "                htmls.append(\"</tr>\")\n",
    "            htmls.append(\"</table>\")\n",
    "            display(HTML(\"\\n\".join(htmls)))\n",
    "            \n",
    "            for k, v in functions.items():\n",
    "                functions[k]=[l for l in v if \"(\" in l]\n",
    "            for f in functions.values():\n",
    "                for idx in range(0,len(f)):\n",
    "                    for c in keys:\n",
    "                        v=columns[c]\n",
    "                        if v.startswith(\"array\") or v.startswith(\"map\") or v.startswith(\"struct\"):\n",
    "                            f[idx]=re.sub(c, '<span style=\"color:red;background-color:yellow\">'+html.escape(v)+\"</span>\",f[idx])\n",
    "                        else:\n",
    "                            f[idx]=re.sub(c, \"<font color=#33cc33>\"+html.escape(v)+\"</font>\",f[idx])\n",
    "            funchtml=\"<table>\"\n",
    "            for k,v in functions.items():\n",
    "                if shownops is not None:\n",
    "                    for ks in shownops:\n",
    "                        if \" \"+ks+\" \" in k:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                funchtml+=\"<tr><td width=10%>\"+k+'</td><td width=90%><table stype=\"width:100%;table-layout:fixed\">'\n",
    "                for f in v:\n",
    "                    funchtml+='<tr><td width=100% ><div align=\"left\" style=\"font-family:Courier New\">'+f+\"</div></td></tr>\"\n",
    "                funchtml+=\"</table></td></tr>\"\n",
    "            funchtml+=\"</table>\"    \n",
    "            display(HTML(funchtml))\n",
    "        \n",
    "        return plans\n",
    "        \n",
    "    def get_physical_allnodes(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        queryid=None\n",
    "        \n",
    "        plans=appals.queryplans.select('real_queryid','physicalPlanDescription').collect() if queryid is None else appals.queryplans.where(f\"real_queryid='{queryid}'\").select(\"physicalPlanDescription\").collect()\n",
    "        \n",
    "        allnodes={}\n",
    "        for pr in range(0,len(plans)):\n",
    "            plan=plans[pr]['physicalPlanDescription']\n",
    "            allnodes[pr]={}\n",
    "            nodes=allnodes[pr]\n",
    "            if plan is None:\n",
    "                continue\n",
    "            lines=plan.split(\"\\n\")\n",
    "            for idx in range(0,len(lines)):\n",
    "                l=lines[idx]\n",
    "                if l=='+- == Final Plan ==':\n",
    "                    while l!='+- == Initial Plan ==':\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"\\(\\d+\\)$\",l).group(0)\n",
    "                        nodes[idv]=[l]\n",
    "                if l.startswith(\"(\"):\n",
    "                    idv=re.search(\"^\\(\\d+\\)\",l).group(0)\n",
    "                    if idv in nodes:\n",
    "                        desc=\"\"\n",
    "                        while l!=\"\":\n",
    "                            desc+=l+\"\\n\"\n",
    "                            idx+=1\n",
    "                            l=lines[idx]\n",
    "                        nodes[idv].append(desc)\n",
    "        return allnodes\n",
    "        \n",
    "        \n",
    "    def get_basic_state(appals):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        display(HTML(\"<a href=http://sr525:18080/history/\"+appals.appid+\">http://sr525:18080/history/\"+appals.appid+\"</a>\"))\n",
    "        \n",
    "        errorcolor=\"#000000\" if appals.executor_instances == appals.realexecutors else \"#c0392b\"\n",
    "        \n",
    "        qtime=appals.get_query_time(plot=False)\n",
    "        sums=qtime.sum()\n",
    "        if len(appals.failed_stages)>0:\n",
    "            failure=\"<br>\".join([\"query: \" + str(l[\"real_queryid\"])+\"|stage: \" + str(l[\"Stage ID\"]) for l in appals.df.where(\"`Stage ID` in (\"+\",\".join(appals.failed_stages)+\")\").select(\"real_queryid\",\"Stage ID\").distinct().collect()])\n",
    "        else:\n",
    "            failure=\"\"\n",
    "        display(HTML(f'''\n",
    "        <table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">appid</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.appid}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.instances</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_instances}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.cores</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_cores}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle.partitions</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.parallelism}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">batch size</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.batchsize}</strong></span></td>\n",
    "                </tr>                \n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">real executors</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{appals.realexecutors}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Failed Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{failure}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{appals.speculativetask}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Killed Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{appals.speculativekilledtask}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Stage</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{appals.speculativestage}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">runtime</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['runtime'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">disk spilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['disk spilled'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">memspilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['memspilled'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">local_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['local_read'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">remote_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['remote_read'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle_write</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['shuffle_write'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">task run time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['run_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">ser_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['ser_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">f_wait_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['f_wait_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">gc_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['gc_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">input read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['input read'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">acc_task_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['acc_task_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "\n",
    "        '''))        \n",
    "        \n",
    "    def generate_trace_view_list_exec(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,**kwargs)\n",
    "        showcpu=kwargs.get('showcpu',False)\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        \n",
    "        showdf=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        \n",
    "        events=showdf.toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=self.starttime\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "        \n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                self.pids.append(pid)\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(\"task {:d} tid is {:s}.{:d}\".format(tsk,pid,t))\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "\n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "                    print(f'applog start time: {starttime}')\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['job id'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "\n",
    "                des_time=l['Executor Deserialize Time']\n",
    "                read_time=l['Fetch Wait Time']\n",
    "                exec_time=l['Executor Run Time']\n",
    "                write_time=math.floor(l['Shuffle Write Time']/1000000)\n",
    "                ser_time=l['Result Serialization Time']\n",
    "                getrst_time=l['Getting Result Time']\n",
    "                durtime=fintime-sstime-starttime;\n",
    "\n",
    "                times=[0,des_time,read_time,exec_time,write_time,ser_time,getrst_time]\n",
    "                time_names=['sched delay','deserialize time','read time','executor time','write time','serialize time','result time']\n",
    "                evttime=reduce((lambda x, y: x + y),times)\n",
    "                if evttime>durtime:\n",
    "                    times=[math.floor(l*1.0*durtime/evttime) for l in times]\n",
    "                else:\n",
    "                    times[0]=durtime-evttime\n",
    "\n",
    "                esstime=sstime\n",
    "                for idx in range(0,len(times)):\n",
    "                    if times[idx]>0:\n",
    "                        trace_events.append({\n",
    "                             'tid':pid+int(t),\n",
    "                             'ts':esstime,\n",
    "                             'dur':times[idx],                \n",
    "                             'pid':pid,\n",
    "                             'ph':'X',\n",
    "                             'name':time_names[idx]})\n",
    "                        if idx==3:\n",
    "                            trace_events.append({\n",
    "                                 'tid':pid+int(t),\n",
    "                                 'ts':esstime,\n",
    "                                 'dur':l['JVM GC Time'],\n",
    "                                 'pid':pid,\n",
    "                                 'ph':'X',\n",
    "                                 'name':'GC Time'})\n",
    "                            if showcpu:\n",
    "                                trace_events.append({\n",
    "                                     'tid':pid+int(t),\n",
    "                                     'ts':esstime,\n",
    "                                     'pid':pid,\n",
    "                                     'ph':'C',\n",
    "                                     'name':'cpu% {:d}'.format(pid+int(t)),\n",
    "                                     'args':{'value':l['Executor CPU Time']/1000000.0/times[idx]}})\n",
    "                                trace_events.append({\n",
    "                                     'tid':pid+int(t),\n",
    "                                     'ts':esstime+times[idx],\n",
    "                                     'pid':pid,\n",
    "                                     'ph':'C',\n",
    "                                     'name':'cpu% {:d}'.format(pid+int(t)),\n",
    "                                     'args':{'value':0}})\n",
    "                        esstime=esstime+times[idx]\n",
    "        self.starttime=starttime\n",
    "        return [json.dumps(l) for l in trace_events]\n",
    "\n",
    "    def generate_trace_view_list(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,**kwargs)\n",
    "        showcpu=kwargs.get('showcpu',False)\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        \n",
    "        showdf=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        \n",
    "        showdf=showdf.orderBy([\"eventtime\", \"Finish Time\"], ascending=[1, 0])\n",
    "        \n",
    "        events=showdf.drop(\"Accumulables\").toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=self.starttime\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "        \n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        tskmap={}\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                self.pids.append(pid)\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(f\"task {tsk} tid is {pid}.{t}\")\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "                \n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "                    print(f'applog start time: {starttime}')\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['Job ID'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "                tskmap[tsk]={'pid':pid,'tid':pid+int(t)}\n",
    "\n",
    "        self.starttime=starttime\n",
    "        self.tskmap=tskmap\n",
    "        output=[json.dumps(l) for l in trace_events]\n",
    "        \n",
    "        df=self.df\n",
    "        \n",
    "        if showcpu and len(self.metricscollect)>0:\n",
    "            metricscollect=self.metricscollect\n",
    "            metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "            m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "            metric_name_df = spark.createDataFrame(metricscollect)\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "\n",
    "            met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "            met_df=met_df.where(\"Update>1\")\n",
    "\n",
    "            metdfx=met_df.groupBy(\"Task ID\",\"elapsedtime\").agg(F.sum(\"Update\").alias(\"totalCnt\"))\n",
    "            taskratio=metdfx.withColumn(\"ratio\",F.when(F.col(\"totalCnt\")<F.col(\"elapsedtime\"),1).otherwise(F.col(\"elapsedtime\")/F.col(\"totalCnt\"))).select(\"Task ID\",\"ratio\")\n",
    "            met_df=met_df.join(taskratio,on=\"Task ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.col(\"Update\")*F.col(\"ratio\"))\n",
    "\n",
    "            w = (Window.partitionBy('Task ID').orderBy(F.desc(\"Update\")).rangeBetween(Window.unboundedPreceding, 0))\n",
    "            met_df=met_df.withColumn('cum_sum', F.sum('Update').over(w))\n",
    "\n",
    "            met_df=met_df.withColumn(\"starttime\",F.col(\"Launch Time\")+F.col(\"cum_sum\")-F.col(\"Update\"))\n",
    "\n",
    "            tskmapdf = spark.createDataFrame(pandas.DataFrame(self.tskmap).T.reset_index())\n",
    "            met_df=met_df.join(tskmapdf,on=[met_df[\"Task ID\"]==tskmapdf[\"index\"]])\n",
    "\n",
    "            rstdf=met_df.select(\n",
    "                F.col(\"tid\"),\n",
    "                F.round(F.col(\"starttime\")-self.starttime,0).alias(\"ts\"),\n",
    "                F.round(F.col(\"Update\"),0).alias(\"dur\"),\n",
    "                F.col(\"pid\"),\n",
    "                F.lit(\"X\").alias(\"ph\"),\n",
    "                F.col(\"mname\").alias(\"name\")\n",
    "            ).where(F.col(\"ts\").isNotNull()).orderBy('ts')\n",
    "\n",
    "            output.extend(rstdf.toJSON().collect())\n",
    "\n",
    "            qtime=df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"real_queryid\").agg(F.min(\"Finish Time\").alias(\"time\"))\n",
    "            output.extend(qtime.select(\n",
    "                F.lit(\"i\").alias(\"ph\"),\n",
    "                (F.col(\"time\")-starttime).alias('ts'),\n",
    "                F.lit(0).alias(\"pid\"),\n",
    "                F.lit(0).alias(\"tid\"),\n",
    "                F.lit(\"p\").alias(\"s\")\n",
    "            ).toJSON().collect())\n",
    "        \n",
    "        self.starttime=starttime\n",
    "        \n",
    "        if kwargs.get(\"show_criticalshow_time_metric_path\",True):\n",
    "            output.extend(self.generate_critical_patch_traceview(hostid-1))\n",
    "        \n",
    "        return output        \n",
    "\n",
    "    def generate_critical_patch_traceview(self,pid):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        traces=[]\n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd' and real_queryid is not null\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        traces.extend(df_ctsk.select(F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)+1).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")-1).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"stg\"),F.col(\"Stage ID\")).alias(\"name\"),\n",
    "                      F.struct(\n",
    "                          F.col(\"Task ID\").alias('taskid'),\n",
    "                          F.col(\"Executor ID\").astype(IntegerType()).alias('exec_id'),\n",
    "                          F.col(\"Host\").alias(\"host\"),\n",
    "                          ).alias(\"args\")\n",
    "                        ).toJSON().collect())\n",
    "        traces.extend(df.groupBy(\"real_queryid\").agg(F.max(\"Finish Time\").alias(\"finish\"),F.min(\"Launch Time\").alias(\"launch\")).select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"qry\"),F.col(\"real_queryid\")).alias(\"name\")).toJSON().collect())\n",
    "\n",
    "\n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df_ctsk.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        metric_name_df=metric_name_df.where(\"mname <> 'totaltime_collectbatch'\")\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        \n",
    "        #pandas UDF doesn't work. hang\n",
    "        #tmbk=met_df.groupBy('Task ID').apply(time_breakdown)\n",
    "        \n",
    "        w=Window.partitionBy('Task ID')\n",
    "        met_df1=met_df.withColumn(\"sum_update\",F.sum(\"Update\").over(w))\n",
    "        met_df2=met_df1.withColumn(\"ratio\",(F.col(\"Finish Time\")-F.col(\"Launch Time\")-2)/F.col(\"sum_update\"))\n",
    "        met_df3=met_df2.withColumn(\"ratio\",F.when(F.col(\"ratio\")>1,1).otherwise(F.col(\"ratio\")))\n",
    "        met_df4=met_df3.withColumn(\"update_ratio\",F.floor(F.col(\"ratio\")*F.col(\"Update\")))\n",
    "        met_df5=met_df4.where(F.col(\"update_ratio\")>2)\n",
    "        w = (Window.partitionBy('Task ID').orderBy(F.desc(\"update_ratio\")).rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "        met_df6=met_df5.withColumn('ltime_dur', F.sum('update_ratio').over(w))\n",
    "        met_df8=met_df6.withColumn(\"ltime\",F.col(\"ltime_dur\")+F.col(\"Launch Time\")-F.col(\"update_ratio\"))\n",
    "\n",
    "        tmbk=met_df8.withColumn(\"taskid\",F.col(\"Task ID\")).withColumn(\"start\",F.col(\"ltime\")+F.lit(1)).withColumn(\"dur\",F.col(\"update_ratio\")-F.lit(1)).withColumn(\"name\",F.col(\"mname\"))\n",
    "        \n",
    "        \n",
    "        traces.extend(tmbk.select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"start\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"dur\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.col(\"name\").alias(\"name\")).toJSON().collect())\n",
    "        traces.append(json.dumps({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"critical path\"}\n",
    "                      }))\n",
    "        return traces    \n",
    "    \n",
    "    def show_Stage_histogram(apps,stageid,bincount):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        inputsize = apps.df.where(\"`Stage ID`={:d}\".format(stageid)).select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Task ID\") \\\n",
    "                      .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "\n",
    "\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) )\\\n",
    "                        .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                        .fillna(0) \\\n",
    "                        .select(F.col('Host'), \n",
    "                                F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                F.round((F.col('`input read`')+F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input'))\n",
    "        stage37=stage37.cache()\n",
    "        hist_elapsedtime=stage37.select('elapsedtime').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        hist_input=stage37.select('input').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        fig, axs = plt.subplots(figsize=(30, 5),nrows=1, ncols=2)\n",
    "        ax=axs[0]\n",
    "        binSides, binCounts = hist_elapsedtime\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} elapsed time breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        ax=axs[1]\n",
    "        binSides, binCounts = hist_input\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} input data breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        out=stage37\n",
    "        outpds=out.toPandas()\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=3, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "        groups= outpds.groupby('Host')\n",
    "        for name, group in groups:\n",
    "            axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "        axs[0].set_xlabel('input size (MB)')\n",
    "        axs[0].set_ylabel('elapsed time (s)')\n",
    "\n",
    "        axs[0].legend()\n",
    "\n",
    "        axs[0].get_shared_y_axes().join(axs[0], axs[1])\n",
    "\n",
    "        sns.violinplot(y='elapsedtime', x='Host', data=outpds,palette=['g'],ax=axs[1])\n",
    "\n",
    "        sns.violinplot(y='input', x='Host', data=outpds,palette=['g'],ax=axs[2])\n",
    "\n",
    "        #ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "        #ax.yaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "\n",
    "        if False:\n",
    "            out=stage37\n",
    "            vecAssembler = VectorAssembler(inputCols=[\"input\",'elapsedtime'], outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "            new_df = vecAssembler.transform(out)\n",
    "            kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "            model = kmeans.fit(new_df.select('features'))\n",
    "            transformed = model.transform(new_df)\n",
    "\n",
    "\n",
    "            outpds=transformed.select('Host','elapsedtime','input','prediction').toPandas()\n",
    "\n",
    "            fig, axs = plt.subplots(nrows=1, ncols=2, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "            plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "            groups= outpds.groupby('prediction')\n",
    "            for name, group in groups:\n",
    "                axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "            axs[0].legend()\n",
    "\n",
    "            bars=transformed.where('prediction=1').groupBy(\"Host\").count().toPandas()\n",
    "\n",
    "            axs[1].bar(bars['Host'], bars['count'], 0.4, color='coral')\n",
    "            axs[1].set_title('cluster=1')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def show_Stages_hist(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df\n",
    "        \n",
    "        totaltime=df.where(\"event='SparkListenerTaskEnd'\" ).agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).collect()[0]['total_time']\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`').agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).orderBy('total_time', ascending=False).toPandas()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "        fig, ax = plt.subplots(figsize=(30, 5))\n",
    "\n",
    "        rects1 = ax.plot(stage_time['index'],stage_time['acc_total'],'b.-')\n",
    "        ax.set_xticks(stage_time['index'])\n",
    "        ax.set_xticklabels(stage_time['Stage ID'])\n",
    "        ax.set_xlabel('stage')\n",
    "        ax.grid(which='major', axis='x')\n",
    "        plt.show()\n",
    "        shownstage=[]\n",
    "        for x in stage_time.index:\n",
    "            if stage_time['acc_total'][x]<=threshold:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "            else:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "                break\n",
    "        for row in shownstage:\n",
    "            apps.show_Stage_histogram(row,bincount) \n",
    "            \n",
    "    def get_hottest_stages(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df.where(\"queryid is not NULL\")\n",
    "\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`','Job ID','real_queryid').agg(\n",
    "            F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time'),\n",
    "            F.stddev(F.col('Finish Time')/1000-F.col('Launch Time')/1000).alias('stdev_time'),\n",
    "            F.count(\"*\").alias(\"cnt\"),\n",
    "            F.first('queryid').astype(IntegerType()).alias('queryid')\n",
    "            )\\\n",
    "            .select('`Stage ID`','Job ID','real_queryid','queryid',\n",
    "                    (F.col(\"total_time\")/1000/(F.when(F.col(\"cnt\")>F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus),F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus)).otherwise(F.col(\"cnt\")))).alias(\"total_time\"),\n",
    "                    F.col(\"stdev_time\")\n",
    "                   ).orderBy('total_time', ascending=False).toPandas()\n",
    "\n",
    "        totaltime=stage_time['total_time'].sum()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time['total'] = stage_time['total_time']/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "\n",
    "        shownstage=stage_time.loc[stage_time['acc_total'] <=threshold]\n",
    "        shownstage['stg']=shownstage['real_queryid'].astype(str)+'_'+shownstage['Job ID'].astype(str)+'_'+shownstage['Stage ID'].astype(str)\n",
    "        if plot:\n",
    "            shownstage.plot.bar(x=\"stg\",y=\"total\",figsize=(30,8))\n",
    "\n",
    "\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=max(stage_time.queryid))\n",
    "        cmap = matplotlib.cm.get_cmap('brg')\n",
    "        def setbkcolor(x):\n",
    "            rgba=cmap(norm(x['queryid']))\n",
    "            return ['background-color:rgba({:d},{:d},{:d},1); color:white'.format(int(rgba[0]*255),int(rgba[1]*255),int(rgba[2]*255))]*9\n",
    "\n",
    "        if plot:\n",
    "            display(stage_time.style.apply(setbkcolor,axis=1).format({\"total_time\":lambda x: '{:,.2f}'.format(x),\"acc_total\":lambda x: '{:,.2%}'.format(x),\"total\":lambda x: '{:,.2%}'.format(x)}))\n",
    "        \n",
    "        return stage_time\n",
    "\n",
    "    def scatter_elapsetime_input(apps,stageid):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) ).select(F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),F.round((F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input')).toPandas()\n",
    "        stage37.plot.scatter('input','elapsedtime',figsize=(30, 5))\n",
    "\n",
    "    def get_critical_path_stages(self):     \n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        df_ctsk=df_ctsk.withColumn(\"elapsed\",(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000)\n",
    "        return df_ctsk.where(\"elapsed>10\").orderBy(F.desc(\"elapsed\")).select(\"real_queryid\",F.round(\"elapsed\",2).alias(\"elapsed\"),\"Host\",\"executor ID\",\"Stage ID\",\"Task ID\",F.round(F.col(\"Bytes Read\")/1000000,0).alias(\"file read\"),F.round((F.col(\"Local Bytes Read\")+F.col(\"Remote Bytes Read\"))/1000000,0).alias(\"shuffle read\")).toPandas()\n",
    "        \n",
    "    def show_time_metric(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        taskids=kwargs.get(\"taskids\",None)\n",
    "        \n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        \n",
    "        showexecutor=kwargs.get(\"showexecutor\",True) if not taskids else False\n",
    "        queryid = query[0] if query else 0\n",
    "        \n",
    "        df=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        df=df.where(F.col(\"real_queryid\").isin(query)) if query else df.where(\"queryid is not NULL\")\n",
    "\n",
    "        df=df.where(F.col(\"Task ID\").isin(taskids)) if taskids else df\n",
    "\n",
    "        exec_cores=1 if taskids else self.executor_cores\n",
    "        execs=1 if taskids else self.executor_instances\n",
    "\n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        runtime=metrics_explode.agg(F.round(F.max(\"Finish Time\")/1000-F.min(\"Launch Time\")/1000,2).alias(\"runtime\")).collect()[0][\"runtime\"]\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        outpdf=met_df.groupBy(\"`Executor ID`\",\"mname\").sum(\"Update\").orderBy(\"Executor ID\").toPandas()\n",
    "\n",
    "        met_time_cnt=df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        exectime=met_time_cnt.groupBy(\"Executor ID\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\"),F.sum(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"tasktime\"))\n",
    "\n",
    "        totaltime_query=met_time_cnt.groupBy(\"real_queryid\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\")).agg(F.sum(\"totaltime\").alias(\"totaltime\")).collect()\n",
    "        totaltime_query=totaltime_query[0][\"totaltime\"]\n",
    "        \n",
    "        pdf=exectime.toPandas()\n",
    "        exeids=set(outpdf['Executor ID'])\n",
    "        outpdfs=[outpdf[outpdf[\"Executor ID\"]==l] for l in exeids]\n",
    "        tasktime=pdf.set_index(\"Executor ID\").to_dict()['tasktime']\n",
    "\n",
    "        def comb(l,r):\n",
    "            execid=list(r['Executor ID'])[0]\n",
    "            lp=r[['mname','sum(Update)']]\n",
    "            lp.columns=[\"mname\",\"val_\"+execid]\n",
    "            idle=totaltime_query*exec_cores-tasktime[execid]\n",
    "            nocount=tasktime[execid]-sum(lp[\"val_\"+execid])\n",
    "            if idle<0:\n",
    "                idle=0\n",
    "            if nocount<0:\n",
    "                nocount=0\n",
    "            lp=lp.append([{\"mname\":\"idle\",\"val_\"+execid:idle}])\n",
    "            lp=lp.append([{\"mname\":\"not_counted\",\"val_\"+execid:nocount}])\n",
    "            if l is not None:\n",
    "                return pandas.merge(lp, l,on=[\"mname\"],how='outer')\n",
    "            else:\n",
    "                return lp\n",
    "\n",
    "        rstpdf=None\n",
    "        for l in outpdfs[0:]:\n",
    "            rstpdf=comb(rstpdf,l)\n",
    "            \n",
    "        for l in [l for l in rstpdf.columns if l!=\"mname\"]:\n",
    "            rstpdf[l]=rstpdf[l]/1000/exec_cores\n",
    "    \n",
    "        rstpdf=rstpdf.sort_values(by=\"val_\"+list(exeids)[0],axis=0,ascending=False)\n",
    "        if showexecutor and plot:\n",
    "            rstpdf.set_index(\"mname\").T.plot.bar(stacked=True,figsize=(30,8))\n",
    "        pdf_sum=pandas.DataFrame(rstpdf.set_index(\"mname\").T.sum())\n",
    "        totaltime=totaltime_query/1000\n",
    "        pdf_sum[0]=pdf_sum[0]/(execs)\n",
    "        pdf_sum[0][\"idle\"]=(totaltime_query-sum(tasktime.values())/execs/exec_cores)/1000\n",
    "        pdf_sum=pdf_sum.sort_values(by=0,axis=0,ascending=False)\n",
    "        pdf_sum=pdf_sum.T\n",
    "        pdf_sum.columns=[\"{:>2.0f}%_{:s}\".format(pdf_sum[l][0]/totaltime*100,l) for l in pdf_sum.columns]\n",
    "        matplotlib.rcParams['font.sans-serif'] = \"monospace\"\n",
    "        matplotlib.rcParams['font.family'] = \"monospace\"\n",
    "        import matplotlib.font_manager as font_manager\n",
    "        if plot:\n",
    "            ax=pdf_sum.plot.bar(stacked=True,figsize=(30,8))\n",
    "            font = font_manager.FontProperties(family='monospace',\n",
    "                                               style='normal', size=14)\n",
    "            ax.legend(prop=font,loc=4)\n",
    "            plt.title(\"{:s} q{:d} executors={:d} cores_per_executor={:d} parallelism={:d} sumtime={:.0f} runtime={:.0f}\".format(self.file.split(\"/\")[2],queryid,self.executor_instances,self.executor_cores,self.parallelism,totaltime,runtime),fontdict={'fontsize':24})\n",
    "        return pdf_sum\n",
    "\n",
    "    def show_critical_path_time_breakdown(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        return self.show_time_metric(taskids=[l[0].item() for l in self.criticaltasks])\n",
    "    \n",
    "    def get_spark_config(self):\n",
    "        df=spark.read.json(self.file)\n",
    "        self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        pandas.set_option('display.max_rows', None)\n",
    "        pandas.set_option('display.max_columns', None)\n",
    "        pandas.set_option('display.max_colwidth', 100000)\n",
    "        return df.select(\"Properties.*\").where(\"`spark.app.id` is not null\").limit(1).toPandas().T\n",
    "    \n",
    "    def get_app_name(self):\n",
    "        cfg=self.get_spark_config()\n",
    "        display(HTML(\"<font size=5 color=red>\" + cfg.loc[cfg.index=='spark.app.name'][0][0]+\"</font>\"))\n",
    "        \n",
    "        \n",
    "    def get_query_time(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        showtable=kwargs.get(\"showtable\",True)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "           \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)) if queryid else self.df.where(\"queryid is not NULL\")\n",
    "        \n",
    "            \n",
    "        stages=df.select(\"real_queryid\",\"Stage ID\").distinct().orderBy(\"Stage ID\").groupBy(\"real_queryid\").agg(F.collect_list(\"Stage ID\").alias(\"stages\")).orderBy(\"real_queryid\")\n",
    "        runtimeacc=df.where(\"Event='SparkListenerTaskEnd'\") \\\n",
    "                      .groupBy(\"real_queryid\") \\\n",
    "                      .agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"acc_task_time\"))\n",
    "        inputsize = df.select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"real_queryid\") \\\n",
    "                      .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read\")).orderBy(\"real_queryid\")\n",
    "        if self.dfacc is not None:\n",
    "            inputsizev1 = self.dfacc.where(\"Name='size of files read'\").groupBy(\"real_queryid\").agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read v1\")).orderBy(\"real_queryid\")\n",
    "            inputsize=inputsize.join(inputsizev1,on=\"real_queryid\",how=\"outer\")\n",
    "            inputsize=inputsize.withColumn(\"input read\",F.coalesce(F.col(\"input read\"),F.col(\"input read v1\"))).drop(\"input read v1\")\n",
    "        \n",
    "        outputrows = df.select(\"real_queryid\",\"Stage ID\",\"Stage ID\",F.explode(\"Accumulables\"))\\\n",
    "                        .select(\"real_queryid\",\"Stage ID\",\"Stage ID\",\"col.*\")\\\n",
    "                        .where(\"Name='number of output rows'\")\\\n",
    "                        .groupBy(\"real_queryid\")\\\n",
    "                        .agg(F.round(F.sum(\"Update\")/1000000000,2).alias(\"output rows\"))\n",
    "        \n",
    "        stages=runtimeacc.join(stages,on=\"real_queryid\",how=\"left\")\n",
    "        stages=inputsize.join(stages,on=\"real_queryid\",how=\"left\")\n",
    "        stages=stages.join(outputrows,on='real_queryid',how=\"left\")\n",
    "        \n",
    "        out=df.groupBy(\"real_queryid\").agg(\n",
    "            F.round(F.max(\"job_stop_time\")/1000-F.min(\"job_start_time\")/1000,2).alias(\"runtime\"),\n",
    "            F.round(F.sum(\"Disk Bytes Spilled\")/1024/1024/1024,2).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(\"Memory Bytes Spilled\")/1024/1024/1024,2).alias(\"memspilled\"),\n",
    "            F.round(F.sum(\"Local Bytes Read\")/1024/1024/1024,2).alias(\"local_read\"),\n",
    "            F.round(F.sum(\"Remote Bytes Read\")/1024/1024/1024,2).alias(\"remote_read\"),\n",
    "            F.round(F.sum(\"Shuffle Bytes Written\")/1024/1024/1024,2).alias(\"shuffle_write\"),\n",
    "            F.round(F.sum(\"Executor Deserialize Time\")/1000/self.parallelism,2).alias(\"deser_time\"),\n",
    "            F.round(F.sum(\"Executor Run Time\")/1000/self.parallelism,2).alias(\"run_time\"),\n",
    "            F.round(F.sum(\"Result Serialization Time\")/1000/self.parallelism,2).alias(\"ser_time\"),\n",
    "            F.round(F.sum(\"Fetch Wait Time\")/1000/self.parallelism,2).alias(\"f_wait_time\"),\n",
    "            F.round(F.sum(\"JVM GC Time\")/1000/self.parallelism,2).alias(\"gc_time\"),\n",
    "            F.round(F.max(\"Peak Execution Memory\")/1000000000*self.executor_instances*self.executor_cores,2).alias(\"peak_mem\"),\n",
    "            F.max(\"queryid\").alias(\"queryid\")\n",
    "            ).join(stages,\"real_queryid\",how=\"left\").orderBy(\"real_queryid\").toPandas().set_index(\"real_queryid\")\n",
    "        out[\"executors\"]=self.executor_instances\n",
    "        out[\"core/exec\"]=self.executor_cores\n",
    "        out[\"task.cpus\"]=self.taskcpus\n",
    "        out['parallelism']=self.parallelism\n",
    "        \n",
    "        if not showtable:\n",
    "            return out\n",
    "\n",
    "        def highlight_greater(x):\n",
    "            m1 = x['acc_task_time'] / x['runtime'] * 100\n",
    "            m2 = x['run_time'] / x['runtime'] * 100\n",
    "            m3 = x['f_wait_time'] / x['runtime'] * 100\n",
    "            \n",
    "\n",
    "            df1 = pandas.DataFrame('', index=x.index, columns=x.columns)\n",
    "\n",
    "            df1['acc_task_time'] = m1.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['run_time'] = m2.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['f_wait_time'] = m3.apply(lambda x: 'background-image: linear-gradient(to right,#d65f5f {:f}%,white {:f}%)'.format(x,x))\n",
    "            return df1\n",
    "\n",
    "\n",
    "        cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "        if plot:\n",
    "            display(out.style.apply(highlight_greater, axis=None).background_gradient(cmap=cm,subset=['input read', 'shuffle_write']))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_query_time_metric(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        querids=self.df.select(\"queryid\").distinct().collect()\n",
    "        for idx,q in enumerate([l[\"queryid\"] for l in querids]):\n",
    "            self.show_time_metric(query=[q,],showexecutor=False)\n",
    "            \n",
    "    def getOperatorCount(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        df=spark.read.json(self.file)\n",
    "        queryids=self.df.select(F.col(\"queryid\").astype(LongType()),F.col(\"real_queryid\")).distinct().orderBy(\"real_queryid\")\n",
    "        queryplans=self.queryplans.collect()\n",
    "        list_queryid=[l.real_queryid for l in queryids.collect()]\n",
    "\n",
    "        def get_child(execid,node):\n",
    "            if node[\"nodeName\"] not in qps:\n",
    "                qps[node[\"nodeName\"]]={l:0 for l in list_queryid}\n",
    "            qps[node[\"nodeName\"]][execid]=qps[node[\"nodeName\"]][execid]+1\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "\n",
    "        qps={}\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        return pandas.DataFrame(qps).T.sort_index(axis=0)        \n",
    "    \n",
    "    def get_query_plan(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        stageid=kwargs.get(\"stageid\",None)\n",
    "        \n",
    "        outputstage=kwargs.get(\"outputstage\",None)\n",
    "        \n",
    "        show_plan_only=kwargs.get(\"show_plan_only\",False)\n",
    "        show_simple_string=kwargs.get(\"show_simple_string\",False)\n",
    "\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        colors=[\"#{:02x}{:02x}{:02x}\".format(int(l[0]*255),int(l[1]*255),int(l[2]*255)) for l in matplotlib.cm.get_cmap('tab20').colors]\n",
    "        \n",
    "        if queryid is not None:\n",
    "            if type(queryid)==int or type(queryid)==str:\n",
    "                queryid = [queryid,]\n",
    "            shown_stageid = [l[\"Stage ID\"] for l in self.df.where(F.col(\"real_queryid\").isin(queryid)).select(\"Stage ID\").distinct().collect()]\n",
    "        if stageid is not None:\n",
    "            if type(stageid)==int:\n",
    "                shown_stageid = [stageid,]\n",
    "            elif type(stageid)==list:\n",
    "                shown_stageid = stageid\n",
    "            queryid = [l[\"real_queryid\"] for l in self.df.where(F.col(\"`Stage ID`\").isin(shown_stageid)).select(\"real_queryid\").limit(1).collect()]\n",
    "\n",
    "\n",
    "        queryplans=[]\n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").select(\"Stage ID\",\"ID\",\"Update\").groupBy(\"ID\",\"Stage ID\").agg(F.round(F.sum(\"Update\"),1).alias(\"value\"),F.round(F.stddev(\"Update\"),1).alias(\"stdev\")).collect()\n",
    "        accid2stageid={l.ID:(l[\"Stage ID\"],l[\"value\"],l[\"stdev\"]) for l in dfmetric}\n",
    "\n",
    "        stagetime=self.df.where((F.col(\"real_queryid\").isin(queryid))).where(F.col(\"Event\")=='SparkListenerTaskEnd').groupBy(\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.stddev(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000,1).alias(\"time stdev\"),\n",
    "            F.count(F.col(\"Task ID\")).alias(\"partitions\")\n",
    "            ).orderBy(F.desc(\"elapsed time\")).collect()\n",
    "\n",
    "        apptotaltime=reduce(lambda x,y: x+y['elapsed time'], stagetime,0)\n",
    "        if apptotaltime==0:\n",
    "            display(HTML(\"<font size=4 color=red>Error, totaltime is 0 </font>\"))\n",
    "            apptotaltime=1\n",
    "            return \"\"\n",
    "\n",
    "        stagemap={l[\"Stage ID\"]:l[\"elapsed time\"] for l in stagetime}\n",
    "        stage_time_stdev_map={l[\"Stage ID\"]:l[\"time stdev\"] for l in stagetime}\n",
    "        stagepartmap={l[\"Stage ID\"]:l[\"partitions\"] for l in stagetime}\n",
    "\n",
    "        keystage=[]\n",
    "        keystagetime=[]\n",
    "        subtotal=0\n",
    "        for s in stagetime:\n",
    "            subtotal=subtotal+s['elapsed time']\n",
    "            keystage.append(s['Stage ID'])\n",
    "            keystagetime.append(s['elapsed time'])\n",
    "            if subtotal/apptotaltime>0.9:\n",
    "                break\n",
    "        keystagetime=[\"{:02x}{:02x}\".format(int(255*l/keystagetime[0]),255-int(255*l/keystagetime[0])) for l in keystagetime if keystagetime[0]>0]\n",
    "        keystagemap=dict(zip(keystage,keystagetime))\n",
    "        outstr=[]\n",
    "        def print_plan(real_queryid,level,node,parent_stageid):\n",
    "            stageid = accid2stageid[int(node[\"metrics\"][0][\"accumulatorId\"])][0]  if node[\"metrics\"] is not None and len(node[\"metrics\"])>0 and node[\"metrics\"][0][\"accumulatorId\"] in accid2stageid else parent_stageid\n",
    "\n",
    "            if stageid in shown_stageid:\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystagemap else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stageParts=0 if stageid not in stagepartmap else stagepartmap[stageid]\n",
    "\n",
    "                input_rowcntstr=\"\"\n",
    "                output_rowcntstr=\"\"\n",
    "                timestr=\"\"\n",
    "                timename=\"\"\n",
    "                input_columnarbatch=\"\"\n",
    "                output_columnarbatch=\"\"\n",
    "                output_row_batch=\"\"\n",
    "                other_metric_name_str=\"\"\n",
    "                other_metric_str=\"\"\n",
    "\n",
    "                outputrows=0\n",
    "                outputbatches=0\n",
    "                if node[\"metrics\"] is not None:\n",
    "                    for m in node[\"metrics\"]:\n",
    "\n",
    "                        if m[\"accumulatorId\"] not in accid2stageid:\n",
    "                            continue\n",
    "\n",
    "                        value=accid2stageid[m[\"accumulatorId\"]][1]\n",
    "                        stdev_value=accid2stageid[m[\"accumulatorId\"]][2]\n",
    "                        stdev_value=0 if stdev_value is None else stdev_value\n",
    "                        if m[\"metricType\"] in ['nsTiming','timing']:\n",
    "                            totaltime=value/1000 if  m[\"metricType\"] == 'timing' else value/1000000000\n",
    "                            stdev_value=stdev_value/1000 if  m[\"metricType\"] == 'timing' else stdev_value/1000000000\n",
    "                            timename=timename+m[\"name\"]+\"<br>\"\n",
    "                            timeratio= 0  if stagetime==0 else totaltime/self.executor_instances/self.executor_cores*self.taskcpus/stagetime*100\n",
    "                            timeratio_query = totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100\n",
    "                            if timeratio > 10 or timeratio_query>10:\n",
    "                                timestr=timestr+\"<font style='background-color:#ffff42'>{:.2f}s ({:.1f}%, {:.1f}%, {:.2f})</font><br>\".format(totaltime,timeratio, totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100,stdev_value)\n",
    "                            else:\n",
    "                                timestr=timestr+\"{:.2f}s ({:.1f}%, {:.1f}%, {:.2f})<br>\".format(totaltime,timeratio, totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100,stdev_value)\n",
    "                        elif m[\"name\"]==\"number of output rows\":\n",
    "                            output_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                            outputrows=value\n",
    "                        elif m[\"name\"] in [\"number of output columnar batches\",\"number of output batches\",\"output_batches\"]:\n",
    "                            output_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                            outputbatches=value\n",
    "                        elif m[\"name\"]==\"number of input rows\":\n",
    "                            input_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                        elif m[\"name\"] in [\"number of input batches\",\"number of Input batches\",\"input_batches\"]:\n",
    "                            input_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                        else:\n",
    "                            other_metric_name_str=other_metric_name_str+m[\"name\"]+\"<br>\"\n",
    "                            if value>1000000000:\n",
    "                                other_metric_str=other_metric_str+\"{:,.1f} G (stdev: {:,.1f})<br>\".format(value/1000000000,stdev_value/1000000000)\n",
    "                            elif value>1000000:\n",
    "                                other_metric_str=other_metric_str+\"{:,.1f} M (stdev: {:,.1f})<br>\".format(value/1000000,stdev_value/1000000)\n",
    "                            elif value>1000:\n",
    "                                other_metric_str=other_metric_str+\"{:,.1f} K (stdev: {:,.1f})<br>\".format(value/1000,stdev_value/1000)\n",
    "                            else:\n",
    "                                other_metric_str=other_metric_str+\"{:,d} (stdev: {:,.1f})<br>\".format(int(value),stdev_value)\n",
    "\n",
    "\n",
    "                if outputrows>0 and outputbatches>0:\n",
    "                    output_row_batch=\"{:,d}\".format(int(outputrows/outputbatches))\n",
    "\n",
    "\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystage else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stage_time_stdev=0 if stageid not in stage_time_stdev_map else stage_time_stdev_map[stageid]\n",
    "                \n",
    "                nodenamestr=node[\"nodeName\"]\n",
    "                if nodenamestr is None:\n",
    "                    nodenamestr=\"\"\n",
    "                if nodenamestr in ['ColumnarToRow','RowToArrowColumnar','ArrowColumnarToRow','ArrowRowToColumnarExec']:\n",
    "                    nodename='<span style=\"color: green; background-color: #ffff42\">'+nodenamestr+'</span>'\n",
    "                else:\n",
    "                    nodename=nodenamestr\n",
    "                if outputstage is not None:\n",
    "                    outputstage.append({\"queryid\":real_queryid,\"stageid\":stageid,\"stagetime\":stagetime,\"stageParts\":stageParts,\"nodename\":nodenamestr,\"output_rowcnt\":outputrows,\"nodename_level\":\" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodenamestr})\n",
    "                if not show_plan_only:\n",
    "                    nodestr= \" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodename\n",
    "                    if show_simple_string :\n",
    "                        simstr=node['simpleString']\n",
    "                        nodestr = nodestr + \"<br>\\n\" +  simstr                                                                 \n",
    "                                                                  \n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime}({stage_time_stdev}) </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + nodestr + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_row_batch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {timename} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'>{timestr}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {other_metric_name_str} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'>{other_metric_str}</td></tr>\")\n",
    "                else:\n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + \" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodename + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td></tr>\")\n",
    "                    \n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    print_plan(real_queryid, level+1,c,stageid)\n",
    "\n",
    "        for c in queryplans:\n",
    "            outstr.append(\"<font color=red size=4>\"+str(c['real_queryid'])+\"</font><table>\")\n",
    "            if not show_plan_only:\n",
    "                outstr.append('''<tr>\n",
    "                                    <td>stage id</td>\n",
    "                                    <td>stage time</td>\n",
    "                                    <td>partions</td>\n",
    "                                    <td>operator</td>\n",
    "                                    <td>input rows</td>\n",
    "                                    <td>input batches</td>\n",
    "                                    <td>output rows</td>\n",
    "                                    <td>output batches</td>\n",
    "                                    <td>output rows/batch</td>\n",
    "                                    <td width=150>time metric name</td>\n",
    "                                    <td width=200>time(%stage,%total,stdev)</td>\n",
    "                                    <td width=150>other metric name</td>\n",
    "                                    <td width=130>value</td>\n",
    "                                </tr>''')\n",
    "            else:\n",
    "                outstr.append('''<tr>\n",
    "                                    <td>stage id</td>\n",
    "                                    <td>stage time</td>\n",
    "                                    <td>partions</td>\n",
    "                                    <td>operator</td>\n",
    "                                    <td>output rows</td>\n",
    "                                </tr>''')\n",
    "\n",
    "            print_plan(c['real_queryid'],0,c,0)\n",
    "            outstr.append(\"</table>\")\n",
    "        if plot:\n",
    "            display(HTML(\" \".join(outstr)))\n",
    "        return \" \".join(outstr)\n",
    "    \n",
    "    def get_metric_output_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of output rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_input_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of input rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_rowcnt(self,rowname, **kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        stageid=kwargs.get(\"stageid\",None)\n",
    "        \n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "            \n",
    "        if stageid and type(stageid)==int:\n",
    "            stageid = [stageid,]\n",
    "            \n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "        qps=[]\n",
    "\n",
    "        rownames=rowname if type(rowname)==list else [rowname,]\n",
    "        def get_child(execid,node):\n",
    "            if node['metrics'] is not None:\n",
    "                outputrows=[x for x in node[\"metrics\"] if \"name\" in x and x[\"name\"] in rownames]\n",
    "                if len(outputrows)>0:\n",
    "                    qps.append([node[\"nodeName\"],execid,outputrows[0]['accumulatorId']])\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        if len(qps)==0:\n",
    "            print(\"Metric \",rowname,\" is not found. \")\n",
    "            return None\n",
    "        stagetime=self.df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"Stage ID\").agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"stage time\"))\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").drop(\"metric\")\n",
    "        numrowmetric=spark.createDataFrame(qps)\n",
    "        numrowmetric=numrowmetric.withColumnRenamed(\"_1\",\"metric\").withColumnRenamed(\"_2\",\"real_queryid\").withColumnRenamed(\"_3\",\"metricid\")\n",
    "        dfmetric_rowcnt=dfmetric.join(numrowmetric.drop(\"real_queryid\"),on=[F.col(\"metricid\")==F.col(\"ID\")],how=\"right\")\n",
    "        stagemetric=dfmetric_rowcnt.groupBy(\"queryid\",\"real_queryid\",\"Job ID\",\"Stage ID\",\"metricid\").agg(F.round(F.sum(\"Update\")/1000000,2).alias(\"total_row\"),F.max(\"metric\").alias(\"nodename\")).join(stagetime,\"Stage ID\")\n",
    "\n",
    "        if queryid:\n",
    "            return stagemetric.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"Stage ID\").toPandas()\n",
    "        else:\n",
    "            noderow=stagemetric.groupBy(\"real_queryid\",\"nodename\").agg(F.round(F.sum(\"total_row\"),2).alias(\"total_row\")).orderBy(\"nodename\").collect()\n",
    "            out={}\n",
    "            qids=set([r.real_queryid for r in noderow])\n",
    "            for r in noderow:\n",
    "                if r.nodename not in out:\n",
    "                    out[r.nodename]={c:0 for c in qids}\n",
    "                out[r.nodename][r.real_queryid]=r.total_row\n",
    "            return pandas.DataFrame(out).T\n",
    "    \n",
    "    def get_query_info(self,queryid):\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time stat info </b></font>\",))\n",
    "        tmp=self.get_query_time(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage stat info </b></font>\",))\n",
    "        display(self.get_stage_stat(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query plan </b></font>\",))\n",
    "        self.get_query_plan(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage hist info </b></font>\",))\n",
    "        self.show_Stages_hist(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time info </b></font>\",))\n",
    "        display(self.show_time_metric(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator and rowcount </b></font>\",))\n",
    "        display(self.get_metric_input_rowcnt(queryid=queryid))\n",
    "        display(self.get_metric_output_rowcnt(queryid=queryid))\n",
    "        \n",
    "    def get_app_info(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        display(HTML(f\"<font color=red size=7 face='Courier New'><b> {self.appid} </b></font>\",))\n",
    "        display(HTML(f\"<a href=http://sr525:18080/history/{self.appid}>http://sr525:18080/history/{self.appid}</a>\"))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query time </b></font>\",))\n",
    "        tmp=self.get_query_time(**kwargs)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator count </b></font>\",))\n",
    "        pdf=self.getOperatorCount()\n",
    "        display(pdf.style.apply(background_gradient,\n",
    "               cmap='OrRd',\n",
    "               m=pdf.min().min(),\n",
    "               M=pdf.max().max(),\n",
    "               low=0,\n",
    "               high=1))\n",
    "        \n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator input row count </b></font>\",))\n",
    "        pdf=self.get_metric_input_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator output row count </b></font>\",))\n",
    "        pdf=self.get_metric_output_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        self.show_time_metric(**kwargs)\n",
    "        \n",
    "    def get_stage_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "\n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "            \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)).where(F.col(\"Event\")=='SparkListenerTaskEnd')\n",
    "        \n",
    "        inputsize = df.select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Stage ID\") \\\n",
    "                      .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read\"))\n",
    "        \n",
    "        return df.groupBy(\"Job ID\",\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.sum(F.col(\"Disk Bytes Spilled\"))/1024/1024/1024,1).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(F.col(\"Memory Bytes Spilled\"))/1024/1024/1024,1).alias(\"mem spilled\"),\n",
    "            F.round(F.sum(F.col(\"Local Bytes Read\"))/1024/1024/1024,1).alias(\"local read\"),\n",
    "            F.round(F.sum(F.col(\"Remote Bytes Read\"))/1024/1024/1024,1).alias(\"remote read\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Bytes Written\"))/1024/1024/1024,1).alias(\"shuffle write\"),\n",
    "            F.round(F.sum(F.col(\"Executor Deserialize Time\"))/1000,1).alias(\"deseri time\"),\n",
    "            F.round(F.sum(F.col(\"Fetch Wait Time\"))/1000,1).alias(\"fetch wait time\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Write Time\"))/1000000000,1).alias(\"shuffle write time\"),\n",
    "            F.round(F.sum(F.col(\"Result Serialization Time\"))/1000,1).alias(\"seri time\"),\n",
    "            F.round(F.sum(F.col(\"Getting Result Time\"))/1000,1).alias(\"get result time\"),\n",
    "            F.round(F.sum(F.col(\"JVM GC Time\"))/1000,1).alias(\"gc time\"),\n",
    "            F.round(F.sum(F.col(\"Executor CPU Time\"))/1000000000,1).alias(\"exe cpu time\")    \n",
    "            ).join(inputsize,on=[\"Stage ID\"],how=\"left\").orderBy(\"Stage ID\").toPandas()\n",
    "    \n",
    "    def get_metrics_by_node(self,node_name):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        metrics=self.queryplans.collect()\n",
    "        coalesce=[]\n",
    "        metricsid=[0]\n",
    "        def get_metric(root):\n",
    "            if root['nodeName']==node_name:\n",
    "                metricsid[0]=metricsid[0]+1\n",
    "                for l in root[\"metrics\"]:\n",
    "                    coalesce.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"],metricsid[0]])\n",
    "            if root[\"children\"] is not None:\n",
    "                for c in root[\"children\"]:\n",
    "                    get_metric(c)\n",
    "        for c in metrics:\n",
    "            get_metric(c)\n",
    "\n",
    "        df=self.df.select(\"queryid\",\"real_queryid\",'Stage ID','Task ID','Job ID',F.explode(\"Accumulables\"))\n",
    "        df=df.select(\"*\",\"col.*\")\n",
    "        metricdf=spark.createDataFrame(coalesce)\n",
    "        metricdf=metricdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"Unit\").withColumnRenamed(\"_3\",\"metricName\").withColumnRenamed(\"_4\",\"nodeName\").withColumnRenamed(\"_5\",\"nodeID\")\n",
    "        df=df.join(metricdf,on=[\"ID\"],how=\"right\")\n",
    "        shufflemetric=set(l[2] for l in coalesce)\n",
    "        metricdfs=[df.where(F.col(\"Name\")==l).groupBy(\"real_queryid\",\"nodeID\",\"Stage ID\").agg(F.stddev(\"Update\").alias(l+\"_stddev\"),F.mean(\"Update\").alias(l+\"_mean\"),F.mean(\"Update\").alias(l) if l.startswith(\"avg\") else F.sum(\"Update\").alias(l)) for l in shufflemetric]\n",
    "        \n",
    "        stagetimedf=self.df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"Stage ID\").agg(F.count(\"*\").alias(\"partnum\"),F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000,2).alias(\"ElapsedTime\"))\n",
    "        \n",
    "        nodemetric=reduce(lambda x,y: x.join(y, on=['nodeID',\"Stage ID\",\"real_queryid\"],how=\"full\"),metricdfs)\n",
    "        return nodemetric.join(stagetimedf,on=\"Stage ID\")\n",
    "    \n",
    "    \n",
    "    def get_coalesce_batch_row_cnt(self,**kwargs):\n",
    "        stagesum=self.get_metrics_by_node(\"CoalesceBatches\")\n",
    "        \n",
    "        pandas.options.display.float_format = '{:,}'.format\n",
    "        \n",
    "        stagesum=stagesum.withColumnRenamed(\"number of output rows\",\"rows\")\n",
    "        \n",
    "        coalescedf = stagesum.orderBy(\"real_queryid\",'Stage ID').where(\"rows>4000\").toPandas()\n",
    "        \n",
    "        coalescedf[\"row/input_batch\"] = coalescedf[\"rows\"]/coalescedf[\"input_batches\"]\n",
    "        coalescedf[\"row/out_batch\"] = coalescedf[\"rows\"]/coalescedf[\"output_batches\"]\n",
    "        coalescedf['stage']=coalescedf[\"real_queryid\"].astype(str)+\"_\"+coalescedf['Stage ID'].astype(str)\n",
    "        \n",
    "        ax=coalescedf.plot(y=[\"row/input_batch\",\"row/out_batch\"],figsize=(30,8),style=\"-*\")\n",
    "        coalescedf.plot(ax=ax,y=['rows'],secondary_y=['rows'],style=\"k_\")\n",
    "        self.print_real_queryid(ax,coalescedf)\n",
    "        \n",
    "        return coalescedf\n",
    "    \n",
    "    def print_real_queryid(self,ax,dataset):\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "\n",
    "        real_queryid=list(dataset['real_queryid'])\n",
    "        s=real_queryid[0]\n",
    "        lastx=0\n",
    "        for idx,v in enumerate(real_queryid):\n",
    "            if v!=s:\n",
    "                xmin = xmax = idx-1+0.5\n",
    "                l = mlines.Line2D([xmin,xmax], [ymin,ymax],color=\"green\")\n",
    "                ax.add_line(l)\n",
    "                ax.text(lastx+(xmin-lastx)/2-0.25,ymin-(ymax-ymin)/20,f\"{s}\",size=20)\n",
    "                s=v\n",
    "                lastx=xmin\n",
    "\n",
    "    def get_shuffle_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        shufflesize=kwargs.get(\"shuffle_size\",1000000)\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "\n",
    "        exchangedf=self.get_metrics_by_node(\"ColumnarExchange\")\n",
    "        exchangedf.cache()\n",
    "        exchangedf.count()\n",
    "        mapdf=exchangedf.where(\"totaltime_split is not null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"map_stageid\"),\"real_queryid\",F.floor(F.col(\"totaltime_split\")/F.col(\"totaltime_split_mean\")).alias(\"map_partnum\"),\"totaltime_compress\",\"totaltime_computepid\",\"totaltime_split\",\"shuffle write time\",\"shuffle spill time\",'shuffle records written','data size','shuffle bytes written','shuffle bytes written_mean','shuffle bytes written_stddev','shuffle bytes spilled','number of input rows')\n",
    "        reducerdf=exchangedf.where(\"totaltime_split is null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"reducer_stageid\"),\"real_queryid\",'local blocks read','local bytes read',F.floor(F.col(\"records read\")/F.col(\"records read_mean\")).alias(\"reducer_partnum\"),(F.col('avg read batch num rows')/10).alias(\"avg read batch num rows\"),'remote bytes read','records read','remote blocks read',(F.col(\"number of output rows\")/F.col(\"records read\")).alias(\"avg rows per split recordbatch\"))\n",
    "        shuffledf=mapdf.join(reducerdf,on=[\"nodeID\",\"real_queryid\"],how=\"full\")\n",
    "        if queryid is not None:\n",
    "            shuffledf=shuffledf.where(F.col(\"real_queryid\")==queryid) \n",
    "        shuffle_pdf=shuffledf.where(\"`shuffle bytes written`>1000000\").orderBy(\"real_queryid\",\"map_stageid\",\"nodeID\").toPandas()\n",
    "        shuffle_pdf[\"shuffle bytes written\"]=shuffle_pdf[\"shuffle bytes written\"]/1000000000\n",
    "        shuffle_pdf[\"data size\"]=shuffle_pdf[\"data size\"]/1000000000\n",
    "        shuffle_pdf[\"shuffle bytes written_mean\"]=shuffle_pdf[\"shuffle bytes written_mean\"]/1000000\n",
    "        shuffle_pdf[\"shuffle bytes written_stddev\"]=shuffle_pdf[\"shuffle bytes written_stddev\"]/1000000\n",
    "        ax=shuffle_pdf.plot(y=[\"avg read batch num rows\",'avg rows per split recordbatch'],figsize=(30,8),style=\"b-*\",title=\"average batch size after split\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"split_ratio\"]=shuffle_pdf[\"records read\"]/shuffle_pdf['shuffle records written']\n",
    "        ax=shuffle_pdf.plot(y=[\"split_ratio\",\"records read\"],secondary_y=[\"records read\"],figsize=(30,8),style=\"-*\",title=\"Split Ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"compress_ratio\"]=shuffle_pdf[\"data size\"]/shuffle_pdf['shuffle bytes written']\n",
    "        ax=shuffle_pdf.plot(y=[\"shuffle bytes written\",\"compress_ratio\"],secondary_y=[\"compress_ratio\"],figsize=(30,8),style=\"-*\",title=\"compress ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shufflewritepdf=shuffle_pdf\n",
    "        ax=shufflewritepdf.plot.bar(y=[\"shuffle write time\",\"shuffle spill time\",\"totaltime_compress\",\"totaltime_split\",\"totaltime_computepid\"],stacked=True,figsize=(30,8),title=\"split time + shuffle write time vs. shuffle bytes written\")\n",
    "        ax=shufflewritepdf.plot(ax=ax,y=[\"shuffle bytes written\"],secondary_y=[\"shuffle bytes written\"],style=\"-*\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        \n",
    "        metrics=self.queryplans.collect()\n",
    "        coalesce=[]\n",
    "        metricsid=[0]\n",
    "        def get_metric(root):\n",
    "            if root['nodeName']==\"ColumnarExchange\":\n",
    "                metricsid[0]=metricsid[0]+1\n",
    "                for l in root[\"metrics\"]:\n",
    "                    coalesce.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"],metricsid[0],root[\"simpleString\"]])\n",
    "            if root[\"children\"] is not None:\n",
    "                for c in root[\"children\"]:\n",
    "                    get_metric(c)\n",
    "        for c in metrics:\n",
    "            get_metric(c)\n",
    "\n",
    "        tps={}\n",
    "        for r in coalesce:\n",
    "            rx=re.search(r\"\\[OUTPUT\\] List\\((.*)\\)\",r[5])\n",
    "            if rx:\n",
    "                if r[4] not in tps:\n",
    "                    tps[r[4]]={}\n",
    "                    fds=rx.group(1).split(\", \")\n",
    "                    for f in fds:\n",
    "                        if not re.search(r\":(.+Type)\",f):\n",
    "                            print(fds)\n",
    "                        else:\n",
    "                            tp=re.search(r\":(.+Type)\",f).group(1)\n",
    "                            if tp not in tps[r[4]]:\n",
    "                                tps[r[4]][tp]=1\n",
    "                            else:\n",
    "                                tps[r[4]][tp]+=1\n",
    "        if len(tps)>0:\n",
    "            typedf=pandas.DataFrame(tps).T.reset_index()\n",
    "            typedf=typedf.fillna(0)\n",
    "            shuffle_pdf=pandas.merge(shuffle_pdf,typedf,left_on=\"nodeID\",right_on=\"index\")\n",
    "            shufflewritepdf=shuffle_pdf\n",
    "            ax=shufflewritepdf.plot.bar(y=[\"number of input rows\"],stacked=True,figsize=(30,8),title=\"rows vs. shuffle data type\")\n",
    "            ax=shufflewritepdf.plot(ax=ax,y=list(typedf.columns[1:]),secondary_y=list(typedf.columns[1:]),style=\"-o\")\n",
    "            self.print_real_queryid(ax,shufflewritepdf)\n",
    "            ax=shufflewritepdf.plot.bar(y=[\"totaltime_split\"],stacked=True,figsize=(30,8),title=\"split time vs. shuffle data type\")\n",
    "            ax=shufflewritepdf.plot(ax=ax,y=list(typedf.columns[1:]),secondary_y=list(typedf.columns[1:]),style=\"-o\")\n",
    "            self.print_real_queryid(ax,shufflewritepdf)\n",
    "\n",
    "        \n",
    "        \n",
    "        shufflewritepdf.plot(x=\"shuffle bytes written\",y=[\"shuffle write time\",\"totaltime_split\"],figsize=(30,8),style=\"*\")\n",
    "        shufflewritepdf[\"avg shuffle batch size after split\"]=shufflewritepdf[\"shuffle bytes written\"]*1000000/shufflewritepdf['records read']\n",
    "        shufflewritepdf[\"avg batch size after split\"]=shufflewritepdf[\"data size\"]*1000000/shufflewritepdf['records read']\n",
    "        ax=shufflewritepdf.plot(y=[\"avg shuffle batch size after split\",\"avg batch size after split\",\"shuffle bytes written\"],secondary_y=[\"shuffle bytes written\"],figsize=(30,8),style=\"-*\",title=\"avg batch KB after split\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        shufflewritepdf[\"avg batch# per splitted partition\"]=shufflewritepdf['records read']/(shufflewritepdf['local blocks read']+shufflewritepdf['remote blocks read'])\n",
    "        ax=shufflewritepdf.plot(y=[\"avg batch# per splitted partition\",'records read'],secondary_y=['records read'],figsize=(30,8),style=\"-*\",title=\"avg batch# per splitted partition\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('shuffle wite bytes with stddev')\n",
    "        ax.errorbar(x=shuffle_pdf.index,y=shuffle_pdf['shuffle bytes written_mean'], yerr=shuffle_pdf['shuffle bytes written_stddev'], linestyle='None', marker='o')\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf['record batch per mapper per reducer']=shuffle_pdf['records read']/(shuffle_pdf[\"map_partnum\"]*shuffle_pdf['reducer_partnum'])\n",
    "        ax=shuffle_pdf.plot(y=[\"record batch per mapper per reducer\"],figsize=(30,8),style=\"b-*\",title=\"record batch per mapper per reducer\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        \n",
    "        inputsize = self.df.select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "              .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "              .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "              .groupBy(\"Task ID\") \\\n",
    "              .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "        stageinput=self.df.where(\"event='SparkListenerTaskEnd'\" )\\\n",
    "                                .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                                .fillna(0) \\\n",
    "                                .select(F.col('Host'), F.col(\"real_queryid\"),F.col('Stage ID'),F.col('Task ID'),\n",
    "                                        F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                        F.round((F.col('`input read`')+F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input'))\n",
    "        baisstage=stageinput.groupBy(\"real_queryid\",\"Stage ID\").agg(F.mean(\"elapsedtime\").alias(\"elapsed\"),F.mean(\"input\").alias(\"input\"),\n",
    "                                                            (F.stddev(\"elapsedtime\")).alias(\"elapsedtime_err\"),\n",
    "                                                            (F.stddev(\"input\")).alias(\"input_err\"),\n",
    "                                                            (F.max(\"elapsedtime\")-F.mean(\"elapsedtime\")).alias(\"elapsed_max\"),\n",
    "                                                            (F.mean(\"elapsedtime\")-F.min(\"elapsedtime\")).alias(\"elapsed_min\"),\n",
    "                                                            (F.max(\"input\")-F.mean(\"input\")).alias(\"input_max\"),\n",
    "                                                            (F.mean(\"input\")-F.min(\"input\")).alias(\"input_min\")).orderBy(\"real_queryid\",\"Stage ID\")\n",
    "        dfx=baisstage.toPandas()\n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('input size')\n",
    "        ax.errorbar(x=dfx.index,y=dfx['input'], yerr=dfx['input_err'], fmt='ok', ecolor='red', lw=3)\n",
    "        ax.errorbar(x=dfx.index,y=dfx['input'],yerr=[dfx['input_min'],dfx['input_max']],\n",
    "                     fmt='.k', ecolor='gray', lw=1)\n",
    "        self.print_real_queryid(ax,dfx)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('stage time')\n",
    "\n",
    "        ax.errorbar(x=dfx.index,y=dfx['elapsed'], yerr=dfx['elapsedtime_err'], fmt='ok', ecolor='red', lw=5)\n",
    "        ax.errorbar(x=dfx.index,y=dfx['elapsed'],yerr=[dfx['elapsed_min'],dfx['elapsed_max']],\n",
    "                     fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "        self.print_real_queryid(ax,dfx)\n",
    "        return (shuffle_pdf,dfx)\n",
    "    \n",
    "    def get_stages_w_odd_partitions(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        return appals.df.where(\"Event='SparkListenerTaskEnd'\")\\\n",
    "                    .groupBy(\"Stage ID\",\"real_queryid\")\\\n",
    "                    .agg((F.sum(F.col('Finish Time')-F.col('Launch Time'))/1000).alias(\"elapsed time\"),\n",
    "                         F.count('*').alias('partitions'))\\\n",
    "                    .where(F.col(\"partitions\")%(appals.executor_cores*appals.executor_instances/appals.taskcpus)!=0)\\\n",
    "                    .orderBy(F.desc(\"elapsed time\")).toPandas()\n",
    "   \n",
    "    def get_scaned_column_v1(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"Scan arrow\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\":\")[0] for l in re.split(r'[<>]',s['metadata']['ReadSchema'])[1].split(\",\")])])\n",
    "        return alltable\n",
    "    \n",
    "    def get_scaned_column_v2(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"ColumnarBatchScan\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\"#\")[0] for l in re.split(r\"[\\[\\]]\",s['simpleString'])[1].split(\",\")])])\n",
    "        return alltable\n",
    "    \n",
    "    def compare_query(appals,queryid,appbaseals):\n",
    "        print(f\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~Query{queryid}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        appals.show_critical_path_time_breakdown(queryid=22)\n",
    "        s1=appals.get_stage_stat(queryid=queryid)\n",
    "        s2=appbaseals.get_stage_stat(queryid=queryid)\n",
    "        ls=s1[['Stage ID','elapsed time']]\n",
    "        ls.columns=['l sid','l time']\n",
    "        rs=s2[['Stage ID','elapsed time']]\n",
    "        rs.columns=['r sid','r time']\n",
    "        js=ls.join(rs)\n",
    "        js['gap']=js['r time'] - js['l time']\n",
    "        js['gap']=js['gap'].round(2)\n",
    "        display(js)\n",
    "        display(s1)\n",
    "        display(s2)\n",
    "        stagesmap={}\n",
    "        for x in range(0,min(len(s1),len(s2))):\n",
    "            stagesmap[s1['Stage ID'][x]]=s2['Stage ID'][x]\n",
    "        totaltime=sum(s1['elapsed time'])\n",
    "        acctime=0\n",
    "        s1time=s1.sort_values(\"elapsed time\",ascending=False,ignore_index=True)\n",
    "        ldfx=appals.get_metric_output_rowcnt(queryid=queryid)\n",
    "        rdfx=appbaseals.get_metric_output_rowcnt(queryid=queryid)\n",
    "\n",
    "        for x in range(0,len(s1time)):\n",
    "            sid1=int(s1time['Stage ID'][x])\n",
    "            sid2=int(stagesmap[sid1])\n",
    "            print(f\"============================================================\")\n",
    "            display(ldfx[ldfx['Stage ID']==sid1])\n",
    "            display(rdfx[ldfx['Stage ID']==sid2])\n",
    "            print(f\" Gazelle  Query {queryid}  Stage {sid1}\")\n",
    "            xf=appals.get_query_plan(stageid=sid1,show_simple_string=True)\n",
    "            print(f\" Photon  Query {queryid}  Stage {sid2}\")\n",
    "            xf=appbaseals.get_query_plan(stageid=sid2,show_simple_string=True)\n",
    "            acctime+=s1time['elapsed time'][x]\n",
    "            if acctime/totaltime>=0.9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notlist=['resource.executor.cores',\n",
    " 'spark.app.id',\n",
    " 'spark.app.initial.file.urls',\n",
    " 'spark.app.name',\n",
    " 'spark.app.startTime',\n",
    " 'spark.driver.port',\n",
    " 'spark.job.description',\n",
    " 'spark.jobGroup.id',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
    " 'spark.rdd.scope',\n",
    " 'spark.sql.execution.id',\n",
    " '__fetch_continuous_blocks_in_batch_enabled',\n",
    " 'spark.driver.appUIAddress'\n",
    " 'spark.driver.appUIAddress',\n",
    " 'spark.driver.host',\n",
    " 'spark.driver.appUIAddress',\n",
    " 'spark.driver.extraClassPath',\n",
    " 'spark.eventLog.dir',\n",
    " 'spark.executorEnv.CC',\n",
    " 'spark.executorEnv.LD_LIBRARY_PATH',\n",
    " 'spark.executorEnv.LD_PRELOAD',\n",
    " 'spark.executorEnv.LIBARROW_DIR',\n",
    " 'spark.files',\n",
    " 'spark.history.fs.logDirectory',\n",
    " 'spark.sql.warehouse.dir',\n",
    " 'spark.yarn.appMasterEnv.LD_PRELOAD',\n",
    " 'spark.yarn.dist.files'\n",
    "]\n",
    "def comp_spark_conf(app0,app1):   \n",
    "    pdf_sparkconf_0=app0.get_spark_config()\n",
    "    pdf_sparkconf_1=app1.get_spark_config()\n",
    "    pdfc=pdf_sparkconf_0.join(pdf_sparkconf_1,lsuffix=app0.appid[-8:],rsuffix=app1.appid[-8:])\n",
    "    pdfc[\"0\"+app0.appid[-8:]]=pdfc[\"0\"+app0.appid[-8:]].str.lower()\n",
    "    pdfc[\"0\"+app1.appid[-8:]]=pdfc[\"0\"+app1.appid[-8:]].str.lower()\n",
    "    \n",
    "    pdfc['comp']=(pdfc[\"0\"+app0.appid[-8:]]==pdfc[\"0\"+app1.appid[-8:]])\n",
    "    return pdfc.loc[(pdfc['comp']==False) & (~pdfc.index.isin(notlist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rst(pdrst):\n",
    "    html='''<style type=\"text/css\">\n",
    "    .tg  {border-collapse:collapse;border-spacing:0;border-color:#aabcfe;}\n",
    "    .tg td{font-family:Courier New;font-size:18px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aabcfe;color:#669;background-color:#e8edff;}\n",
    "    .tg th{font-family:Courier New;font-size:18px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aabcfe;color:#039;background-color:#b9c9fe;}\n",
    "    .tg .tg-phtq{background-color:#D2E4FC;border-color:inherit;text-align:left;vertical-align:top}\n",
    "    .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    "    .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "    .tg .tg-phtq_v{background-color:#D2E4FC;border-color:inherit;text-align:left;vertical-align:top; color:#FF0000}\n",
    "    .tg .tg-0pky_v{border-color:inherit;text-align:left;vertical-align:top; color:#FF0000}\n",
    "    </style>\n",
    "    <table class=\"tg\">\n",
    "      <tr>\n",
    "    '''\n",
    "    cols=pdrst.columns  \n",
    "    html=html+''.join(['<th class=\"tg-c3ow\">{:s}</th>'.format(l) for l in cols if l!='app_id'])\n",
    "    html=html+'<th class=\"tg-c3ow\">spark log</th><th class=\"tg-c3ow\">trace_view</th></tr>'\n",
    "    for idx, r in pdrst.iterrows():\n",
    "        html=html+\"<tr>\"\n",
    "        html=html+\"\".join(['<td class=\"{:s}\">{:s}</td>'.format('tg-phtq' if l!='elapsed time' else 'tg-phtq_v', str(r[l])) for l in cols if l!='app_id'])\n",
    "        html=html+'''<td class=\"tg-phtq\"><a href=\"http://10.1.2.107:18080/history/{:s}/jobs\"> {:s}</a></td><td class=\"tg-phtq\"><a href=http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{:s}.json> {:s}</a></td>'''.format(r['app_id'].appid,r['app_id'].appid,r['app_id'].appid,r['app_id'].appid)\n",
    "        html=html+\"</tr>\"\n",
    "    html=html+\"</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_metric(pdrst,slave_id,metric,core,agg_func):\n",
    "    pdrst['rst']=pdrst.apply(lambda x:x['app_id'].get_reduce_metric(slave_id,metric,core,agg_func), axis=1)\n",
    "    for l in agg_func:\n",
    "        pdrst[get_alias_name(metric,l)]=pdrst.apply(lambda x:x['rst'].iloc[0][get_alias_name(metric,l)],axis=1)\n",
    "    return pdrst.drop(columns=['rst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(rsta):\n",
    "    for i in range(0,r):\n",
    "        m=rsta.loc[i,'emon_mem_bw']\n",
    "        if m>40000:\n",
    "            for j in range(i,i+5):\n",
    "                if j>=r:\n",
    "                    break\n",
    "                if rsta.loc[j,'emon_mem_bw']<40000:\n",
    "                    m=0\n",
    "                    break\n",
    "            rsta.loc[i,'emon_mem_bw']=m\n",
    "        else:\n",
    "            rsta.loc[i,'emon_mem_bw']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_gradient(s, m, M, cmap='PuBu', low=0, high=0):\n",
    "    from matplotlib import colors\n",
    "    rng = M - m\n",
    "    norm = colors.Normalize(m - (rng * low),\n",
    "                            M + (rng * high))\n",
    "    normed = norm(s.values)\n",
    "    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n",
    "    return ['background-color: {:s}'.format(color) for color in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPCDS query map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "m='''1\tq01\n",
    "    2\tq02\n",
    "    3\tq03\n",
    "    4\tq04\n",
    "    5\tq05\n",
    "    6\tq06\n",
    "    7\tq07\n",
    "    8\tq08\n",
    "    9\tq09\n",
    "    10\tq10\n",
    "    11\tq11\n",
    "    12\tq12\n",
    "    13\tq13\n",
    "    14\tq14a\n",
    "    15\tq14b\n",
    "    16\tq15\n",
    "    17\tq16\n",
    "    18\tq17\n",
    "    19\tq18\n",
    "    20\tq19\n",
    "    21\tq20\n",
    "    22\tq21\n",
    "    23\tq22\n",
    "    24\tq23a\n",
    "    25\tq23b\n",
    "    26\tq24a\n",
    "    27\tq24b\n",
    "    28\tq25\n",
    "    29\tq26\n",
    "    30\tq27\n",
    "    31\tq28\n",
    "    32\tq29\n",
    "    33\tq30\n",
    "    34\tq31\n",
    "    35\tq32\n",
    "    36\tq33\n",
    "    37\tq34\n",
    "    38\tq35\n",
    "    39\tq36\n",
    "    40\tq37\n",
    "    41\tq38\n",
    "    42\tq39a\n",
    "    43\tq39b\n",
    "    44\tq40\n",
    "    45\tq41\n",
    "    46\tq42\n",
    "    47\tq43\n",
    "    48\tq44\n",
    "    49\tq45\n",
    "    50\tq46\n",
    "    51\tq47\n",
    "    52\tq48\n",
    "    53\tq49\n",
    "    54\tq50\n",
    "    55\tq51\n",
    "    56\tq52\n",
    "    57\tq53\n",
    "    58\tq54\n",
    "    59\tq55\n",
    "    60\tq56\n",
    "    61\tq57\n",
    "    62\tq58\n",
    "    63\tq59\n",
    "    64\tq60\n",
    "    65\tq61\n",
    "    66\tq62\n",
    "    67\tq63\n",
    "    68\tq64\n",
    "    69\tq65\n",
    "    70\tq66\n",
    "    71\tq67\n",
    "    72\tq68\n",
    "    73\tq69\n",
    "    74\tq70\n",
    "    75\tq71\n",
    "    76\tq72\n",
    "    77\tq73\n",
    "    78\tq74\n",
    "    79\tq75\n",
    "    80\tq76\n",
    "    81\tq77\n",
    "    82\tq78\n",
    "    83\tq79\n",
    "    84\tq80\n",
    "    85\tq81\n",
    "    86\tq82\n",
    "    87\tq83\n",
    "    88\tq84\n",
    "    89\tq85\n",
    "    90\tq86\n",
    "    91\tq87\n",
    "    92\tq88\n",
    "    93\tq89\n",
    "    94\tq90\n",
    "    95\tq91\n",
    "    96\tq92\n",
    "    97\tq93\n",
    "    98\tq94\n",
    "    99\tq95\n",
    "    100\tq96\n",
    "    101\tq97\n",
    "    102\tq98\n",
    "    103\tq99'''.split(\"\\n\")\n",
    "tpcds_query_map=[l.strip().split(\"\\t\") for l in m]\n",
    "tpcds_query_map={int(l[0]):l[1] for l in tpcds_query_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "260.969px",
    "left": "1571.97px",
    "top": "111.125px",
    "width": "295px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
