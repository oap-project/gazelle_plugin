{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML('<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib import colors\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.sans-serif'] =  'Courier New'\n",
    "rcParams['font.family'] = 'Courier New'\n",
    "rcParams['font.size'] = '12'\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display,HTML\n",
    "import threading\n",
    "import collections\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress,Layout\n",
    "import time\n",
    "import threading\n",
    "import gzip\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, DateType,\n",
    "    TimestampType, StringType, LongType, IntegerType, DoubleType,FloatType)\n",
    "from pyspark.sql.functions import to_date, floor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import lit\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "\n",
    "import re\n",
    "import math\n",
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start analysis cluster and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "executors_per_node = 5\n",
    "nodes=3\n",
    "cores_per_executor=4\n",
    "task_per_core=4\n",
    "\n",
    "\n",
    "cache_size=20\n",
    "total_size=100000\n",
    "print('executor per node: {:d}\\nparallelism: {:d}\\nmemory: {:d}m\\noffheap:{:d}m'.format(executors_per_node,nodes*executors_per_node*cores_per_executor*task_per_core,int(math.floor(nodes*total_size/(nodes*executors_per_node)))-1024-int(math.floor(cache_size*1024/(nodes*executors_per_node))),int(math.floor(cache_size*1024/(nodes*executors_per_node)))))\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.default.parallelism\", \"{:d}\".format(nodes*executors_per_node))\n",
    "    .set('spark.executor.instances', '{:d}'.format(nodes*executors_per_node))\n",
    "    .set('spark.sql.files.maxPartitionBytes', '256m')\n",
    "    .set('spark.app.name', 'pyspark_final')\n",
    "    .set('spark.rdd.compress', 'False')\n",
    "    .set('spark.serializer','org.apache.spark.serializer.KryoSerializer')    \n",
    "    .set('spark.executor.cores','{:d}'.format(cores_per_executor))\n",
    "    .set('spark.sql.adaptive.coalescePartitions.initialPartitionNum','{:d}'.format(nodes*executors_per_node*cores_per_executor*task_per_core))\n",
    "    .set('spark.sql.adaptive.enabled',True)\n",
    "    .set('spark.sql.adaptive.advisoryPartitionSizeInBytes', '256m')\n",
    "    .set('spark.executor.memory', '{:d}m'.format(int(math.floor(nodes*total_size/(nodes*executors_per_node)))-1024-int(math.floor(cache_size*1024/(nodes*executors_per_node)))))\n",
    "    .set('spark.task.cpus','1')\n",
    "    .set('spark.driver.memory','128g')\n",
    "    .set('spark.sql.inMemoryColumnarStorage.compressed','False')\n",
    "    .set('spark.sql.inMemoryColumnarStorage.batchSize','100000')\n",
    "#    .set('spark.memory.storageFraction','0.8')\n",
    "#    .set('spark.memory.fraction','0.7')\n",
    "        \n",
    "    .set('spark.sql.execution.arrow.fallback.enabled','True')\n",
    "    .set('spark.sql.execution.arrow.enabled','True')\n",
    "    .set('spark.sql.execution.arrow.maxRecordsPerBatch','100000')\n",
    "    .set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        \n",
    "    .set('spark.memory.offHeap.enabled','True')\n",
    "    .set('spark.memory.offHeap.size','{:d}m'.format(int(math.floor(cache_size*1024/(nodes*executors_per_node)))))\n",
    "    .set('spark.executor.memoryOverhead','{:d}m'.format(int(math.floor(cache_size*1024/(nodes*executors_per_node)))+3000))\n",
    "    .set('spark.sql.join.preferSortMergeJoin','False')\n",
    "#    .set('spark.executor.extraJavaOptions',\n",
    "#          '-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:InitiatingHeapOccupancyPercent=35')\n",
    "    .set('spark.executor.extraJavaOptions',\n",
    "          '-XX:+UseParallelGC -XX:+UseParallelOldGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps')\n",
    "    .set('spark.dynamicAllocation.enabled', 'False')\n",
    "    .set('spark.shuffle.service.enabled', 'False')\n",
    "    .set('spark.dynamicAllocation.shuffleTracking.enabled', 'Falze')\n",
    "    .set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "        \n",
    "#    .set(\"spark.sql.session.timeZone\", \"PST\")\n",
    "     .set(\"spark.sql.session.timeZone\", \"Etc/GMT-8\")\n",
    "#     .set(\"spark.driver.extraJavaOptions\",\"-Duser.timezone=UTC+8\")\n",
    "    \n",
    "       )\n",
    "\n",
    "sc = SparkContext(conf=conf,master='yarn')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SQLContext(sc)\n",
    "time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_stderr {\n",
    "background: #ffdd;\n",
    "display: none;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparklog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run ./sparklog_monitor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir=\"BASEDIR_TEMP\"\n",
    "appid=\"APPID_TEMP\"\n",
    "\n",
    "last_basedir=\"LAST_BASEDIR\"\n",
    "last_appid=\"LAST_APPID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# native, partition table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app=Application_Run(appid,basedir=basedir)\n",
    "appals=app.analysis['app']['als']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appals.get_basic_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appals.get_app_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle_df, dfx=appals.get_shuffle_stat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APP info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app.generate_trace_view(showemon=False,show_metric=emonmetric,disk_prefix='nvme%n1p1',nic_prefix=[\"'enp134s0f1'\",\"'eno1'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appals=app.analysis['app']['als']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "appals.get_app_info(disk_prefix='nvme%n1p1',nic_prefix=[\"'enp134s0f1'\",\"'eno1'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "appals.show_critical_path_time_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "197px",
    "left": "2188px",
    "top": "111px",
    "width": "269px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
